<!doctype html><html lang=en-us dir=ltr><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QB4H991E1G"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QB4H991E1G")</script><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content='使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統'><meta name=keywords content="翻轉吧金魚腦,使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統,kubernetes,rook,rook-ceph"><title>使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統</title>
<link rel=canonical href=https://blog.goldfishbrain-fighting.com/2023/kubernetes-rook-ceph/><link rel=stylesheet href=/scss/style.min.7fb7f5621081d8ecec310edc2d95cc0b431358ce9c30c3ee8601b16732dbef57.css><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script src=//cdn.bootcss.com/jquery/3.2.1/jquery.min.js></script><script>$(document).ready(function(){var e=setInterval(s,100),t=100,n=50;function s(){$("#busuanzi_container_site_pv").css("display")!="none"&&(clearInterval(e),$("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html())+t),$("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html())+n))}})</script><meta property='og:title' content='使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統'><meta property='og:description' content='使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統'><meta property='og:url' content='https://blog.goldfishbrain-fighting.com/2023/kubernetes-rook-ceph/'><meta property='og:site_name' content='翻轉吧金魚腦'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='kubernetes'><meta property='article:tag' content='ceph'><meta property='article:tag' content='rook-ceph'><meta property='article:published_time' content='2023-11-27T21:13:16+08:00'><meta property='article:modified_time' content='2023-11-27T21:13:16+08:00'><meta property='og:image' content='https://blog.goldfishbrain-fighting.com/2023/kubernetes-rook-ceph/media/Pasted-image-20231205214201.png'><meta name=twitter:title content="使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統"><meta name=twitter:description content="使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://blog.goldfishbrain-fighting.com/2023/kubernetes-rook-ceph/media/Pasted-image-20231205214201.png'><link rel="shortcut icon" href=/favicon.png><script async src="https://www.googletagmanager.com/gtag/js?id=G-QB4H991E1G"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QB4H991E1G")</script><meta name=google-site-verification content="EkUcMLTrO3RDZzjREX9Dffy2lcjWCR-2Qtu8VF4b8sQ"><meta content="翻轉吧金魚腦,使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統,kubernetes,rook,rook-ceph" name=keywords></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><div id=article-toolbar style=position:sticky;top:5px;z-index:1000><a href=/ class=back-home><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="15 6 9 12 15 18"/></svg>
<span>返回</span></a></div><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目錄</h2><div class=widget--toc><nav id=TableOfContents><ul><li><ul><li><a href=#確認磁碟無格式化過>確認磁碟無格式化過</a></li><li><a href=#ceph-cluster佈屬>ceph cluster佈屬</a><ul><li><a href=#依序執行三個yaml>依序執行三個yaml</a></li><li><a href=#執行clusteryaml>執行cluster.yaml</a></li><li><a href=#主機打標籤>主機打標籤</a></li><li><a href=#確認佈屬結果pending狀況發生>確認佈屬結果(Pending狀況發生)</a></li><li><a href=#執行toolboxyaml>執行toolbox.yaml</a></li><li><a href=#主機直接查看cephceph-common安裝>主機直接查看ceph(ceph-common安裝)</a></li></ul></li><li><a href=#cephfs佈屬>cephfs佈屬</a><ul><li><a href=#主機打標籤-1>主機打標籤</a></li><li><a href=#執行filesystemyaml>執行filesystem.yaml</a></li><li><a href=#確認佈屬結果>確認佈屬結果</a></li><li><a href=#創建storageclassstorageclassyaml>創建storageclass(storageclass.yaml)</a></li><li><a href=#測試storageclassnamerook-cephfs>測試storageClassName(rook-cephfs)</a></li></ul></li><li><a href=#ceph管理平台>Ceph管理平台</a><ul><li><a href=#查看mgr暴露的metric監控指標>查看mgr暴露的metric監控指標</a></li></ul></li><li><a href=#安裝prometheus收集指標>安裝Prometheus收集指標</a><ul><li><ul><li><a href=#安裝prometheus-operator>安裝prometheus-operator</a></li><li><a href=#安裝prometheus-server>安裝prometheus server</a></li></ul></li></ul></li><li><a href=#安裝grafana展示圖表>安裝Grafana展示圖表</a></li></ul></li></ul></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/2023/kubernetes-rook-ceph/><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231205214201_hu7a89e185921f36bf665966d7e56502de_57574_800x0_resize_box_3.png srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231205214201_hu7a89e185921f36bf665966d7e56502de_57574_800x0_resize_box_3.png 800w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231205214201_hu7a89e185921f36bf665966d7e56502de_57574_1600x0_resize_box_3.png 1600w" width=800 height=481 loading=lazy alt="Featured image of post 使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統"></a></div><div class=article-details><header class=article-category><a href=/categories/kubernetes/ style=background-color:#0000e3;color:#fff>kubernetes</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/2023/kubernetes-rook-ceph/>使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統</a></h2><h3 class=article-subtitle>使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2023/11/27</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>閱讀時間: 13 分鐘</time></div></footer></div></header><section class=article-content><section class="toc toc--inline"><h2>目錄</h2><div><nav id=TableOfContents><ul><li><ul><li><a href=#確認磁碟無格式化過>確認磁碟無格式化過</a></li><li><a href=#ceph-cluster佈屬>ceph cluster佈屬</a><ul><li><a href=#依序執行三個yaml>依序執行三個yaml</a></li><li><a href=#執行clusteryaml>執行cluster.yaml</a></li><li><a href=#主機打標籤>主機打標籤</a></li><li><a href=#確認佈屬結果pending狀況發生>確認佈屬結果(Pending狀況發生)</a></li><li><a href=#執行toolboxyaml>執行toolbox.yaml</a></li><li><a href=#主機直接查看cephceph-common安裝>主機直接查看ceph(ceph-common安裝)</a></li></ul></li><li><a href=#cephfs佈屬>cephfs佈屬</a><ul><li><a href=#主機打標籤-1>主機打標籤</a></li><li><a href=#執行filesystemyaml>執行filesystem.yaml</a></li><li><a href=#確認佈屬結果>確認佈屬結果</a></li><li><a href=#創建storageclassstorageclassyaml>創建storageclass(storageclass.yaml)</a></li><li><a href=#測試storageclassnamerook-cephfs>測試storageClassName(rook-cephfs)</a></li></ul></li><li><a href=#ceph管理平台>Ceph管理平台</a><ul><li><a href=#查看mgr暴露的metric監控指標>查看mgr暴露的metric監控指標</a></li></ul></li><li><a href=#安裝prometheus收集指標>安裝Prometheus收集指標</a><ul><li><ul><li><a href=#安裝prometheus-operator>安裝prometheus-operator</a></li><li><a href=#安裝prometheus-server>安裝prometheus server</a></li></ul></li></ul></li><li><a href=#安裝grafana展示圖表>安裝Grafana展示圖表</a></li></ul></li></ul></nav></div></section><p><a class=link href=https://rook.io/docs/rook/latest-release/Getting-Started/intro/ target=_blank rel=noopener>https://rook.io/docs/rook/latest-release/Getting-Started/intro/</a></p><div class=table-wrapper><table><thead><tr><th>節點名稱</th><th>IP</th><th>K8S角色</th><th>組件</th><th>磁碟 16G</th></tr></thead><tbody><tr><td>k8s-master71u</td><td>192.168.1.71</td><td>master</td><td>mon mgr osd mds mds</td><td>/dev/sdb</td></tr><tr><td>k8s-master72u</td><td>192.168.1.72</td><td>master</td><td>mon mgr osd mds</td><td>/dev/sdb</td></tr><tr><td>k8s-master73u</td><td>192.168.1.73</td><td>master</td><td>mon osd mds</td><td>/dev/sdb</td></tr><tr><td>k8s-node75u</td><td>192.168.1.75</td><td>worker</td><td>osd mds</td><td>/dev/sdb</td></tr><tr><td>k8s-node76u</td><td>192.168.1.76</td><td>worker</td><td>osd</td><td>/dev/sdb</td></tr></tbody></table></div><h2 id=確認磁碟無格式化過>確認磁碟無格式化過</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# lsblk
</span></span><span class=line><span class=cl>NAME             MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
</span></span><span class=line><span class=cl>loop0              7:0    <span class=m>0</span>  63.4M  <span class=m>1</span> loop /snap/core20/1974
</span></span><span class=line><span class=cl>loop1              7:1    <span class=m>0</span>  63.5M  <span class=m>1</span> loop /snap/core20/2015
</span></span><span class=line><span class=cl>loop2              7:2    <span class=m>0</span> 111.9M  <span class=m>1</span> loop /snap/lxd/24322
</span></span><span class=line><span class=cl>loop3              7:3    <span class=m>0</span>  40.8M  <span class=m>1</span> loop /snap/snapd/20092
</span></span><span class=line><span class=cl>loop4              7:4    <span class=m>0</span>  40.9M  <span class=m>1</span> loop /snap/snapd/20290
</span></span><span class=line><span class=cl>sda                8:0    <span class=m>0</span>    50G  <span class=m>0</span> disk
</span></span><span class=line><span class=cl>├─sda1             8:1    <span class=m>0</span>     1M  <span class=m>0</span> part
</span></span><span class=line><span class=cl>├─sda2             8:2    <span class=m>0</span>     2G  <span class=m>0</span> part /boot
</span></span><span class=line><span class=cl>└─sda3             8:3    <span class=m>0</span>    48G  <span class=m>0</span> part
</span></span><span class=line><span class=cl>  └─ubuntu--vg-root
</span></span><span class=line><span class=cl>                 253:0    <span class=m>0</span>    48G  <span class=m>0</span> lvm  /var/lib/kubelet/pods/ce7174f8-ad8f-4f42-8e58-2abca8c91424/volume-subpaths/tigera-ca-bundle/calico-node/1
</span></span><span class=line><span class=cl>                                           /
</span></span><span class=line><span class=cl>sdb                8:16   <span class=m>0</span>    16G  <span class=m>0</span> disk
</span></span><span class=line><span class=cl>sr0               11:0    <span class=m>1</span>  1024M  <span class=m>0</span> rom
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# lsblk -f
</span></span><span class=line><span class=cl>NAME FSTYPE FSVER LABEL UUID                                   FSAVAIL FSUSE% MOUNTPOINTS
</span></span><span class=line><span class=cl>loop0
</span></span><span class=line><span class=cl>     squash 4.0                                                      <span class=m>0</span>   100% /snap/core20/1974
</span></span><span class=line><span class=cl>loop1
</span></span><span class=line><span class=cl>     squash 4.0                                                      <span class=m>0</span>   100% /snap/core20/2015
</span></span><span class=line><span class=cl>loop2
</span></span><span class=line><span class=cl>     squash 4.0                                                      <span class=m>0</span>   100% /snap/lxd/24322
</span></span><span class=line><span class=cl>loop3
</span></span><span class=line><span class=cl>     squash 4.0                                                      <span class=m>0</span>   100% /snap/snapd/20092
</span></span><span class=line><span class=cl>loop4
</span></span><span class=line><span class=cl>     squash 4.0                                                      <span class=m>0</span>   100% /snap/snapd/20290
</span></span><span class=line><span class=cl>sda
</span></span><span class=line><span class=cl>├─sda1
</span></span><span class=line><span class=cl>│
</span></span><span class=line><span class=cl>├─sda2
</span></span><span class=line><span class=cl>│    xfs                f681192a-1cf2-4362-a74c-745374011700      1.8G     9% /boot
</span></span><span class=line><span class=cl>└─sda3
</span></span><span class=line><span class=cl>     LVM2_m LVM2        JEduSv-mV9g-tzdJ-sYEc-6piR-6nAo-48Srdh
</span></span><span class=line><span class=cl>  └─ubuntu--vg-root
</span></span><span class=line><span class=cl>     xfs                19cd87ec-9741-4295-9762-e87fb4f472c8     37.9G    21% /var/lib/kubelet/pods/ce7174f8-ad8f-4f42-8e58-2abca8c91424/volume-subpaths/tigera-ca-bundle/calico-node/1
</span></span><span class=line><span class=cl>                                                                              /
</span></span><span class=line><span class=cl>sdb
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# lsblk
</span></span><span class=line><span class=cl>NAME             MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
</span></span><span class=line><span class=cl>loop0              7:0    <span class=m>0</span>  63.4M  <span class=m>1</span> loop /snap/core20/1974
</span></span><span class=line><span class=cl>loop1              7:1    <span class=m>0</span>  63.5M  <span class=m>1</span> loop /snap/core20/2015
</span></span><span class=line><span class=cl>loop2              7:2    <span class=m>0</span> 111.9M  <span class=m>1</span> loop /snap/lxd/24322
</span></span><span class=line><span class=cl>loop3              7:3    <span class=m>0</span>  40.8M  <span class=m>1</span> loop /snap/snapd/20092
</span></span><span class=line><span class=cl>loop4              7:4    <span class=m>0</span>  40.9M  <span class=m>1</span> loop /snap/snapd/20290
</span></span><span class=line><span class=cl>sda                8:0    <span class=m>0</span>    50G  <span class=m>0</span> disk
</span></span><span class=line><span class=cl>├─sda1             8:1    <span class=m>0</span>     1M  <span class=m>0</span> part
</span></span><span class=line><span class=cl>├─sda2             8:2    <span class=m>0</span>     2G  <span class=m>0</span> part /boot
</span></span><span class=line><span class=cl>└─sda3             8:3    <span class=m>0</span>    48G  <span class=m>0</span> part
</span></span><span class=line><span class=cl>  └─ubuntu--vg-root
</span></span><span class=line><span class=cl>                 253:0    <span class=m>0</span>    48G  <span class=m>0</span> lvm  /var/lib/kubelet/pods/ce7174f8-ad8f-4f42-8e58-2abca8c91424/volume-subpaths/tigera-ca-bundle/calico-node/1
</span></span><span class=line><span class=cl>                                           /
</span></span><span class=line><span class=cl>sdb                8:16   <span class=m>0</span>    16G  <span class=m>0</span> disk
</span></span><span class=line><span class=cl>sr0               11:0    <span class=m>1</span>  1024M  <span class=m>0</span> rom
</span></span></code></pre></div><h2 id=ceph-cluster佈屬>ceph cluster佈屬</h2><h3 id=依序執行三個yaml>依序執行三個yaml</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>rook/deploy/examples/crds.yaml
</span></span><span class=line><span class=cl>rook/deploy/examples/common.yaml
</span></span><span class=line><span class=cl>rook/deploy/examples/operator.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# git clone --single-branch --branch v1.12.8 https://github.com/rook/rook.git
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# <span class=nb>cd</span> rook/deploy/examples
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f crds.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f common.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f operator.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 確認 rook-ceph-operator pod啟動完成</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get deployments.apps -n rook-ceph
</span></span><span class=line><span class=cl>NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span class=line><span class=cl>rook-ceph-operator   1/1     <span class=m>1</span>            <span class=m>1</span>           22s
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get pod -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                  READY   STATUS    RESTARTS   AGE
</span></span><span class=line><span class=cl>rook-ceph-operator-58775c8bdf-vmdkn   1/1     Running   <span class=m>0</span>          39s
</span></span></code></pre></div><h3 id=執行clusteryaml>執行cluster.yaml</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>cluster.yaml 分別調整
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl># 調度(使用節點親和性調度)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl># 角色資源
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl># 磁碟調度
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl># 關閉磁碟自動全部調度
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# vim cluster.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 角色調度(使用節點親和性調度)</span>
</span></span><span class=line><span class=cl>  placement:
</span></span><span class=line><span class=cl>    mon:
</span></span><span class=line><span class=cl>      nodeAffinity:
</span></span><span class=line><span class=cl>        requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          nodeSelectorTerms:
</span></span><span class=line><span class=cl>          - matchExpressions:
</span></span><span class=line><span class=cl>            - key: ceph-mon
</span></span><span class=line><span class=cl>              operator: In
</span></span><span class=line><span class=cl>              values:
</span></span><span class=line><span class=cl>              - enabled
</span></span><span class=line><span class=cl>      tolerations:
</span></span><span class=line><span class=cl>      - effect: NoSchedule
</span></span><span class=line><span class=cl>        operator: Exists
</span></span><span class=line><span class=cl>    mgr:
</span></span><span class=line><span class=cl>      nodeAffinity:
</span></span><span class=line><span class=cl>        requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          nodeSelectorTerms:
</span></span><span class=line><span class=cl>          - matchExpressions:
</span></span><span class=line><span class=cl>            - key: ceph-mgr
</span></span><span class=line><span class=cl>              operator: In
</span></span><span class=line><span class=cl>              values:
</span></span><span class=line><span class=cl>              - enabled
</span></span><span class=line><span class=cl>      tolerations:
</span></span><span class=line><span class=cl>      - effect: NoSchedule
</span></span><span class=line><span class=cl>        operator: Exists
</span></span><span class=line><span class=cl>    osd:
</span></span><span class=line><span class=cl>      nodeAffinity:
</span></span><span class=line><span class=cl>        requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          nodeSelectorTerms:
</span></span><span class=line><span class=cl>          - matchExpressions:
</span></span><span class=line><span class=cl>            - key: ceph-osd
</span></span><span class=line><span class=cl>              operator: In
</span></span><span class=line><span class=cl>              values:
</span></span><span class=line><span class=cl>              - enabled
</span></span><span class=line><span class=cl>      tolerations:
</span></span><span class=line><span class=cl>      - effect: NoSchedule
</span></span><span class=line><span class=cl>        operator: Exists
</span></span><span class=line><span class=cl>    prepareosd:
</span></span><span class=line><span class=cl>      nodeAffinity:
</span></span><span class=line><span class=cl>        requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          nodeSelectorTerms:
</span></span><span class=line><span class=cl>          - matchExpressions:
</span></span><span class=line><span class=cl>            - key: ceph-osd
</span></span><span class=line><span class=cl>              operator: In
</span></span><span class=line><span class=cl>              values:
</span></span><span class=line><span class=cl>              - enabled
</span></span><span class=line><span class=cl>      tolerations:
</span></span><span class=line><span class=cl>      - effect: NoSchedule
</span></span><span class=line><span class=cl>        operator: Exists
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 角色資源</span>
</span></span><span class=line><span class=cl>  resources:
</span></span><span class=line><span class=cl>    mon:
</span></span><span class=line><span class=cl>      limits:
</span></span><span class=line><span class=cl>        cpu: <span class=s2>&#34;1000m&#34;</span>
</span></span><span class=line><span class=cl>        memory: <span class=s2>&#34;2048Mi&#34;</span>
</span></span><span class=line><span class=cl>      requests:
</span></span><span class=line><span class=cl>        cpu: <span class=s2>&#34;1000m&#34;</span>
</span></span><span class=line><span class=cl>        memory: <span class=s2>&#34;2048Mi&#34;</span>
</span></span><span class=line><span class=cl>    mgr:
</span></span><span class=line><span class=cl>      limits:
</span></span><span class=line><span class=cl>        cpu: <span class=s2>&#34;1000m&#34;</span>
</span></span><span class=line><span class=cl>        memory: <span class=s2>&#34;1024Mi&#34;</span>
</span></span><span class=line><span class=cl>      requests:
</span></span><span class=line><span class=cl>        cpu: <span class=s2>&#34;1000m&#34;</span>
</span></span><span class=line><span class=cl>        memory: <span class=s2>&#34;1024Mi&#34;</span>
</span></span><span class=line><span class=cl>    osd:
</span></span><span class=line><span class=cl>      limits:
</span></span><span class=line><span class=cl>        cpu: <span class=s2>&#34;1000m&#34;</span>
</span></span><span class=line><span class=cl>        memory: <span class=s2>&#34;2048Mi&#34;</span>
</span></span><span class=line><span class=cl>      requests:
</span></span><span class=line><span class=cl>        cpu: <span class=s2>&#34;1000m&#34;</span>
</span></span><span class=line><span class=cl>        memory: <span class=s2>&#34;2048Mi&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 磁碟調度</span>
</span></span><span class=line><span class=cl>    nodes:
</span></span><span class=line><span class=cl>    - name: <span class=s2>&#34;k8s-master71u&#34;</span>
</span></span><span class=line><span class=cl>      devices:
</span></span><span class=line><span class=cl>      - name: <span class=s2>&#34;sdb&#34;</span>
</span></span><span class=line><span class=cl>        config:
</span></span><span class=line><span class=cl>          storeType: bluestore
</span></span><span class=line><span class=cl>          journalSizeMB: <span class=s2>&#34;4096&#34;</span>
</span></span><span class=line><span class=cl>    - name: <span class=s2>&#34;k8s-master72u&#34;</span>
</span></span><span class=line><span class=cl>      devices:
</span></span><span class=line><span class=cl>      - name: <span class=s2>&#34;sdb&#34;</span>
</span></span><span class=line><span class=cl>        config:
</span></span><span class=line><span class=cl>          storeType: bluestore
</span></span><span class=line><span class=cl>          journalSizeMB: <span class=s2>&#34;4096&#34;</span>
</span></span><span class=line><span class=cl>    - name: <span class=s2>&#34;k8s-master73u&#34;</span>
</span></span><span class=line><span class=cl>      devices:
</span></span><span class=line><span class=cl>      - name: <span class=s2>&#34;sdb&#34;</span>
</span></span><span class=line><span class=cl>        config:
</span></span><span class=line><span class=cl>          storeType: bluestore
</span></span><span class=line><span class=cl>          journalSizeMB: <span class=s2>&#34;4096&#34;</span>
</span></span><span class=line><span class=cl>    - name: <span class=s2>&#34;k8s-node75u&#34;</span>
</span></span><span class=line><span class=cl>      devices:
</span></span><span class=line><span class=cl>      - name: <span class=s2>&#34;sdb&#34;</span>
</span></span><span class=line><span class=cl>        config:
</span></span><span class=line><span class=cl>          storeType: bluestore
</span></span><span class=line><span class=cl>          journalSizeMB: <span class=s2>&#34;4096&#34;</span>
</span></span><span class=line><span class=cl>    - name: <span class=s2>&#34;k8s-node76u&#34;</span>
</span></span><span class=line><span class=cl>      devices:
</span></span><span class=line><span class=cl>      - name: <span class=s2>&#34;sdb&#34;</span>
</span></span><span class=line><span class=cl>        config:
</span></span><span class=line><span class=cl>          storeType: bluestore
</span></span><span class=line><span class=cl>          journalSizeMB: <span class=s2>&#34;4096&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 關閉磁碟自動全部調度</span>
</span></span><span class=line><span class=cl>  storage: <span class=c1># cluster level storage configuration and selection</span>
</span></span><span class=line><span class=cl>    <span class=c1>#設置磁碟的參數，調整為false，方便後面訂製</span>
</span></span><span class=line><span class=cl>    useAllNodes: <span class=nb>false</span>
</span></span><span class=line><span class=cl>    useAllDevices: <span class=nb>false</span>
</span></span></code></pre></div><h3 id=主機打標籤>主機打標籤</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master71u ceph-mon<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master72u ceph-mon<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master73u ceph-mon<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master71u ceph-mgr<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master72u ceph-mgr<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master73u ceph-mgr<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master71u ceph-osd<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master72u ceph-osd<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master73u ceph-osd<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-node75u ceph-osd<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-node76u ceph-osd<span class=o>=</span>enabled
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f cluster.yaml
</span></span><span class=line><span class=cl>cephcluster.ceph.rook.io/rook-ceph created
</span></span></code></pre></div><h3 id=確認佈屬結果pending狀況發生>確認佈屬結果(Pending狀況發生)</h3><p><font color=red>全部節點重開機就可以</font></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get pod -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                            READY   STATUS      RESTARTS   AGE
</span></span><span class=line><span class=cl>csi-cephfsplugin-dfz59                          2/2     Running     <span class=m>0</span>          17s
</span></span><span class=line><span class=cl>csi-cephfsplugin-provisioner-668dfcf95b-dgs5l   5/5     Running     <span class=m>0</span>          17s
</span></span><span class=line><span class=cl>csi-cephfsplugin-provisioner-668dfcf95b-wlz6r   5/5     Running     <span class=m>0</span>          17s
</span></span><span class=line><span class=cl>csi-cephfsplugin-xqdvc                          2/2     Running     <span class=m>0</span>          17s
</span></span><span class=line><span class=cl>csi-rbdplugin-l294v                             2/2     Running     <span class=m>0</span>          17s
</span></span><span class=line><span class=cl>csi-rbdplugin-lts27                             2/2     Running     <span class=m>0</span>          17s
</span></span><span class=line><span class=cl>csi-rbdplugin-provisioner-5b78f67bbb-7gz4x      5/5     Running     <span class=m>0</span>          17s
</span></span><span class=line><span class=cl>csi-rbdplugin-provisioner-5b78f67bbb-8w47k      5/5     Running     <span class=m>0</span>          17s
</span></span><span class=line><span class=cl>rook-ceph-csi-detect-version-tg598              0/1     Completed   <span class=m>0</span>          20s
</span></span><span class=line><span class=cl>rook-ceph-detect-version-z52cj                  0/1     Completed   <span class=m>0</span>          20s
</span></span><span class=line><span class=cl>rook-ceph-mon-a-6b8f546466-h5rp2                0/2     Pending     <span class=m>0</span>          9s
</span></span><span class=line><span class=cl>rook-ceph-mon-a-canary-5554b85b75-wf78h         2/2     Running     <span class=m>0</span>          12s
</span></span><span class=line><span class=cl>rook-ceph-mon-b-canary-7569748cf6-cr59z         2/2     Running     <span class=m>0</span>          12s
</span></span><span class=line><span class=cl>rook-ceph-mon-c-canary-85d88b48d5-649pv         2/2     Running     <span class=m>0</span>          12s
</span></span><span class=line><span class=cl>rook-ceph-operator-6bfb456b57-297f8             1/1     Running     <span class=m>0</span>          31s
</span></span></code></pre></div><p>無法調度</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl describe pod rook-ceph-mon-a-6b8f546466-h5rp2 -n rook-ceph
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Node-Selectors:              kubernetes.io/hostname<span class=o>=</span>k8s-master71u
</span></span><span class=line><span class=cl>Tolerations:                 :NoSchedule <span class=nv>op</span><span class=o>=</span>Exists
</span></span><span class=line><span class=cl>                             node.kubernetes.io/not-ready:NoExecute <span class=nv>op</span><span class=o>=</span>Exists <span class=k>for</span> 300s
</span></span><span class=line><span class=cl>                             node.kubernetes.io/unreachable:NoExecute <span class=nv>op</span><span class=o>=</span>Exists <span class=k>for</span> 300s
</span></span><span class=line><span class=cl>Events:
</span></span><span class=line><span class=cl>  Type     Reason            Age   From               Message
</span></span><span class=line><span class=cl>  ----     ------            ----  ----               -------
</span></span><span class=line><span class=cl>  Warning  FailedScheduling  81s   default-scheduler  0/5 nodes are available: <span class=m>1</span> node<span class=o>(</span>s<span class=o>)</span> didn<span class=s1>&#39;t satisfy existing pods anti-affinity rules, 4 node(s) didn&#39;</span>t match Pod<span class=err>&#39;</span>s node affinity/selector. preemption: 0/5 nodes are available: <span class=m>1</span> No preemption victims found <span class=k>for</span> incoming pod, <span class=m>4</span> Preemption is not helpful <span class=k>for</span> scheduling..
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# kubectl get pod -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                                      READY   STATUS      RESTARTS         AGE
</span></span><span class=line><span class=cl>csi-cephfsplugin-bk8gt                                    2/2     Running     <span class=m>4</span> <span class=o>(</span>5m36s ago<span class=o>)</span>    18m
</span></span><span class=line><span class=cl>csi-cephfsplugin-provisioner-668dfcf95b-858rd             5/5     Running     <span class=m>10</span> <span class=o>(</span>5m37s ago<span class=o>)</span>   18m
</span></span><span class=line><span class=cl>csi-cephfsplugin-provisioner-668dfcf95b-n4wdx             5/5     Running     <span class=m>10</span> <span class=o>(</span>5m36s ago<span class=o>)</span>   18m
</span></span><span class=line><span class=cl>csi-cephfsplugin-rp5nh                                    2/2     Running     <span class=m>4</span> <span class=o>(</span>5m37s ago<span class=o>)</span>    18m
</span></span><span class=line><span class=cl>csi-rbdplugin-provisioner-5b78f67bbb-4vrtb                5/5     Running     <span class=m>10</span> <span class=o>(</span>5m36s ago<span class=o>)</span>   18m
</span></span><span class=line><span class=cl>csi-rbdplugin-provisioner-5b78f67bbb-m5789                5/5     Running     <span class=m>10</span> <span class=o>(</span>5m36s ago<span class=o>)</span>   18m
</span></span><span class=line><span class=cl>csi-rbdplugin-qfrtk                                       2/2     Running     <span class=m>4</span> <span class=o>(</span>5m36s ago<span class=o>)</span>    18m
</span></span><span class=line><span class=cl>csi-rbdplugin-z68hx                                       2/2     Running     <span class=m>4</span> <span class=o>(</span>5m36s ago<span class=o>)</span>    18m
</span></span><span class=line><span class=cl>rook-ceph-crashcollector-k8s-master71u-55fcdbd66c-k6pkj   1/1     Running     <span class=m>0</span>                2m13s
</span></span><span class=line><span class=cl>rook-ceph-crashcollector-k8s-master72u-675646bf64-hnpw9   1/1     Running     <span class=m>0</span>                2m12s
</span></span><span class=line><span class=cl>rook-ceph-crashcollector-k8s-master73u-84fdc469f7-6khkr   1/1     Running     <span class=m>0</span>                2m42s
</span></span><span class=line><span class=cl>rook-ceph-crashcollector-k8s-node75u-bbc794dd9-5zbhs      1/1     Running     <span class=m>0</span>                15s
</span></span><span class=line><span class=cl>rook-ceph-crashcollector-k8s-node76u-5fd59cd68b-b9pxq     1/1     Running     <span class=m>0</span>                24s
</span></span><span class=line><span class=cl>rook-ceph-csi-detect-version-pqgj7                        0/1     Completed   <span class=m>0</span>                3m52s
</span></span><span class=line><span class=cl>rook-ceph-detect-version-rtgsq                            0/1     Completed   <span class=m>0</span>                13s
</span></span><span class=line><span class=cl>rook-ceph-mgr-a-767b9fc956-xm4ns                          3/3     Running     <span class=m>0</span>                2m49s
</span></span><span class=line><span class=cl>rook-ceph-mgr-b-8956fb9c-rvjff                            3/3     Running     <span class=m>0</span>                2m48s
</span></span><span class=line><span class=cl>rook-ceph-mon-a-769fd77b5b-trw4f                          2/2     Running     <span class=m>0</span>                9m7s
</span></span><span class=line><span class=cl>rook-ceph-mon-b-65cf769696-7dncd                          2/2     Running     <span class=m>0</span>                3m14s
</span></span><span class=line><span class=cl>rook-ceph-mon-c-58664986b-vqrzw                           2/2     Running     <span class=m>0</span>                3m2s
</span></span><span class=line><span class=cl>rook-ceph-operator-58775c8bdf-mfp6n                       1/1     Running     <span class=m>2</span> <span class=o>(</span>5m36s ago<span class=o>)</span>    23m
</span></span><span class=line><span class=cl>rook-ceph-osd-0-55f6d9b-xrjb6                             2/2     Running     <span class=m>0</span>                2m13s
</span></span><span class=line><span class=cl>rook-ceph-osd-1-7db86764b-2fn48                           2/2     Running     <span class=m>0</span>                2m12s
</span></span><span class=line><span class=cl>rook-ceph-osd-2-84cd64ffcb-nxrkx                          2/2     Running     <span class=m>0</span>                2m10s
</span></span><span class=line><span class=cl>rook-ceph-osd-3-748c78f4dd-whsxk                          2/2     Running     <span class=m>0</span>                24s
</span></span><span class=line><span class=cl>rook-ceph-osd-4-65c79df6dd-2w7z9                          1/2     Running     <span class=m>0</span>                15s
</span></span><span class=line><span class=cl>rook-ceph-osd-prepare-k8s-master71u-shwfs                 0/1     Completed   <span class=m>0</span>                2m26s
</span></span><span class=line><span class=cl>rook-ceph-osd-prepare-k8s-master72u-z4vbt                 0/1     Completed   <span class=m>0</span>                2m26s
</span></span><span class=line><span class=cl>rook-ceph-osd-prepare-k8s-master73u-rgxtn                 0/1     Completed   <span class=m>0</span>                2m25s
</span></span><span class=line><span class=cl>rook-ceph-osd-prepare-k8s-node75u-sdm5n                   0/1     Completed   <span class=m>0</span>                2m24s
</span></span><span class=line><span class=cl>rook-ceph-osd-prepare-k8s-node76u-l8dlp                   0/1     Completed   <span class=m>0</span>                2m23s
</span></span></code></pre></div><h3 id=執行toolboxyaml>執行toolbox.yaml</h3><p>創建工具POD</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f toolbox.yaml
</span></span><span class=line><span class=cl>deployment.apps/rook-ceph-tools created
</span></span></code></pre></div><p>可以看到ceph集群狀況</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl <span class=nb>exec</span> -it rook-ceph-tools-7cd4cd9c9c-kd8pf -n rook-ceph -- /bin/bash
</span></span><span class=line><span class=cl>bash-4.4$ ceph -s
</span></span><span class=line><span class=cl>  cluster:
</span></span><span class=line><span class=cl>    id:     b5e028e3-725d-4a26-8a14-17fe4dead850
</span></span><span class=line><span class=cl>    health: HEALTH_OK
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  services:
</span></span><span class=line><span class=cl>    mon: <span class=m>3</span> daemons, quorum a,b,c <span class=o>(</span>age 5m<span class=o>)</span>
</span></span><span class=line><span class=cl>    mgr: a<span class=o>(</span>active, since 86s<span class=o>)</span>, standbys: b
</span></span><span class=line><span class=cl>    osd: <span class=m>5</span> osds: <span class=m>5</span> up <span class=o>(</span>since 2m<span class=o>)</span>, <span class=m>5</span> in <span class=o>(</span>since 3m<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  data:
</span></span><span class=line><span class=cl>    pools:   <span class=m>1</span> pools, <span class=m>1</span> pgs
</span></span><span class=line><span class=cl>    objects: <span class=m>2</span> objects, <span class=m>449</span> KiB
</span></span><span class=line><span class=cl>    usage:   <span class=m>41</span> MiB used, <span class=m>80</span> GiB / <span class=m>80</span> GiB avail
</span></span><span class=line><span class=cl>    pgs:     <span class=m>1</span> active+clean
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash-4.4$ ceph status
</span></span><span class=line><span class=cl>  cluster:
</span></span><span class=line><span class=cl>    id:     b5e028e3-725d-4a26-8a14-17fe4dead850
</span></span><span class=line><span class=cl>    health: HEALTH_OK
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  services:
</span></span><span class=line><span class=cl>    mon: <span class=m>3</span> daemons, quorum a,b,c <span class=o>(</span>age 6m<span class=o>)</span>
</span></span><span class=line><span class=cl>    mgr: a<span class=o>(</span>active, since 116s<span class=o>)</span>, standbys: b
</span></span><span class=line><span class=cl>    osd: <span class=m>5</span> osds: <span class=m>5</span> up <span class=o>(</span>since 3m<span class=o>)</span>, <span class=m>5</span> in <span class=o>(</span>since 3m<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  data:
</span></span><span class=line><span class=cl>    pools:   <span class=m>1</span> pools, <span class=m>1</span> pgs
</span></span><span class=line><span class=cl>    objects: <span class=m>2</span> objects, <span class=m>449</span> KiB
</span></span><span class=line><span class=cl>    usage:   <span class=m>41</span> MiB used, <span class=m>80</span> GiB / <span class=m>80</span> GiB avail
</span></span><span class=line><span class=cl>    pgs:     <span class=m>1</span> active+clean
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash-4.4$ ceph osd status
</span></span><span class=line><span class=cl>ID  HOST            USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE
</span></span><span class=line><span class=cl> <span class=m>0</span>  k8s-master71u  8368k  15.9G      <span class=m>0</span>        <span class=m>0</span>       <span class=m>0</span>        <span class=m>0</span>   exists,up
</span></span><span class=line><span class=cl> <span class=m>1</span>  k8s-master72u  8820k  15.9G      <span class=m>0</span>        <span class=m>0</span>       <span class=m>0</span>        <span class=m>0</span>   exists,up
</span></span><span class=line><span class=cl> <span class=m>2</span>  k8s-master73u  8820k  15.9G      <span class=m>0</span>        <span class=m>0</span>       <span class=m>0</span>        <span class=m>0</span>   exists,up
</span></span><span class=line><span class=cl> <span class=m>3</span>  k8s-node76u    8432k  15.9G      <span class=m>0</span>        <span class=m>0</span>       <span class=m>0</span>        <span class=m>0</span>   exists,up
</span></span><span class=line><span class=cl> <span class=m>4</span>  k8s-node75u    7920k  15.9G      <span class=m>0</span>        <span class=m>0</span>       <span class=m>0</span>        <span class=m>0</span>   exists,up
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash-4.4$ ceph df
</span></span><span class=line><span class=cl>--- RAW STORAGE ---
</span></span><span class=line><span class=cl>CLASS    SIZE   AVAIL    USED  RAW USED  %RAW USED
</span></span><span class=line><span class=cl>ssd    <span class=m>80</span> GiB  <span class=m>80</span> GiB  <span class=m>41</span> MiB    <span class=m>41</span> MiB       0.05
</span></span><span class=line><span class=cl>TOTAL  <span class=m>80</span> GiB  <span class=m>80</span> GiB  <span class=m>41</span> MiB    <span class=m>41</span> MiB       0.05
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>--- POOLS ---
</span></span><span class=line><span class=cl>POOL  ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
</span></span><span class=line><span class=cl>.mgr   <span class=m>1</span>    <span class=m>1</span>  <span class=m>449</span> KiB        <span class=m>2</span>  <span class=m>449</span> KiB      <span class=m>0</span>     <span class=m>25</span> GiB
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash-4.4$ rados df
</span></span><span class=line><span class=cl>POOL_NAME     USED  OBJECTS  CLONES  COPIES  MISSING_ON_PRIMARY  UNFOUND  DEGRADED  RD_OPS       RD  WR_OPS       WR  USED COMPR  UNDER COMPR
</span></span><span class=line><span class=cl>.mgr       <span class=m>449</span> KiB        <span class=m>2</span>       <span class=m>0</span>       <span class=m>6</span>                   <span class=m>0</span>        <span class=m>0</span>         <span class=m>0</span>     <span class=m>288</span>  <span class=m>494</span> KiB     <span class=m>153</span>  1.3 MiB         <span class=m>0</span> B          <span class=m>0</span> B
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>total_objects    <span class=m>2</span>
</span></span><span class=line><span class=cl>total_used       <span class=m>41</span> MiB
</span></span><span class=line><span class=cl>total_avail      <span class=m>80</span> GiB
</span></span><span class=line><span class=cl>total_space      <span class=m>80</span> GiB
</span></span></code></pre></div><h3 id=主機直接查看cephceph-common安裝>主機直接查看ceph(ceph-common安裝)</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 安裝ceph client工具</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# apt update
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# apt install ceph-common
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 將連線設定，複製出來</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl <span class=nb>exec</span> -it rook-ceph-tools-7cd4cd9c9c-kd8pf -n rook-ceph -- /bin/bash
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash-4.4$ cat /etc/ceph/ceph.conf
</span></span><span class=line><span class=cl><span class=o>[</span>global<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>mon_host</span> <span class=o>=</span> 10.245.146.85:6789,10.245.203.29:6789,10.245.216.126:6789
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>[</span>client.admin<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>keyring</span> <span class=o>=</span> /etc/ceph/keyring
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash-4.4$ cat /etc/ceph/keyring
</span></span><span class=line><span class=cl><span class=o>[</span>client.admin<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>key</span> <span class=o>=</span> <span class=nv>AQA8LmZlDA21GhAAvgMoEOkXur9olDYtGkF8kQ</span><span class=o>==</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 主機創建連線設定</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# vim /etc/ceph/ceph.conf
</span></span><span class=line><span class=cl><span class=o>[</span>global<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>mon_host</span> <span class=o>=</span> 10.245.146.85:6789,10.245.203.29:6789,10.245.216.126:6789
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>[</span>client.admin<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>keyring</span> <span class=o>=</span> /etc/ceph/keyring
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# vim /etc/ceph/keyring
</span></span><span class=line><span class=cl><span class=o>[</span>client.admin<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>key</span> <span class=o>=</span> <span class=nv>AQA8LmZlDA21GhAAvgMoEOkXur9olDYtGkF8kQ</span><span class=o>==</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 主機直接連接ceph管理</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# ceph -s
</span></span><span class=line><span class=cl>  cluster:
</span></span><span class=line><span class=cl>    id:     b5e028e3-725d-4a26-8a14-17fe4dead850
</span></span><span class=line><span class=cl>    health: HEALTH_OK
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  services:
</span></span><span class=line><span class=cl>    mon: <span class=m>3</span> daemons, quorum a,b,c <span class=o>(</span>age 2m<span class=o>)</span>
</span></span><span class=line><span class=cl>    mgr: a<span class=o>(</span>active, since 2m<span class=o>)</span>, standbys: b
</span></span><span class=line><span class=cl>    osd: <span class=m>5</span> osds: <span class=m>5</span> up <span class=o>(</span>since 2m<span class=o>)</span>, <span class=m>5</span> in <span class=o>(</span>since 10m<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  data:
</span></span><span class=line><span class=cl>    pools:   <span class=m>1</span> pools, <span class=m>1</span> pgs
</span></span><span class=line><span class=cl>    objects: <span class=m>2</span> objects, <span class=m>449</span> KiB
</span></span><span class=line><span class=cl>    usage:   <span class=m>39</span> MiB used, <span class=m>80</span> GiB / <span class=m>80</span> GiB avail
</span></span><span class=line><span class=cl>    pgs:     <span class=m>1</span> active+clean
</span></span></code></pre></div><h2 id=cephfs佈屬>cephfs佈屬</h2><h3 id=主機打標籤-1>主機打標籤</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master71u ceph-mds<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master72u ceph-mds<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master73u ceph-mds<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-node75u ceph-mds<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-node76u ceph-mds<span class=o>=</span>enabled
</span></span></code></pre></div><h3 id=執行filesystemyaml>執行filesystem.yaml</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>filesystem.yaml 分別調整
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl># activeCount: 2 (2 active、2 stanby)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl># 調度(使用節點親和性調度) -&gt; nodeAffinity
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl># POD反親和性 -&gt; podAntiAffinity
</span></span><span class=line><span class=cl>	防止同一台主機上，有兩個mds
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# vim filesystem.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    activeCount: <span class=m>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    placement:
</span></span><span class=line><span class=cl>      nodeAffinity:
</span></span><span class=line><span class=cl>        requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          nodeSelectorTerms:
</span></span><span class=line><span class=cl>          - matchExpressions:
</span></span><span class=line><span class=cl>            - key: ceph-mds
</span></span><span class=line><span class=cl>              operator: In
</span></span><span class=line><span class=cl>              values:
</span></span><span class=line><span class=cl>              - enabled
</span></span><span class=line><span class=cl>      tolerations:
</span></span><span class=line><span class=cl>      - effect: NoSchedule
</span></span><span class=line><span class=cl>        operator: Exists
</span></span><span class=line><span class=cl>      podAntiAffinity:
</span></span><span class=line><span class=cl>        requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          - labelSelector:
</span></span><span class=line><span class=cl>              matchExpressions:
</span></span><span class=line><span class=cl>                - key: app
</span></span><span class=line><span class=cl>                  operator: In
</span></span><span class=line><span class=cl>                  values:
</span></span><span class=line><span class=cl>                    - rook-ceph-mds
</span></span><span class=line><span class=cl>            topologyKey: kubernetes.io/hostname
</span></span><span class=line><span class=cl>        preferredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          - weight: <span class=m>100</span>
</span></span><span class=line><span class=cl>            podAffinityTerm:
</span></span><span class=line><span class=cl>              labelSelector:
</span></span><span class=line><span class=cl>                matchExpressions:
</span></span><span class=line><span class=cl>                  - key: app
</span></span><span class=line><span class=cl>                    operator: In
</span></span><span class=line><span class=cl>                    values:
</span></span><span class=line><span class=cl>                      - rook-ceph-mds
</span></span><span class=line><span class=cl>              topologyKey: topology.kubernetes.io/zone
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f filesystem.yaml
</span></span></code></pre></div><h3 id=確認佈屬結果>確認佈屬結果</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get pod -n rook-ceph -o wide <span class=p>|</span> grep -i rook-ceph-mds
</span></span><span class=line><span class=cl>rook-ceph-mds-myfs-a-656c99cf6b-mp7bq                     2/2     Running     <span class=m>0</span>              2m37s   10.244.96.33     k8s-master73u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>rook-ceph-mds-myfs-b-65bf4fdbcd-j9gzt                     2/2     Running     <span class=m>0</span>              2m36s   10.244.14.186    k8s-node75u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>rook-ceph-mds-myfs-c-b7bd7658-tt845                       2/2     Running     <span class=m>0</span>              2m34s   10.244.255.213   k8s-node76u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>rook-ceph-mds-myfs-d-dff64fb98-zb4rv                      2/2     Running     <span class=m>0</span>              2m31s   10.244.133.17    k8s-master71u   &lt;none&gt;           &lt;none&gt;
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# ceph -s
</span></span><span class=line><span class=cl>  cluster:
</span></span><span class=line><span class=cl>    id:     b5e028e3-725d-4a26-8a14-17fe4dead850
</span></span><span class=line><span class=cl>    health: HEALTH_OK
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  services:
</span></span><span class=line><span class=cl>    mon: <span class=m>3</span> daemons, quorum a,b,c <span class=o>(</span>age 11m<span class=o>)</span>
</span></span><span class=line><span class=cl>    mgr: a<span class=o>(</span>active, since 12m<span class=o>)</span>, standbys: b
</span></span><span class=line><span class=cl>    mds: 2/2 daemons up, <span class=m>2</span> hot standby
</span></span><span class=line><span class=cl>    osd: <span class=m>5</span> osds: <span class=m>5</span> up <span class=o>(</span>since 12m<span class=o>)</span>, <span class=m>5</span> in <span class=o>(</span>since 19m<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  data:
</span></span><span class=line><span class=cl>    volumes: 1/1 healthy
</span></span><span class=line><span class=cl>    pools:   <span class=m>3</span> pools, <span class=m>49</span> pgs
</span></span><span class=line><span class=cl>    objects: <span class=m>45</span> objects, <span class=m>452</span> KiB
</span></span><span class=line><span class=cl>    usage:   <span class=m>46</span> MiB used, <span class=m>80</span> GiB / <span class=m>80</span> GiB avail
</span></span><span class=line><span class=cl>    pgs:     2.041% pgs not active
</span></span><span class=line><span class=cl>             <span class=m>48</span> active+clean
</span></span><span class=line><span class=cl>             <span class=m>1</span>  peering
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  io:
</span></span><span class=line><span class=cl>    client:   1.5 KiB/s rd, <span class=m>3</span> op/s rd, <span class=m>0</span> op/s wr
</span></span><span class=line><span class=cl>    recovery: <span class=m>95</span> B/s, <span class=m>0</span> objects/s
</span></span></code></pre></div><h3 id=創建storageclassstorageclassyaml>創建storageclass(storageclass.yaml)</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/csi/cephfs# kubectl create -f storageclass.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/csi/cephfs# kubectl get storageclasses.storage.k8s.io
</span></span><span class=line><span class=cl>NAME          PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
</span></span><span class=line><span class=cl>rook-cephfs   rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           <span class=nb>true</span>                   8s
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/csi/cephfs# ceph fs ls
</span></span><span class=line><span class=cl>name: myfs, metadata pool: myfs-metadata, data pools: <span class=o>[</span>myfs-replicated <span class=o>]</span>
</span></span></code></pre></div><h3 id=測試storageclassnamerook-cephfs>測試storageClassName(rook-cephfs)</h3><p>申請PVC，指定storageClassName: &ldquo;rook-cephfs&rdquo;</p><p>實現自動提供掛載空間</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# cat sc.yaml
</span></span><span class=line><span class=cl>apiVersion: apps/v1
</span></span><span class=line><span class=cl>kind: Deployment
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>  name: web-sc
</span></span><span class=line><span class=cl>spec:
</span></span><span class=line><span class=cl>  selector:
</span></span><span class=line><span class=cl>    matchLabels:
</span></span><span class=line><span class=cl>      app: nginx
</span></span><span class=line><span class=cl>  replicas: <span class=m>3</span>
</span></span><span class=line><span class=cl>  template:
</span></span><span class=line><span class=cl>    metadata:
</span></span><span class=line><span class=cl>      labels:
</span></span><span class=line><span class=cl>        app: nginx
</span></span><span class=line><span class=cl>    spec:
</span></span><span class=line><span class=cl>      containers:
</span></span><span class=line><span class=cl>      - name: nginx
</span></span><span class=line><span class=cl>        image: nginx
</span></span><span class=line><span class=cl>        volumeMounts:
</span></span><span class=line><span class=cl>          - name: wwwroot
</span></span><span class=line><span class=cl>            mountPath: /usr/share/nginx/html
</span></span><span class=line><span class=cl>      volumes:
</span></span><span class=line><span class=cl>      - name: wwwroot
</span></span><span class=line><span class=cl>        persistentVolumeClaim:
</span></span><span class=line><span class=cl>          claimName: web-sc
</span></span><span class=line><span class=cl>---
</span></span><span class=line><span class=cl>apiVersion: v1
</span></span><span class=line><span class=cl>kind: PersistentVolumeClaim
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>  name: web-sc
</span></span><span class=line><span class=cl>spec:
</span></span><span class=line><span class=cl>  storageClassName: <span class=s2>&#34;rook-cephfs&#34;</span>
</span></span><span class=line><span class=cl>  accessModes:
</span></span><span class=line><span class=cl>    - ReadWriteMany
</span></span><span class=line><span class=cl>  resources:
</span></span><span class=line><span class=cl>    requests:
</span></span><span class=line><span class=cl>      storage: 5Gi
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# kubectl create -f sc.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl get pod
</span></span><span class=line><span class=cl>NAME                      READY   STATUS    RESTARTS      AGE
</span></span><span class=line><span class=cl>test-nginx                1/1     Running   <span class=m>5</span> <span class=o>(</span>33m ago<span class=o>)</span>   69d
</span></span><span class=line><span class=cl>web-sc-7b6c54fbb9-8q254   1/1     Running   <span class=m>0</span>             46s
</span></span><span class=line><span class=cl>web-sc-7b6c54fbb9-hkw2z   1/1     Running   <span class=m>0</span>             46s
</span></span><span class=line><span class=cl>web-sc-7b6c54fbb9-m4qlf   1/1     Running   <span class=m>0</span>             46s
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl get pv
</span></span><span class=line><span class=cl>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE
</span></span><span class=line><span class=cl>pvc-ea9ac011-5fd5-4436-ad61-468e9ce95239   5Gi        RWX            Delete           Bound    default/web-sc   rook-cephfs             51s
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl get pvc
</span></span><span class=line><span class=cl>NAME     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span class=line><span class=cl>web-sc   Bound    pvc-ea9ac011-5fd5-4436-ad61-468e9ce95239   5Gi        RWX            rook-cephfs    54s
</span></span></code></pre></div><p>測試兩個POD確實掛載了相同位置</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# kubectl <span class=nb>exec</span> -ti web-sc-7b6c54fbb9-8q254 -- /bin/bash
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-8q254:/#
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-8q254:/# df -h
</span></span><span class=line><span class=cl>Filesystem                                                                                                                                                Size  Used Avail Use% Mounted on
</span></span><span class=line><span class=cl>10.245.146.85:6789,10.245.203.29:6789,10.245.216.126:6789:/volumes/csi/csi-vol-d7642fd0-5da4-4599-b69e-bc0c1b948ded/dccf324f-8ad6-40f3-9e99-6186a32d3faf  5.0G     <span class=m>0</span>  5.0G   0% /usr/share/nginx/html
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-8q254:/# <span class=nb>cd</span> /usr/share/nginx/html/
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-8q254:/usr/share/nginx/html# touch <span class=m>123</span>
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-8q254:/usr/share/nginx/html# ls
</span></span><span class=line><span class=cl><span class=m>123</span>
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-8q254:/usr/share/nginx/html# <span class=nb>exit</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl <span class=nb>exec</span> -ti web-sc-7b6c54fbb9-hkw2z -- /bin/bash
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-hkw2z:/# df -h
</span></span><span class=line><span class=cl>Filesystem                                                                                                                                                Size  Used Avail Use% Mounted on
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>10.245.146.85:6789,10.245.203.29:6789,10.245.216.126:6789:/volumes/csi/csi-vol-d7642fd0-5da4-4599-b69e-bc0c1b948ded/dccf324f-8ad6-40f3-9e99-6186a32d3faf  5.0G     <span class=m>0</span>  5.0G   0% /usr/share/nginx/html
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-hkw2z:/# <span class=nb>cd</span> /usr/share/nginx/html/
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-hkw2z:/usr/share/nginx/html# ls
</span></span><span class=line><span class=cl><span class=m>123</span>
</span></span></code></pre></div><h2 id=ceph管理平台>Ceph管理平台</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get services -n rook-ceph
</span></span><span class=line><span class=cl>NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>             AGE
</span></span><span class=line><span class=cl>rook-ceph-mgr             ClusterIP   10.245.33.129    &lt;none&gt;        9283/TCP            34m
</span></span><span class=line><span class=cl>rook-ceph-mgr-dashboard   ClusterIP   10.245.17.131    &lt;none&gt;        7000/TCP            34m
</span></span><span class=line><span class=cl>rook-ceph-mon-a           ClusterIP   10.245.146.85    &lt;none&gt;        6789/TCP,3300/TCP   41m
</span></span><span class=line><span class=cl>rook-ceph-mon-b           ClusterIP   10.245.203.29    &lt;none&gt;        6789/TCP,3300/TCP   35m
</span></span><span class=line><span class=cl>rook-ceph-mon-c           ClusterIP   10.245.216.126   &lt;none&gt;        6789/TCP,3300/TCP   35m
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f dashboard-external-https.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get services -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>             AGE
</span></span><span class=line><span class=cl>rook-ceph-mgr                            ClusterIP   10.245.33.129    &lt;none&gt;        9283/TCP            36m
</span></span><span class=line><span class=cl>rook-ceph-mgr-dashboard                  ClusterIP   10.245.17.131    &lt;none&gt;        7000/TCP            36m
</span></span><span class=line><span class=cl>rook-ceph-mgr-dashboard-external-https   NodePort    10.245.106.246   &lt;none&gt;        8443:30901/TCP      10s
</span></span><span class=line><span class=cl>rook-ceph-mon-a                          ClusterIP   10.245.146.85    &lt;none&gt;        6789/TCP,3300/TCP   43m
</span></span><span class=line><span class=cl>rook-ceph-mon-b                          ClusterIP   10.245.203.29    &lt;none&gt;        6789/TCP,3300/TCP   37m
</span></span><span class=line><span class=cl>rook-ceph-mon-c                          ClusterIP   10.245.216.126   &lt;none&gt;        6789/TCP,3300/TCP   37m
</span></span></code></pre></div><p>https://192.168.1.75:30901/#/login?returnUrl=%2Fdashboard</p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129030025.png width=895 height=556 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129030025_hufe507121e66ab48a7e7cb9da2387980a_27937_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129030025_hufe507121e66ab48a7e7cb9da2387980a_27937_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=160 data-flex-basis=386px></p><p>獲取密碼</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get secrets -n rook-ceph rook-ceph-dashboard-password -o yaml
</span></span><span class=line><span class=cl>apiVersion: v1
</span></span><span class=line><span class=cl>data:
</span></span><span class=line><span class=cl>  password: <span class=nv>Y0w5bG1Vb0QlTCpQViM2RFB3U3k</span><span class=o>=</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# <span class=nb>echo</span> <span class=s2>&#34;Y0w5bG1Vb0QlTCpQViM2RFB3U3k=&#34;</span> <span class=p>|</span> base64 -d
</span></span><span class=line><span class=cl>cL9lmUoD%L*PV#6DPwSy
</span></span></code></pre></div><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129030211.png width=770 height=579 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129030211_hu5cc736b502bda6979ebd445587d015ee_25678_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129030211_hu5cc736b502bda6979ebd445587d015ee_25678_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=132 data-flex-basis=319px></p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129035101.png width=1448 height=551 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129035101_hu902dc51f2cb745e9a49fdfbb29fe9200_92494_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129035101_hu902dc51f2cb745e9a49fdfbb29fe9200_92494_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=262 data-flex-basis=630px></p><p>更改密碼
<img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231130001900.png width=840 height=484 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231130001900_hu1ffbcc6acbf3c769c46025e1fc48d4c4_60177_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231130001900_hu1ffbcc6acbf3c769c46025e1fc48d4c4_60177_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=173 data-flex-basis=416px></p><p>一直報這個錯誤，導致集群檢查不健康
<img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231130010912.png width=1112 height=261 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231130010912_huca94f267da5be9db3bd5e717b7a2fdc1_41768_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231130010912_huca94f267da5be9db3bd5e717b7a2fdc1_41768_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=426 data-flex-basis=1022px></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>overall HEALTH_WARN <span class=m>1</span> mgr modules have recently crashed
</span></span></code></pre></div><p>查看報錯</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# ceph crash ls
</span></span><span class=line><span class=cl>ID                                                                ENTITY  NEW
</span></span><span class=line><span class=cl>2023-11-28T19:02:15.741903Z_cff323ca-e2d5-415c-b5e6-e518a4a2e892  mgr.a    *
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# ceph crash info 2023-11-28T19:02:15.741903Z_cff323ca-e2d5-415c-b5e6-e518a4a2e892
</span></span><span class=line><span class=cl><span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;backtrace&#34;</span>: <span class=o>[</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;  File \&#34;/usr/share/ceph/mgr/nfs/module.py\&#34;, line 169, in cluster_ls\n    return available_clusters(self)&#34;</span>,
</span></span><span class=line><span class=cl>        <span class=s2>&#34;  File \&#34;/usr/share/ceph/mgr/nfs/utils.py\&#34;, line 38, in available_clusters\n    completion = mgr.describe_service(service_type=&#39;nfs&#39;)&#34;</span>,
</span></span><span class=line><span class=cl>        <span class=s2>&#34;  File \&#34;/usr/share/ceph/mgr/orchestrator/_interface.py\&#34;, line 1488, in inner\n    completion = self._oremote(method_name, args, kwargs)&#34;</span>,
</span></span><span class=line><span class=cl>        <span class=s2>&#34;  File \&#34;/usr/share/ceph/mgr/orchestrator/_interface.py\&#34;, line 1555, in _oremote\n    raise NoOrchestrator()&#34;</span>,
</span></span><span class=line><span class=cl>        <span class=s2>&#34;orchestrator._interface.NoOrchestrator: No orchestrator configured (try `ceph orch set backend`)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=o>]</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;ceph_version&#34;</span>: <span class=s2>&#34;17.2.6&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;crash_id&#34;</span>: <span class=s2>&#34;2023-11-28T19:02:15.741903Z_cff323ca-e2d5-415c-b5e6-e518a4a2e892&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;entity_name&#34;</span>: <span class=s2>&#34;mgr.a&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;mgr_module&#34;</span>: <span class=s2>&#34;nfs&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;mgr_module_caller&#34;</span>: <span class=s2>&#34;ActivePyModule::dispatch_remote cluster_ls&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;mgr_python_exception&#34;</span>: <span class=s2>&#34;NoOrchestrator&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;os_id&#34;</span>: <span class=s2>&#34;centos&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;os_name&#34;</span>: <span class=s2>&#34;CentOS Stream&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;os_version&#34;</span>: <span class=s2>&#34;8&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;os_version_id&#34;</span>: <span class=s2>&#34;8&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;process_name&#34;</span>: <span class=s2>&#34;ceph-mgr&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;stack_sig&#34;</span>: <span class=s2>&#34;b01db59d356dd52f69bfb0b128a216e7606f54a60674c3c82711c23cf64832ce&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;timestamp&#34;</span>: <span class=s2>&#34;2023-11-28T19:02:15.741903Z&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;utsname_hostname&#34;</span>: <span class=s2>&#34;rook-ceph-mgr-a-767b9fc956-xm4ns&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;utsname_machine&#34;</span>: <span class=s2>&#34;x86_64&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;utsname_release&#34;</span>: <span class=s2>&#34;5.15.0-84-generic&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;utsname_sysname&#34;</span>: <span class=s2>&#34;Linux&#34;</span>,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;utsname_version&#34;</span>: <span class=s2>&#34;#93-Ubuntu SMP Tue Sep 5 17:16:10 UTC 2023&#34;</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></div><p>需啟用mgr rook的modules</p><p>參考網址： <a class=link href=https://github.com/rook/rook/issues/11316 target=_blank rel=noopener>https://github.com/rook/rook/issues/11316</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 預設rook沒有啟用</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# ceph mgr module ls
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# ceph mgr module <span class=nb>enable</span> rook
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# ceph mgr module ls
</span></span><span class=line><span class=cl>MODULE
</span></span><span class=line><span class=cl>balancer              on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>crash                 on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>devicehealth          on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>orchestrator          on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>pg_autoscaler         on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>progress              on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>rbd_support           on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>status                on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>telemetry             on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>volumes               on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>dashboard             on
</span></span><span class=line><span class=cl>iostat                on
</span></span><span class=line><span class=cl>nfs                   on
</span></span><span class=line><span class=cl>prometheus            on
</span></span><span class=line><span class=cl>restful               on
</span></span><span class=line><span class=cl>rook                  on
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# ceph crash archive 2023-11-28T19:02:15.741903Z_cff323ca-e2d5-415c-b5e6-e518a4a2e892
</span></span></code></pre></div><p>這樣集群檢查就會顯示健康</p><h3 id=查看mgr暴露的metric監控指標>查看mgr暴露的metric監控指標</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 服務改成 NodePort</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl edit services rook-ceph-mgr -n rook-ceph
</span></span><span class=line><span class=cl>  type: NodePort
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl get svc -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>             AGE
</span></span><span class=line><span class=cl>rook-ceph-mgr                            NodePort    10.245.33.129    &lt;none&gt;        9283:30602/TCP      48m
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 可以看到監控指標</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# curl http://10.245.33.129:9283/metrics
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># HELP ceph_purge_queue_pq_item_in_journal Purge item left in journal</span>
</span></span><span class=line><span class=cl><span class=c1># TYPE ceph_purge_queue_pq_item_in_journal gauge</span>
</span></span><span class=line><span class=cl>ceph_purge_queue_pq_item_in_journal<span class=o>{</span><span class=nv>ceph_daemon</span><span class=o>=</span><span class=s2>&#34;mds.myfs-d&#34;</span><span class=o>}</span> 0.0
</span></span><span class=line><span class=cl>ceph_purge_queue_pq_item_in_journal<span class=o>{</span><span class=nv>ceph_daemon</span><span class=o>=</span><span class=s2>&#34;mds.myfs-a&#34;</span><span class=o>}</span> 0.0
</span></span><span class=line><span class=cl>ceph_purge_queue_pq_item_in_journal<span class=o>{</span><span class=nv>ceph_daemon</span><span class=o>=</span><span class=s2>&#34;mds.myfs-b&#34;</span><span class=o>}</span> 0.0
</span></span><span class=line><span class=cl>ceph_purge_queue_pq_item_in_journal<span class=o>{</span><span class=nv>ceph_daemon</span><span class=o>=</span><span class=s2>&#34;mds.myfs-c&#34;</span><span class=o>}</span> 0.0
</span></span></code></pre></div><p>http://192.168.1.75:30602/metrics</p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129031249.png width=1442 height=674 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129031249_hu5e187ea5f63f89641c7771755bc26dad_274456_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129031249_hu5e187ea5f63f89641c7771755bc26dad_274456_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=213 data-flex-basis=513px></p><h2 id=安裝prometheus收集指標>安裝Prometheus收集指標</h2><h4 id=安裝prometheus-operator>安裝prometheus-operator</h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# wget https://raw.githubusercontent.com/coreos/prometheus-operator/v0.70.0/bundle.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl create -f bundle.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl get pod
</span></span><span class=line><span class=cl>NAME                                   READY   STATUS    RESTARTS      AGE
</span></span><span class=line><span class=cl>prometheus-operator-754cf6c978-wgn4w   1/1     Running   <span class=m>0</span>             23s
</span></span><span class=line><span class=cl>prometheus-operator-847b56864c-6gp95   1/1     Running   <span class=m>0</span>             5m55s
</span></span></code></pre></div><h4 id=安裝prometheus-server>安裝prometheus server</h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl create -f prometheus.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl create -f prometheus-service.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl create -f service-monitor.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl get pod -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                                      READY   STATUS      RESTARTS       AGE
</span></span><span class=line><span class=cl>prometheus-rook-prometheus-0                              2/2     Running     <span class=m>0</span>              63s
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl get services -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>             AGE
</span></span><span class=line><span class=cl>prometheus-operated                      ClusterIP   None             &lt;none&gt;        9090/TCP            88s
</span></span><span class=line><span class=cl>rook-ceph-mgr                            NodePort    10.245.33.129    &lt;none&gt;        9283:30602/TCP      53m
</span></span><span class=line><span class=cl>rook-ceph-mgr-dashboard                  ClusterIP   10.245.17.131    &lt;none&gt;        7000/TCP            53m
</span></span><span class=line><span class=cl>rook-ceph-mgr-dashboard-external-https   NodePort    10.245.106.246   &lt;none&gt;        8443:30901/TCP      17m
</span></span><span class=line><span class=cl>rook-ceph-mon-a                          ClusterIP   10.245.146.85    &lt;none&gt;        6789/TCP,3300/TCP   60m
</span></span><span class=line><span class=cl>rook-ceph-mon-b                          ClusterIP   10.245.203.29    &lt;none&gt;        6789/TCP,3300/TCP   54m
</span></span><span class=line><span class=cl>rook-ceph-mon-c                          ClusterIP   10.245.216.126   &lt;none&gt;        6789/TCP,3300/TCP   54m
</span></span><span class=line><span class=cl>rook-prometheus                          NodePort    10.245.174.189   &lt;none&gt;        9090:30900/TCP      77s
</span></span></code></pre></div><p>http://192.168.1.75:30900/graph</p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129031704.png width=737 height=504 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129031704_hu3d3d66b2b74cd2c013d9b7d93aea72d6_46635_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129031704_hu3d3d66b2b74cd2c013d9b7d93aea72d6_46635_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=146 data-flex-basis=350px></p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129031715.png width=1092 height=538 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129031715_hu6e6c777882a0e2051f1fc5a866fb2514_82515_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129031715_hu6e6c777882a0e2051f1fc5a866fb2514_82515_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=202 data-flex-basis=487px></p><h2 id=安裝grafana展示圖表>安裝Grafana展示圖表</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# vim grafana-all.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kind: PersistentVolumeClaim
</span></span><span class=line><span class=cl>apiVersion: v1
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>  name: grafana
</span></span><span class=line><span class=cl>  namespace: rook-ceph
</span></span><span class=line><span class=cl>spec:
</span></span><span class=line><span class=cl>  accessModes:
</span></span><span class=line><span class=cl>    - ReadWriteOnce
</span></span><span class=line><span class=cl>  storageClassName: rook-cephfs
</span></span><span class=line><span class=cl>  resources:
</span></span><span class=line><span class=cl>    requests:
</span></span><span class=line><span class=cl>      storage: 100Gi
</span></span><span class=line><span class=cl>---
</span></span><span class=line><span class=cl>apiVersion: apps/v1
</span></span><span class=line><span class=cl>kind: Deployment
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  name: grafana
</span></span><span class=line><span class=cl>  namespace: rook-ceph
</span></span><span class=line><span class=cl>spec:
</span></span><span class=line><span class=cl>  selector:
</span></span><span class=line><span class=cl>    matchLabels:
</span></span><span class=line><span class=cl>      app: grafana
</span></span><span class=line><span class=cl>  template:
</span></span><span class=line><span class=cl>    metadata:
</span></span><span class=line><span class=cl>      labels:
</span></span><span class=line><span class=cl>        app: grafana
</span></span><span class=line><span class=cl>    spec:
</span></span><span class=line><span class=cl>      volumes:
</span></span><span class=line><span class=cl>      - name: storage
</span></span><span class=line><span class=cl>        persistentVolumeClaim:
</span></span><span class=line><span class=cl>          claimName: grafana
</span></span><span class=line><span class=cl>      securityContext:
</span></span><span class=line><span class=cl>        runAsUser: <span class=m>0</span>
</span></span><span class=line><span class=cl>      containers:
</span></span><span class=line><span class=cl>      - name: grafana
</span></span><span class=line><span class=cl>        image: grafana/grafana:9.5.14
</span></span><span class=line><span class=cl>        imagePullPolicy: IfNotPresent
</span></span><span class=line><span class=cl>        ports:
</span></span><span class=line><span class=cl>        - containerPort: <span class=m>3000</span>
</span></span><span class=line><span class=cl>          name: grafana
</span></span><span class=line><span class=cl>        env:
</span></span><span class=line><span class=cl>        - name: GF_SECURITY_ADMIN_USER
</span></span><span class=line><span class=cl>          value: admin
</span></span><span class=line><span class=cl>        - name: GF_SECURITY_ADMIN_PASSWORD
</span></span><span class=line><span class=cl>          value: admin
</span></span><span class=line><span class=cl>        readinessProbe:
</span></span><span class=line><span class=cl>          failureThreshold: <span class=m>10</span>
</span></span><span class=line><span class=cl>          httpGet:
</span></span><span class=line><span class=cl>            path: /api/health
</span></span><span class=line><span class=cl>            port: <span class=m>3000</span>
</span></span><span class=line><span class=cl>            scheme: HTTP
</span></span><span class=line><span class=cl>          initialDelaySeconds: <span class=m>60</span>
</span></span><span class=line><span class=cl>          periodSeconds: <span class=m>10</span>
</span></span><span class=line><span class=cl>          successThreshold: <span class=m>1</span>
</span></span><span class=line><span class=cl>          timeoutSeconds: <span class=m>30</span>
</span></span><span class=line><span class=cl>        livenessProbe:
</span></span><span class=line><span class=cl>          failureThreshold: <span class=m>3</span>
</span></span><span class=line><span class=cl>          httpGet:
</span></span><span class=line><span class=cl>            path: /api/health
</span></span><span class=line><span class=cl>            port: <span class=m>3000</span>
</span></span><span class=line><span class=cl>            scheme: HTTP
</span></span><span class=line><span class=cl>          periodSeconds: <span class=m>10</span>
</span></span><span class=line><span class=cl>          successThreshold: <span class=m>1</span>
</span></span><span class=line><span class=cl>          timeoutSeconds: <span class=m>1</span>
</span></span><span class=line><span class=cl>        resources:
</span></span><span class=line><span class=cl>          limits:
</span></span><span class=line><span class=cl>            cpu: 150m
</span></span><span class=line><span class=cl>            memory: 512Mi
</span></span><span class=line><span class=cl>          requests:
</span></span><span class=line><span class=cl>            cpu: 150m
</span></span><span class=line><span class=cl>            memory: 512Mi
</span></span><span class=line><span class=cl>        volumeMounts:
</span></span><span class=line><span class=cl>        - mountPath: /var/lib/grafana
</span></span><span class=line><span class=cl>          name: storage
</span></span><span class=line><span class=cl>---
</span></span><span class=line><span class=cl>apiVersion: v1
</span></span><span class=line><span class=cl>kind: Service
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>  name: grafana
</span></span><span class=line><span class=cl>  namespace: rook-ceph
</span></span><span class=line><span class=cl>spec:
</span></span><span class=line><span class=cl>  type: NodePort
</span></span><span class=line><span class=cl>  ports:
</span></span><span class=line><span class=cl>    - port: <span class=m>3000</span>
</span></span><span class=line><span class=cl>  selector:
</span></span><span class=line><span class=cl>    app: grafana
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>---
</span></span><span class=line><span class=cl>apiVersion: networking.k8s.io/v1
</span></span><span class=line><span class=cl>kind: Ingress
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>  name: grafana
</span></span><span class=line><span class=cl>  namespace: rook-ceph
</span></span><span class=line><span class=cl>spec:
</span></span><span class=line><span class=cl>  ingressClassName: nginx
</span></span><span class=line><span class=cl>  rules:
</span></span><span class=line><span class=cl>  - host: grafana.jimmyhome.tw
</span></span><span class=line><span class=cl>    http:
</span></span><span class=line><span class=cl>      paths:
</span></span><span class=line><span class=cl>      - path: /
</span></span><span class=line><span class=cl>        pathType: Prefix
</span></span><span class=line><span class=cl>        backend:
</span></span><span class=line><span class=cl>          service:
</span></span><span class=line><span class=cl>            name: grafana
</span></span><span class=line><span class=cl>            port:
</span></span><span class=line><span class=cl>              number: <span class=m>3000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl create -f grafana-all.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl get pod -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                                      READY   STATUS      RESTARTS       AGE
</span></span><span class=line><span class=cl>grafana-654886fbff-wzx9n                                  0/1     Running     <span class=m>0</span>              42s
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl get svc,pvc -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>             AGE
</span></span><span class=line><span class=cl>service/grafana                                  NodePort    10.245.92.95     &lt;none&gt;        3000:31348/TCP      68s
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME                            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span class=line><span class=cl>persistentvolumeclaim/grafana   Bound    pvc-75e0cc6e-5130-4ab6-b447-0d19d95880f3   200Gi      RWO            rook-cephfs    69s
</span></span></code></pre></div><p>http://192.168.1.75:31348/login</p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129032738.png width=1043 height=755 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129032738_hudda5a2b7780cac5e07b36804ae763151_429646_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129032738_hudda5a2b7780cac5e07b36804ae763151_429646_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=138 data-flex-basis=331px></p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129032828.png width=1041 height=815 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129032828_hu3e30c24a2138532c17c5d5d206eeb5f9_106197_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129032828_hu3e30c24a2138532c17c5d5d206eeb5f9_106197_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=127 data-flex-basis=306px></p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129032923.png width=1054 height=646 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129032923_hu120378f266eaff630516552849427be9_66205_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129032923_hu120378f266eaff630516552849427be9_66205_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=163 data-flex-basis=391px></p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129032945.png width=1442 height=669 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129032945_hu5d0c8a00be6c0591a620efba06e3e206_110994_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129032945_hu5d0c8a00be6c0591a620efba06e3e206_110994_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=215 data-flex-basis=517px></p><p>填寫Prometheus service ip
<img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033201.png width=977 height=933 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033201_hu5e056007209b8a60982c4deeb9023e90_114179_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129033201_hu5e056007209b8a60982c4deeb9023e90_114179_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=104 data-flex-basis=251px></p><p>連線成功
<img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033216.png width=965 height=931 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033216_hu68e031f3409d496c5c0e74e866f39174_98080_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129033216_hu68e031f3409d496c5c0e74e866f39174_98080_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=103 data-flex-basis=248px></p><p>Dashboard導入</p><p><a class=link href=https://grafana.com/grafana/dashboards/2842-ceph-cluster/ target=_blank rel=noopener>https://grafana.com/grafana/dashboards/2842-ceph-cluster/</a>
<a class=link href=https://grafana.com/grafana/dashboards/5336-ceph-osd-single/ target=_blank rel=noopener>https://grafana.com/grafana/dashboards/5336-ceph-osd-single/</a>
<a class=link href=https://grafana.com/grafana/dashboards/5342-ceph-pools/ target=_blank rel=noopener>https://grafana.com/grafana/dashboards/5342-ceph-pools/</a></p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033454.png width=944 height=662 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033454_hufc14a3eb5fdd28879fee795a8c8a1ae4_84558_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129033454_hufc14a3eb5fdd28879fee795a8c8a1ae4_84558_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=142 data-flex-basis=342px></p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033528.png width=1444 height=952 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033528_huc2735031242b707af4c1b9b365618eef_238113_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129033528_huc2735031242b707af4c1b9b365618eef_238113_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=151 data-flex-basis=364px></p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033556.png width=938 height=692 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033556_hu3753b8992440733a08d09a641a826f9d_85471_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129033556_hu3753b8992440733a08d09a641a826f9d_85471_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=135 data-flex-basis=325px></p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033610.png width=1445 height=935 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033610_hu1758e4ffe16c9e0a29ee87f12fcf0052_147630_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129033610_hu1758e4ffe16c9e0a29ee87f12fcf0052_147630_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=154 data-flex-basis=370px></p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033640.png width=918 height=683 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033640_hu9ef17ef25dce363feba9835b84028665_84160_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129033640_hu9ef17ef25dce363feba9835b84028665_84160_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=134 data-flex-basis=322px></p><p><img src=/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033654.png width=1443 height=944 srcset="/2023/kubernetes-rook-ceph/media/Pasted-image-20231129033654_hu0975ae701899ae8815b97f0fddb79516_161344_480x0_resize_box_3.png 480w, /2023/kubernetes-rook-ceph/media/Pasted-image-20231129033654_hu0975ae701899ae8815b97f0fddb79516_161344_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=152 data-flex-basis=366px></p></section><footer class=article-footer><section class=article-tags><a href=/tags/kubernetes/>Kubernetes</a>
<a href=/tags/ceph/>Ceph</a>
<a href=/tags/rook-ceph/>Rook-Ceph</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>相關文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/2024/kubernetes-use-kubespray-deploy-calico-bgp-backend-process-bird-is-not-running/><div class=article-details><h2 class=article-title>使用kubespray專案佈屬，calico bgp沒有啟動</h2></div></a></article><article><a href=/2024/kubenetes-nodelocaldns-nslookup-custom-domain-error/><div class=article-details><h2 class=article-title>kubernetes nodelocaldns 解析自定義domain失敗</h2></div></a></article><article><a href=/2023/kubernetes-rancher-install-prometheus/><div class=article-details><h2 class=article-title>Kubernetes 使用Rancher管理平台 安裝Prometheus監控</h2></div></a></article><article class=has-image><a href=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/><div class=article-image><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/_hu22d4264949f439b8802769db5f69cb0d_36536_c0cd8f7cf63dffc0c7cb863e69c61601.png width=250 height=150 loading=lazy alt="Featured image of post 使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群(haproxy、keepalived)" data-key=ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived data-hash="md5-ksOzdmc4vGgYLEYiVrH4yg=="></div><div class=article-details><h2 class=article-title>使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群(haproxy、keepalived)</h2></div></a></article><article class=has-image><a href=/2023/kubernetes-summit-2023/><div class=article-image><img src=/2023/kubernetes-summit-2023/media/_hub7b044231ef4aeecfc91c4975bdc3f98_300198_549732b0a870c59377d4ec865acd8eb7.png width=250 height=150 loading=lazy alt="Featured image of post Kubernetes Summit 2023" data-key=kubernetes-summit-2023 data-hash="md5-l3szp9B4JNVqHi1UN38u6w=="></div><div class=article-details><h2 class=article-title>Kubernetes Summit 2023</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//blog-goldfishbrain-fighting-com.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2022 -
2024 翻轉吧金魚腦</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 建立<br>主題 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.20.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 設計</section><section class=totalcount>發表了125篇文章 ·
總計59.03k字</section><section class=running-time>本站已運行
<span id=runningdays class=running-days></span></section><section class=visit-count><span id=busuanzi_container_site_pv style=display:none>本站總訪問量 <span id=busuanzi_value_site_pv></span> 次 </span>·
<span id=busuanzi_container_site_uv style=display:none>總訪客數 <span id=busuanzi_value_site_uv></span> 人</span></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><a href=# id=back-to-top title=返回顶部></a><style>#back-to-top{display:none;position:fixed;bottom:20px;right:55px;width:55px;height:55px;border-radius:7px;background-color:rgba(64,158,255,.5);box-shadow:var(--shadow-l2);font-size:30px;text-align:center;line-height:50px;cursor:pointer}#back-to-top:before{content:' ';display:inline-block;position:relative;top:0;transform:rotate(135deg);height:10px;width:10px;border-width:0 0 2px 2px;border-color:var(--back-to-top-color);border-style:solid}#back-to-top:hover:before{border-color:#2674e0}@media screen and (max-width:768px){#back-to-top{bottom:20px;right:20px;width:40px;height:40px;font-size:10px}}@media screen and (min-width:1024px){#back-to-top{bottom:20px;right:40px}}@media screen and (min-width:1280px){#back-to-top{bottom:20px;right:55px}}@media screen and (min-width:1536px){#back-to-top{visibility:hidden}}</style><script>function backToTop(){document.documentElement.scrollIntoView({behavior:"smooth"})}window.onload=function(){let t=this.document.documentElement.scrollTop||this.document.body.scrollTop,e=this.document.getElementById("back-to-top");t>0?e.style.display="inline":e.style.display="none"},window.onscroll=function(){let t=this.document.documentElement.scrollTop||this.document.body.scrollTop,e=this.document.getElementById("back-to-top");t<200?e.style.display="none":(e.style.display="inline",e.addEventListener("click",backToTop,!1))}</script><script>(function(){var t,e=window;if(e.ChannelIO)return e.console.error("ChannelIO script included twice.");t=function(){t.c(arguments)},t.q=[],t.c=function(e){t.q.push(e)},e.ChannelIO=t;function n(){if(e.ChannelIOInitialized)return;e.ChannelIOInitialized=!0;var n,t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src="https://cdn.channel.io/plugin/ch-plugin-web.js",n=document.getElementsByTagName("script")[0],n.parentNode&&n.parentNode.insertBefore(t,n)}document.readyState==="complete"?n():(e.addEventListener("DOMContentLoaded",n),e.addEventListener("load",n))})(),ChannelIO("boot",{pluginKey:"ad479103-2d28-421a-a916-ae09e72ce014"})</script><script>let s1="2023-10-7";s1=new Date(s1.replace(/-/g,"/"));let s2=new Date,timeDifference=s2.getTime()-s1.getTime(),days=Math.floor(timeDifference/(1e3*60*60*24)),hours=Math.floor(timeDifference%(1e3*60*60*24)/(1e3*60*60)),minutes=Math.floor(timeDifference%(1e3*60*60)/(1e3*60)),result=days+"天"+hours+"小時"+minutes+"分鐘";document.getElementById("runningdays").innerHTML=result</script></body></html>