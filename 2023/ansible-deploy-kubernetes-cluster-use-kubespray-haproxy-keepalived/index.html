<!doctype html><html lang=en-us dir=ltr><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QB4H991E1G"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QB4H991E1G")</script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群，高可用採用haproxy+keepalived"><meta name=keywords content="翻轉吧金魚腦,kubespray,ansible,kubernetes,使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群(haproxy、keepalived),haproxy,keepalived"><title>使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群(haproxy、keepalived)</title>
<link rel=canonical href=https://blog.goldfishbrain-fighting.com/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/><link rel=stylesheet href=/scss/style.min.7fb7f5621081d8ecec310edc2d95cc0b431358ce9c30c3ee8601b16732dbef57.css><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script src=//cdn.bootcss.com/jquery/3.2.1/jquery.min.js></script><script>$(document).ready(function(){var e=setInterval(s,100),t=100,n=50;function s(){$("#busuanzi_container_site_pv").css("display")!="none"&&(clearInterval(e),$("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html())+t),$("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html())+n))}})</script><meta property="og:title" content="使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群(haproxy、keepalived)"><meta property="og:description" content="使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群，高可用採用haproxy+keepalived"><meta property="og:url" content="https://blog.goldfishbrain-fighting.com/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/"><meta property="og:site_name" content="翻轉吧金魚腦"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="kubernetes"><meta property="article:tag" content="ansible"><meta property="article:tag" content="kubespray"><meta property="article:tag" content="docker"><meta property="article:tag" content="cri-docker"><meta property="article:tag" content="keepalived"><meta property="article:tag" content="haproxy"><meta property="article:published_time" content="2023-12-02T17:34:57+08:00"><meta property="article:modified_time" content="2023-12-02T17:34:57+08:00"><meta property="og:image" content="https://blog.goldfishbrain-fighting.com/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231205222815.png"><meta name=twitter:title content="使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群(haproxy、keepalived)"><meta name=twitter:description content="使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群，高可用採用haproxy+keepalived"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blog.goldfishbrain-fighting.com/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231205222815.png"><link rel="shortcut icon" href=/favicon.png><script async src="https://www.googletagmanager.com/gtag/js?id=G-QB4H991E1G"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QB4H991E1G")</script><meta name=google-site-verification content="EkUcMLTrO3RDZzjREX9Dffy2lcjWCR-2Qtu8VF4b8sQ"><meta content="翻轉吧金魚腦,kubespray,ansible,kubernetes,使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群(haproxy、keepalived),haproxy,keepalived" name=keywords></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><div id=article-toolbar style=position:sticky;top:5px;z-index:1000><a href=/ class=back-home><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="15 6 9 12 15 18"/></svg><span>返回</span></a></div><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目錄</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#架構圖>架構圖</a></li><li><a href=#集群自動化建置>集群自動化建置</a><ul><li><a href=#準備>準備</a></li><li><a href=#生成hostsyaml>生成hosts.yaml</a></li><li><a href=#修改allyml>修改all.yml</a></li><li><a href=#修改k8s-clusteryml>修改k8s-cluster.yml</a></li><li><a href=#修改addonsyml>修改addons.yml</a></li><li><a href=#修改mainyml>修改main.yml</a></li><li><a href=#修改hostsyaml>修改hosts.yaml</a></li><li><a href=#安裝配置haproxykeepalived>安裝配置haproxy、keepalived</a></li><li><a href=#佈屬主機etchosts填寫>佈屬主機/etc/hosts填寫</a></li><li><a href=#開始佈屬>開始佈屬</a></li><li><a href=#功能驗證>功能驗證</a><ul><li><a href=#命令補全>命令補全</a></li><li><a href=#etchosts>/etc/hosts</a></li><li><a href=#連線配置文件config>連線配置文件(config)</a></li><li><a href=#節點ready>節點Ready</a></li><li><a href=#master都有污點>master都有污點</a></li><li><a href=#helm有安裝>helm有安裝</a></li><li><a href=#所有pod都running>所有POD都Running</a></li><li><a href=#佈屬kubernetes的yaml>佈屬kubernetes的yaml</a></li><li><a href=#master組件yaml>master組件yaml</a></li><li><a href=#測試創建pod>測試創建POD</a></li><li><a href=#測試deploymentserviceingress>測試Deployment、Service、Ingress</a></li><li><a href=#etcd服務>etcd服務</a></li><li><a href=#cri-dockerd服務>cri-dockerd服務</a></li><li><a href=#kubelet服務>kubelet服務</a></li><li><a href=#憑證與金鑰>憑證與金鑰</a></li><li><a href=#etcd憑證與金鑰>etcd憑證與金鑰</a></li><li><a href=#安裝rancher管理平台高可用>安裝rancher管理平台(高可用)</a><ul><li><a href=#一添加-helm-chart-倉庫>一、添加 Helm Chart 倉庫</a></li><li><a href=#二為-rancher-建立命名空間>二、為 Rancher 建立命名空間</a></li><li><a href=#三選擇-ssl-配置>三、選擇 SSL 配置</a></li><li><a href=#四安裝-cert-manager>四、安裝 cert-manager</a></li><li><a href=#五根據你選擇的證書選項通過-helm-安裝-rancher>五、根據你選擇的證書選項，通過 Helm 安裝 Rancher</a></li><li><a href=#六驗證-rancher-server-是否部署成功>六、驗證 Rancher Server 是否部署成功</a></li></ul></li><li><a href=#安裝rook-ceph分布式存儲系統>安裝rook-ceph分布式存儲系統</a><ul><li><ul><li><a href=#cephfs>cephfs</a></li></ul></li></ul></li></ul></li><li><a href=#reset清除集群設定>reset清除集群設定</a></li><li><a href=#如果沒外部vipworker-node透過nginx-proxy反代理訪問master-node的kube-apiserver>如果沒外部VIP，worker node透過nginx proxy反代理，訪問master node的kube-apiserver</a></li></ul></li></ul></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231205222815_hu22d4264949f439b8802769db5f69cb0d_36536_800x0_resize_box_3.png srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231205222815_hu22d4264949f439b8802769db5f69cb0d_36536_800x0_resize_box_3.png 800w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231205222815_hu22d4264949f439b8802769db5f69cb0d_36536_1600x0_resize_box_3.png 1600w" width=800 height=201 loading=lazy alt="Featured image of post 使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群(haproxy、keepalived)"></a></div><div class=article-details><header class=article-category><a href=/categories/kubernetes/ style=background-color:#0000e3;color:#fff>kubernetes</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/>使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群(haproxy、keepalived)</a></h2><h3 class=article-subtitle>使用kubespray專案，用ansible批量佈屬生產環境高可用kubernetes集群，高可用採用haproxy+keepalived</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>2023/12/02</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>閱讀時間: 34 分鐘</time></div></footer></div></header><section class=article-content><section class="toc toc--inline"><h2>目錄</h2><div><nav id=TableOfContents><ul><li><a href=#架構圖>架構圖</a></li><li><a href=#集群自動化建置>集群自動化建置</a><ul><li><a href=#準備>準備</a></li><li><a href=#生成hostsyaml>生成hosts.yaml</a></li><li><a href=#修改allyml>修改all.yml</a></li><li><a href=#修改k8s-clusteryml>修改k8s-cluster.yml</a></li><li><a href=#修改addonsyml>修改addons.yml</a></li><li><a href=#修改mainyml>修改main.yml</a></li><li><a href=#修改hostsyaml>修改hosts.yaml</a></li><li><a href=#安裝配置haproxykeepalived>安裝配置haproxy、keepalived</a></li><li><a href=#佈屬主機etchosts填寫>佈屬主機/etc/hosts填寫</a></li><li><a href=#開始佈屬>開始佈屬</a></li><li><a href=#功能驗證>功能驗證</a><ul><li><a href=#命令補全>命令補全</a></li><li><a href=#etchosts>/etc/hosts</a></li><li><a href=#連線配置文件config>連線配置文件(config)</a></li><li><a href=#節點ready>節點Ready</a></li><li><a href=#master都有污點>master都有污點</a></li><li><a href=#helm有安裝>helm有安裝</a></li><li><a href=#所有pod都running>所有POD都Running</a></li><li><a href=#佈屬kubernetes的yaml>佈屬kubernetes的yaml</a></li><li><a href=#master組件yaml>master組件yaml</a></li><li><a href=#測試創建pod>測試創建POD</a></li><li><a href=#測試deploymentserviceingress>測試Deployment、Service、Ingress</a></li><li><a href=#etcd服務>etcd服務</a></li><li><a href=#cri-dockerd服務>cri-dockerd服務</a></li><li><a href=#kubelet服務>kubelet服務</a></li><li><a href=#憑證與金鑰>憑證與金鑰</a></li><li><a href=#etcd憑證與金鑰>etcd憑證與金鑰</a></li><li><a href=#安裝rancher管理平台高可用>安裝rancher管理平台(高可用)</a><ul><li><a href=#一添加-helm-chart-倉庫>一、添加 Helm Chart 倉庫</a></li><li><a href=#二為-rancher-建立命名空間>二、為 Rancher 建立命名空間</a></li><li><a href=#三選擇-ssl-配置>三、選擇 SSL 配置</a></li><li><a href=#四安裝-cert-manager>四、安裝 cert-manager</a></li><li><a href=#五根據你選擇的證書選項通過-helm-安裝-rancher>五、根據你選擇的證書選項，通過 Helm 安裝 Rancher</a></li><li><a href=#六驗證-rancher-server-是否部署成功>六、驗證 Rancher Server 是否部署成功</a></li></ul></li><li><a href=#安裝rook-ceph分布式存儲系統>安裝rook-ceph分布式存儲系統</a><ul><li><ul><li><a href=#cephfs>cephfs</a></li></ul></li></ul></li></ul></li><li><a href=#reset清除集群設定>reset清除集群設定</a></li><li><a href=#如果沒外部vipworker-node透過nginx-proxy反代理訪問master-node的kube-apiserver>如果沒外部VIP，worker node透過nginx proxy反代理，訪問master node的kube-apiserver</a></li></ul></li></ul></nav></div></section><h1 id=架構圖>架構圖</h1><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231205214821.png width=1276 height=728 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231205214821_hua7912735140a57843d03f863f5ef97a0_414365_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231205214821_hua7912735140a57843d03f863f5ef97a0_414365_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=175 data-flex-basis=420px></p><h1 id=集群自動化建置>集群自動化建置</h1><h2 id=準備>準備</h2><p><a class=link href="https://kubespray.io/#/docs/ansible?id=installing-ansible" target=_blank rel=noopener>https://kubespray.io/#/docs/ansible?id=installing-ansible</a></p><p>使用python3創建虛擬目錄，安裝依賴包</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>[</span>root@ansible ~<span class=o>]</span><span class=c1># python3 --version</span>
</span></span><span class=line><span class=cl>Python 3.9.18
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>[</span>root@ansible ~<span class=o>]</span><span class=c1># git clone https://github.com/kubernetes-sigs/kubespray.git</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>[</span>root@ansible ~<span class=o>]</span><span class=c1># VENVDIR=kubespray-venv</span>
</span></span><span class=line><span class=cl><span class=o>[</span>root@ansible ~<span class=o>]</span><span class=c1># KUBESPRAYDIR=kubespray</span>
</span></span><span class=line><span class=cl><span class=o>[</span>root@ansible ~<span class=o>]</span><span class=c1># python3 -m venv $VENVDIR</span>
</span></span><span class=line><span class=cl><span class=o>[</span>root@ansible ~<span class=o>]</span><span class=c1># source $VENVDIR/bin/activate</span>
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible ~<span class=o>]</span><span class=c1># cd $KUBESPRAYDIR</span>
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># pip install -U -r requirements.txt</span>
</span></span><span class=line><span class=cl>Collecting <span class=nv>ansible</span><span class=o>==</span>8.5.0
</span></span><span class=line><span class=cl>  Downloading ansible-8.5.0-py3-none-any.whl <span class=o>(</span>47.5 MB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.5/47.5 MB 4.0 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>Collecting <span class=nv>cryptography</span><span class=o>==</span>41.0.4
</span></span><span class=line><span class=cl>  Downloading cryptography-41.0.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl <span class=o>(</span>4.4 MB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 9.2 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>Collecting <span class=nv>jinja2</span><span class=o>==</span>3.1.2
</span></span><span class=line><span class=cl>  Downloading Jinja2-3.1.2-py3-none-any.whl <span class=o>(</span><span class=m>133</span> kB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 kB 9.1 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>Collecting <span class=nv>jmespath</span><span class=o>==</span>1.0.1
</span></span><span class=line><span class=cl>  Downloading jmespath-1.0.1-py3-none-any.whl <span class=o>(</span><span class=m>20</span> kB<span class=o>)</span>
</span></span><span class=line><span class=cl>Collecting <span class=nv>MarkupSafe</span><span class=o>==</span>2.1.3
</span></span><span class=line><span class=cl>  Downloading MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl <span class=o>(</span><span class=m>25</span> kB<span class=o>)</span>
</span></span><span class=line><span class=cl>Collecting <span class=nv>netaddr</span><span class=o>==</span>0.9.0
</span></span><span class=line><span class=cl>  Downloading netaddr-0.9.0-py3-none-any.whl <span class=o>(</span>2.2 MB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 9.2 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>Collecting <span class=nv>pbr</span><span class=o>==</span>5.11.1
</span></span><span class=line><span class=cl>  Downloading pbr-5.11.1-py2.py3-none-any.whl <span class=o>(</span><span class=m>112</span> kB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.7/112.7 kB 8.2 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>Collecting ruamel.yaml<span class=o>==</span>0.17.35
</span></span><span class=line><span class=cl>  Downloading ruamel.yaml-0.17.35-py3-none-any.whl <span class=o>(</span><span class=m>112</span> kB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.9/112.9 kB 9.4 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>Collecting ruamel.yaml.clib<span class=o>==</span>0.2.8
</span></span><span class=line><span class=cl>  Downloading ruamel.yaml.clib-0.2.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl <span class=o>(</span><span class=m>562</span> kB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 562.1/562.1 kB 10.4 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>Collecting ansible-core~<span class=o>=</span>2.15.5
</span></span><span class=line><span class=cl>  Downloading ansible_core-2.15.6-py3-none-any.whl <span class=o>(</span>2.2 MB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 9.3 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>Collecting cffi&gt;<span class=o>=</span>1.12
</span></span><span class=line><span class=cl>  Downloading cffi-1.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl <span class=o>(</span><span class=m>443</span> kB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 443.4/443.4 kB 9.7 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>Collecting importlib-resources&lt;5.1,&gt;<span class=o>=</span>5.0
</span></span><span class=line><span class=cl>  Downloading importlib_resources-5.0.7-py3-none-any.whl <span class=o>(</span><span class=m>24</span> kB<span class=o>)</span>
</span></span><span class=line><span class=cl>Collecting PyYAML&gt;<span class=o>=</span>5.1
</span></span><span class=line><span class=cl>  Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl <span class=o>(</span><span class=m>738</span> kB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 738.9/738.9 kB 8.6 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>Collecting packaging
</span></span><span class=line><span class=cl>  Downloading packaging-23.2-py3-none-any.whl <span class=o>(</span><span class=m>53</span> kB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 7.5 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>Collecting resolvelib&lt;1.1.0,&gt;<span class=o>=</span>0.5.3
</span></span><span class=line><span class=cl>  Downloading resolvelib-1.0.1-py2.py3-none-any.whl <span class=o>(</span><span class=m>17</span> kB<span class=o>)</span>
</span></span><span class=line><span class=cl>Collecting pycparser
</span></span><span class=line><span class=cl>  Downloading pycparser-2.21-py2.py3-none-any.whl <span class=o>(</span><span class=m>118</span> kB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.7/118.7 kB 8.6 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>Installing collected packages: resolvelib, netaddr, ruamel.yaml.clib, PyYAML, pycparser, pbr, packaging, MarkupSafe, jmespath, importlib-resources, ruamel.yaml, jinja2, cffi, cryptography, ansible-core, ansible
</span></span><span class=line><span class=cl>Successfully installed MarkupSafe-2.1.3 PyYAML-6.0.1 ansible-8.5.0 ansible-core-2.15.6 cffi-1.16.0 cryptography-41.0.4 importlib-resources-5.0.7 jinja2-3.1.2 jmespath-1.0.1 netaddr-0.9.0 packaging-23.2 pbr-5.11.1 pycparser-2.21 resolvelib-1.0.1 ruamel.yaml-0.17.35 ruamel.yaml.clib-0.2.8
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>[</span>notice<span class=o>]</span> A new release of pip is available: 23.0.1 -&gt; 23.3.1
</span></span><span class=line><span class=cl><span class=o>[</span>notice<span class=o>]</span> To update, run: pip install --upgrade pip
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1>#</span>
</span></span></code></pre></div><h2 id=生成hostsyaml>生成hosts.yaml</h2><p><a class=link href="https://kubespray.io/#/?id=deploy-a-production-ready-kubernetes-cluster" target=_blank rel=noopener>https://kubespray.io/#/?id=deploy-a-production-ready-kubernetes-cluster</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># cp -rfp inventory/sample inventory/mycluster</span>
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># declare -a IPS=(192.168.1.71 192.168.1.72 192.168.1.73 192.168.1.75 192.168.1.76)</span>
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}</span>
</span></span><span class=line><span class=cl>DEBUG: Adding group all
</span></span><span class=line><span class=cl>DEBUG: Adding group kube_control_plane
</span></span><span class=line><span class=cl>DEBUG: Adding group kube_node
</span></span><span class=line><span class=cl>DEBUG: Adding group etcd
</span></span><span class=line><span class=cl>DEBUG: Adding group k8s_cluster
</span></span><span class=line><span class=cl>DEBUG: Adding group calico_rr
</span></span><span class=line><span class=cl>DEBUG: adding host node1 to group all
</span></span><span class=line><span class=cl>DEBUG: adding host node2 to group all
</span></span><span class=line><span class=cl>DEBUG: adding host node3 to group all
</span></span><span class=line><span class=cl>DEBUG: adding host node4 to group all
</span></span><span class=line><span class=cl>DEBUG: adding host node5 to group all
</span></span><span class=line><span class=cl>DEBUG: adding host node1 to group etcd
</span></span><span class=line><span class=cl>DEBUG: adding host node2 to group etcd
</span></span><span class=line><span class=cl>DEBUG: adding host node3 to group etcd
</span></span><span class=line><span class=cl>DEBUG: adding host node1 to group kube_control_plane
</span></span><span class=line><span class=cl>DEBUG: adding host node2 to group kube_control_plane
</span></span><span class=line><span class=cl>DEBUG: adding host node1 to group kube_node
</span></span><span class=line><span class=cl>DEBUG: adding host node2 to group kube_node
</span></span><span class=line><span class=cl>DEBUG: adding host node3 to group kube_node
</span></span><span class=line><span class=cl>DEBUG: adding host node4 to group kube_node
</span></span><span class=line><span class=cl>DEBUG: adding host node5 to group kube_node
</span></span></code></pre></div><h2 id=修改allyml>修改all.yml</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>主要修改內容如下：
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>VIP與domain_name設定
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>需要Proxy的可以設定
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># vim inventory/mycluster/group_vars/all/all.yml</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>apiserver_loadbalancer_domain_name: <span class=s2>&#34;k8s-apiserver-lb.jimmyhome.tw&#34;</span>
</span></span><span class=line><span class=cl>loadbalancer_apiserver:
</span></span><span class=line><span class=cl>  address: 192.168.1.74
</span></span><span class=line><span class=cl>  port: <span class=m>7443</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## Set these proxy values in order to update package manager and docker daemon to use proxies and custom CA for https_proxy if needed</span>
</span></span><span class=line><span class=cl><span class=c1># http_proxy: &#34;http://x.x.x.x:3128&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># https_proxy: &#34;http://x.x.x.x:3128&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># https_proxy_cert_file: &#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## Refer to roles/kubespray-defaults/defaults/main.yml before modifying no_proxy</span>
</span></span><span class=line><span class=cl><span class=c1># no_proxy: &#34;127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,cattle-system.svc,.svc,.cluster.local,k8s-apiserver-lb.jimmyhome.tw&#34;</span>
</span></span></code></pre></div><h2 id=修改k8s-clusteryml>修改k8s-cluster.yml</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>主要修改內容如下：
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>要安裝的kubernetes版本
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>service ip
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>pod ip
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>network_node_prefix
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>容器引擎，我這裡使用docker
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>是否憑證自動更新(每個月月初，會自動更新)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># vim inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## Change this to use another Kubernetes version, e.g. a current beta release</span>
</span></span><span class=line><span class=cl>kube_version: v1.26.3
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Kubernetes internal network for services, unused block of space.</span>
</span></span><span class=line><span class=cl>kube_service_addresses: 10.202.0.0/16
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># internal network. When used, it will assign IP</span>
</span></span><span class=line><span class=cl><span class=c1># addresses from this range to individual pods.</span>
</span></span><span class=line><span class=cl><span class=c1># This network must be unused in your network infrastructure!</span>
</span></span><span class=line><span class=cl>kube_pods_subnet: 10.201.0.0/16
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kube_network_node_prefix: <span class=m>16</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## Container runtime</span>
</span></span><span class=line><span class=cl><span class=c1>## docker for docker, crio for cri-o and containerd for containerd.</span>
</span></span><span class=line><span class=cl><span class=c1>## Default: containerd</span>
</span></span><span class=line><span class=cl>container_manager: docker
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## Automatically renew K8S control plane certificates on first Monday of each month</span>
</span></span><span class=line><span class=cl>auto_renew_certificates: <span class=nb>true</span>
</span></span></code></pre></div><h2 id=修改addonsyml>修改addons.yml</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>主要修改內容如下：
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>安裝Nginx ingress
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>安裝helm工具
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>安裝metrics_server
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># vim inventory/mycluster/group_vars/k8s_cluster/addons.yml</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Nginx ingress controller deployment</span>
</span></span><span class=line><span class=cl>ingress_nginx_enabled: <span class=nb>true</span>
</span></span><span class=line><span class=cl>ingress_nginx_host_network: <span class=nb>true</span>
</span></span><span class=line><span class=cl>ingress_publish_status_address: <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># ingress_nginx_nodeselector:</span>
</span></span><span class=line><span class=cl><span class=c1>#   kubernetes.io/os: &#34;linux&#34;</span>
</span></span><span class=line><span class=cl>ingress_nginx_tolerations:
</span></span><span class=line><span class=cl>  - key: <span class=s2>&#34;node-role.kubernetes.io/control-plane&#34;</span>
</span></span><span class=line><span class=cl>    operator: <span class=s2>&#34;Equal&#34;</span>
</span></span><span class=line><span class=cl>    value: <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>    effect: <span class=s2>&#34;NoSchedule&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Helm deployment</span>
</span></span><span class=line><span class=cl>helm_enabled: <span class=nb>true</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Metrics Server deployment</span>
</span></span><span class=line><span class=cl>metrics_server_enabled: <span class=nb>true</span>
</span></span></code></pre></div><h2 id=修改mainyml>修改main.yml</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>主要修改內容如下：
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>網路組件calico版本
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># vim roles/kubespray-defaults/vars/main.yml</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>calico_min_version_required: <span class=s2>&#34;v3.26.0&#34;</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># cri-docker版本修改位置，有需要再改，這裡就依這個版本</span>
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># vim roles/download/defaults/main/main.yml</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cri_dockerd_version: 0.3.4
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># docker版本修改位置，有需要再改，這裡就依這個版本</span>
</span></span><span class=line><span class=cl><span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># vim roles/container-engine/docker/defaults/main.yml</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>docker_version: <span class=s1>&#39;20.10&#39;</span>
</span></span></code></pre></div><h2 id=修改hostsyaml>修改hosts.yaml</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>主要修改內容如下：
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>依據現況去填寫
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kube_control_plane -&gt; master節點(會有不能調度污點)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kube_node -&gt; worker節點
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>etcd -&gt; 數據庫節點
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># cat inventory/mycluster/hosts.yaml</span>
</span></span><span class=line><span class=cl>all:
</span></span><span class=line><span class=cl>  hosts:
</span></span><span class=line><span class=cl>    k8s-master71u:
</span></span><span class=line><span class=cl>      ansible_host: k8s-master71u
</span></span><span class=line><span class=cl>      ip: 192.168.1.71
</span></span><span class=line><span class=cl>      access_ip: 192.168.1.71
</span></span><span class=line><span class=cl>    k8s-master72u:
</span></span><span class=line><span class=cl>      ansible_host: k8s-master72u
</span></span><span class=line><span class=cl>      ip: 192.168.1.72
</span></span><span class=line><span class=cl>      access_ip: 192.168.1.72
</span></span><span class=line><span class=cl>    k8s-master73u:
</span></span><span class=line><span class=cl>      ansible_host: k8s-master73u
</span></span><span class=line><span class=cl>      ip: 192.168.1.73
</span></span><span class=line><span class=cl>      access_ip: 192.168.1.73
</span></span><span class=line><span class=cl>    k8s-node75u:
</span></span><span class=line><span class=cl>      ansible_host: k8s-node75u
</span></span><span class=line><span class=cl>      ip: 192.168.1.75
</span></span><span class=line><span class=cl>      access_ip: 192.168.1.75
</span></span><span class=line><span class=cl>    k8s-node76u:
</span></span><span class=line><span class=cl>      ansible_host: k8s-node76u
</span></span><span class=line><span class=cl>      ip: 192.168.1.76
</span></span><span class=line><span class=cl>      access_ip: 192.168.1.76
</span></span><span class=line><span class=cl>  children:
</span></span><span class=line><span class=cl>    kube_control_plane:
</span></span><span class=line><span class=cl>      hosts:
</span></span><span class=line><span class=cl>        k8s-master71u:
</span></span><span class=line><span class=cl>        k8s-master72u:
</span></span><span class=line><span class=cl>        k8s-master73u:
</span></span><span class=line><span class=cl>    kube_node:
</span></span><span class=line><span class=cl>      hosts:
</span></span><span class=line><span class=cl>        k8s-node75u:
</span></span><span class=line><span class=cl>        k8s-node76u:
</span></span><span class=line><span class=cl>    etcd:
</span></span><span class=line><span class=cl>      hosts:
</span></span><span class=line><span class=cl>        k8s-master71u:
</span></span><span class=line><span class=cl>        k8s-master72u:
</span></span><span class=line><span class=cl>        k8s-master73u:
</span></span><span class=line><span class=cl>    k8s_cluster:
</span></span><span class=line><span class=cl>      children:
</span></span><span class=line><span class=cl>        kube_control_plane:
</span></span><span class=line><span class=cl>        kube_node:
</span></span><span class=line><span class=cl>    calico_rr:
</span></span><span class=line><span class=cl>      hosts: <span class=o>{}</span>
</span></span></code></pre></div><h2 id=安裝配置haproxykeepalived>安裝配置haproxy、keepalived</h2><p>操作節點： 所有的master</p><p>注意：如果有兩個集群，都在同一網段內，/etc/keepalived/keepalived.conf裡面的virtual_router_id 60記得要不一樣，不然/var/log/message會一直報錯。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# apt-get install keepalived haproxy -y
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 所有master節點執行,注意替換最後的master節點IP地址</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# cat /etc/haproxy/haproxy.cfg
</span></span><span class=line><span class=cl>global
</span></span><span class=line><span class=cl>  maxconn  <span class=m>2000</span>
</span></span><span class=line><span class=cl>  ulimit-n  <span class=m>16384</span>
</span></span><span class=line><span class=cl>  log  127.0.0.1 local0 err
</span></span><span class=line><span class=cl>  stats timeout 30s
</span></span><span class=line><span class=cl>defaults
</span></span><span class=line><span class=cl>  log global
</span></span><span class=line><span class=cl>  mode  http
</span></span><span class=line><span class=cl>  option  httplog
</span></span><span class=line><span class=cl>  timeout connect <span class=m>5000</span>
</span></span><span class=line><span class=cl>  timeout client  <span class=m>50000</span>
</span></span><span class=line><span class=cl>  timeout server  <span class=m>50000</span>
</span></span><span class=line><span class=cl>  timeout http-request 15s
</span></span><span class=line><span class=cl>  timeout http-keep-alive 15s
</span></span><span class=line><span class=cl>frontend monitor-in
</span></span><span class=line><span class=cl>  <span class=nb>bind</span> *:33305
</span></span><span class=line><span class=cl>  mode http
</span></span><span class=line><span class=cl>  option httplog
</span></span><span class=line><span class=cl>  monitor-uri /monitor
</span></span><span class=line><span class=cl>frontend k8s-master
</span></span><span class=line><span class=cl>  <span class=nb>bind</span> 0.0.0.0:7443
</span></span><span class=line><span class=cl>  <span class=nb>bind</span> 127.0.0.1:7443
</span></span><span class=line><span class=cl>  mode tcp
</span></span><span class=line><span class=cl>  option tcplog
</span></span><span class=line><span class=cl>  tcp-request inspect-delay 5s
</span></span><span class=line><span class=cl>  default_backend k8s-master
</span></span><span class=line><span class=cl>backend k8s-master
</span></span><span class=line><span class=cl>  mode tcp
</span></span><span class=line><span class=cl>  option tcplog
</span></span><span class=line><span class=cl>  option tcp-check
</span></span><span class=line><span class=cl>  balance roundrobin
</span></span><span class=line><span class=cl>  default-server inter 10s downinter 5s rise <span class=m>2</span> fall <span class=m>2</span> slowstart 60s maxconn <span class=m>250</span> maxqueue <span class=m>256</span> weight <span class=m>100</span>
</span></span><span class=line><span class=cl>  server k8s-master71u    192.168.1.71:6443   check
</span></span><span class=line><span class=cl>  server k8s-master72u    192.168.1.72:6443   check
</span></span><span class=line><span class=cl>  server k8s-master73u    192.168.1.73:6443   check
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 在k8s-master71u節點，注意mcast_src_ip換成實際的master1ip地址，virtual_ipaddress換成lb地址，interface要替換成主機IP使用的介面</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# cat /etc/keepalived/keepalived.conf
</span></span><span class=line><span class=cl>! Configuration File <span class=k>for</span> keepalived
</span></span><span class=line><span class=cl>global_defs <span class=o>{</span>
</span></span><span class=line><span class=cl>    router_id LVS_DEVEL
</span></span><span class=line><span class=cl>script_user root
</span></span><span class=line><span class=cl>    enable_script_security
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>vrrp_script chk_apiserver <span class=o>{</span>
</span></span><span class=line><span class=cl>    script <span class=s2>&#34;/etc/keepalived/check_apiserver.sh&#34;</span>
</span></span><span class=line><span class=cl>    interval <span class=m>5</span>
</span></span><span class=line><span class=cl>    weight -5
</span></span><span class=line><span class=cl>    fall <span class=m>2</span>
</span></span><span class=line><span class=cl>    rise <span class=m>1</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>vrrp_instance VI_1 <span class=o>{</span>
</span></span><span class=line><span class=cl>    state MASTER
</span></span><span class=line><span class=cl>    interface ens160
</span></span><span class=line><span class=cl>    mcast_src_ip 192.168.1.71
</span></span><span class=line><span class=cl>    virtual_router_id <span class=m>64</span>
</span></span><span class=line><span class=cl>    priority <span class=m>101</span>
</span></span><span class=line><span class=cl>    advert_int <span class=m>2</span>
</span></span><span class=line><span class=cl>    authentication <span class=o>{</span>
</span></span><span class=line><span class=cl>        auth_type PASS
</span></span><span class=line><span class=cl>        auth_pass K8SHA_KA_AUTH
</span></span><span class=line><span class=cl>    <span class=o>}</span>
</span></span><span class=line><span class=cl>    virtual_ipaddress <span class=o>{</span>
</span></span><span class=line><span class=cl>        192.168.1.74
</span></span><span class=line><span class=cl>    <span class=o>}</span>
</span></span><span class=line><span class=cl>    track_script <span class=o>{</span>
</span></span><span class=line><span class=cl>       chk_apiserver
</span></span><span class=line><span class=cl>    <span class=o>}</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 在k8s-master72u和k8s-master73u分別創建/etc/keepalived/keepalived.conf，注意修改mcast_src_ip和virtual_ipaddress</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#所有master節點配置KeepAlived健康檢查文件：</span>
</span></span><span class=line><span class=cl>root@ck8s-p-4faj09-24:~# vim /etc/keepalived/check_apiserver.sh
</span></span><span class=line><span class=cl><span class=c1>#!/bin/bash</span>
</span></span><span class=line><span class=cl><span class=nv>err</span><span class=o>=</span><span class=m>0</span>
</span></span><span class=line><span class=cl><span class=k>for</span> k in <span class=k>$(</span>seq <span class=m>1</span> 3<span class=k>)</span>
</span></span><span class=line><span class=cl><span class=k>do</span>
</span></span><span class=line><span class=cl>    <span class=nv>check_code</span><span class=o>=</span><span class=k>$(</span>pgrep haproxy<span class=k>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=o>[[</span> <span class=nv>$check_code</span> <span class=o>==</span> <span class=s2>&#34;&#34;</span> <span class=o>]]</span><span class=p>;</span> <span class=k>then</span>
</span></span><span class=line><span class=cl>        <span class=nv>err</span><span class=o>=</span><span class=k>$(</span>expr <span class=nv>$err</span> + 1<span class=k>)</span>
</span></span><span class=line><span class=cl>        sleep <span class=m>1</span>
</span></span><span class=line><span class=cl>        <span class=k>continue</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span>
</span></span><span class=line><span class=cl>        <span class=nv>err</span><span class=o>=</span><span class=m>0</span>
</span></span><span class=line><span class=cl>        <span class=nb>break</span>
</span></span><span class=line><span class=cl>    <span class=k>fi</span>
</span></span><span class=line><span class=cl><span class=k>done</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=o>[[</span> <span class=nv>$err</span> !<span class=o>=</span> <span class=s2>&#34;0&#34;</span> <span class=o>]]</span><span class=p>;</span> <span class=k>then</span>
</span></span><span class=line><span class=cl>    <span class=nb>echo</span> <span class=s2>&#34;systemctl stop keepalived&#34;</span>
</span></span><span class=line><span class=cl>    /usr/bin/systemctl stop keepalived
</span></span><span class=line><span class=cl>    <span class=nb>exit</span> <span class=m>1</span>
</span></span><span class=line><span class=cl><span class=k>else</span>
</span></span><span class=line><span class=cl>    <span class=nb>exit</span> <span class=m>0</span>
</span></span><span class=line><span class=cl><span class=k>fi</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 啟動haproxy和keepalived----&gt;所有master節點</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# chmod +x /etc/keepalived/check_apiserver.sh
</span></span><span class=line><span class=cl>root@k8s-master71u:~# systemctl daemon-reload
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# systemctl <span class=nb>enable</span> --now haproxy
</span></span><span class=line><span class=cl>root@k8s-master71u:~# systemctl <span class=nb>enable</span> --now keepalived 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# systemctl restart haproxy
</span></span><span class=line><span class=cl>root@k8s-master71u:~# systemctl restart keepalived 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# systemctl status haproxy
</span></span><span class=line><span class=cl>root@k8s-master71u:~# systemctl status keepalived 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 測試lbip是否生效(從worker節點測試的)</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# telnet 192.168.1.74 <span class=m>7443</span>
</span></span><span class=line><span class=cl>Trying 192.168.1.74...
</span></span><span class=line><span class=cl>Connected to 192.168.1.74.
</span></span><span class=line><span class=cl>Escape character is <span class=s1>&#39;^]&#39;</span>.
</span></span><span class=line><span class=cl>Connection closed by foreign host.
</span></span></code></pre></div><h2 id=佈屬主機etchosts填寫>佈屬主機/etc/hosts填寫</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># vim /etc/hosts</span>
</span></span><span class=line><span class=cl>192.168.1.71 k8s-master71u
</span></span><span class=line><span class=cl>192.168.1.72 k8s-master72u
</span></span><span class=line><span class=cl>192.168.1.73 k8s-master73u
</span></span><span class=line><span class=cl>192.168.1.75 k8s-node75u
</span></span><span class=line><span class=cl>192.168.1.76 k8s-node76u
</span></span></code></pre></div><p>將key推送至要佈屬的主機</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># ssh-copy-id k8s-master71u</span>
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># ssh-copy-id k8s-master72u</span>
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># ssh-copy-id k8s-master73u</span>
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># ssh-copy-id k8s-node75u</span>
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># ssh-copy-id k8s-node76u</span>
</span></span></code></pre></div><h2 id=開始佈屬>開始佈屬</h2><p>中間如果有突然報錯中斷，查看報錯訊息，調整一下，再次執行佈屬即可</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># ansible-playbook -i inventory/mycluster/hosts.yaml  --become --become-user=root cluster.yml</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>PLAY RECAP *****************************************************************************************************************************
</span></span><span class=line><span class=cl>k8s-master71u              : <span class=nv>ok</span><span class=o>=</span><span class=m>746</span>  <span class=nv>changed</span><span class=o>=</span><span class=m>145</span>  <span class=nv>unreachable</span><span class=o>=</span><span class=m>0</span>    <span class=nv>failed</span><span class=o>=</span><span class=m>0</span>    <span class=nv>skipped</span><span class=o>=</span><span class=m>1288</span> <span class=nv>rescued</span><span class=o>=</span><span class=m>0</span>    <span class=nv>ignored</span><span class=o>=</span><span class=m>8</span>
</span></span><span class=line><span class=cl>k8s-master72u              : <span class=nv>ok</span><span class=o>=</span><span class=m>643</span>  <span class=nv>changed</span><span class=o>=</span><span class=m>130</span>  <span class=nv>unreachable</span><span class=o>=</span><span class=m>0</span>    <span class=nv>failed</span><span class=o>=</span><span class=m>0</span>    <span class=nv>skipped</span><span class=o>=</span><span class=m>1138</span> <span class=nv>rescued</span><span class=o>=</span><span class=m>0</span>    <span class=nv>ignored</span><span class=o>=</span><span class=m>3</span>
</span></span><span class=line><span class=cl>k8s-master73u              : <span class=nv>ok</span><span class=o>=</span><span class=m>645</span>  <span class=nv>changed</span><span class=o>=</span><span class=m>131</span>  <span class=nv>unreachable</span><span class=o>=</span><span class=m>0</span>    <span class=nv>failed</span><span class=o>=</span><span class=m>0</span>    <span class=nv>skipped</span><span class=o>=</span><span class=m>1136</span> <span class=nv>rescued</span><span class=o>=</span><span class=m>0</span>    <span class=nv>ignored</span><span class=o>=</span><span class=m>3</span>
</span></span><span class=line><span class=cl>k8s-node75u                : <span class=nv>ok</span><span class=o>=</span><span class=m>467</span>  <span class=nv>changed</span><span class=o>=</span><span class=m>79</span>   <span class=nv>unreachable</span><span class=o>=</span><span class=m>0</span>    <span class=nv>failed</span><span class=o>=</span><span class=m>0</span>    <span class=nv>skipped</span><span class=o>=</span><span class=m>794</span>  <span class=nv>rescued</span><span class=o>=</span><span class=m>0</span>    <span class=nv>ignored</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>k8s-node76u                : <span class=nv>ok</span><span class=o>=</span><span class=m>467</span>  <span class=nv>changed</span><span class=o>=</span><span class=m>79</span>   <span class=nv>unreachable</span><span class=o>=</span><span class=m>0</span>    <span class=nv>failed</span><span class=o>=</span><span class=m>0</span>    <span class=nv>skipped</span><span class=o>=</span><span class=m>793</span>  <span class=nv>rescued</span><span class=o>=</span><span class=m>0</span>    <span class=nv>ignored</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>localhost                  : <span class=nv>ok</span><span class=o>=</span><span class=m>3</span>    <span class=nv>changed</span><span class=o>=</span><span class=m>0</span>    <span class=nv>unreachable</span><span class=o>=</span><span class=m>0</span>    <span class=nv>failed</span><span class=o>=</span><span class=m>0</span>    <span class=nv>skipped</span><span class=o>=</span><span class=m>0</span>    <span class=nv>rescued</span><span class=o>=</span><span class=m>0</span>    <span class=nv>ignored</span><span class=o>=</span><span class=m>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>週日 <span class=m>03</span> 十二月 <span class=m>2023</span>  22:56:39 +0800 <span class=o>(</span>0:00:01.997<span class=o>)</span>       0:44:07.257 ****************
</span></span><span class=line><span class=cl><span class=o>===============================================================================</span>
</span></span><span class=line><span class=cl>download : Download_file <span class=p>|</span> Download item -------------------------------------------------------------------------------------- 114.13s
</span></span><span class=line><span class=cl>container-engine/docker : Ensure docker packages are installed ----------------------------------------------------------------- 80.93s
</span></span><span class=line><span class=cl>download : Download_file <span class=p>|</span> Download item --------------------------------------------------------------------------------------- 59.73s
</span></span><span class=line><span class=cl>download : Download_container <span class=p>|</span> Download image <span class=k>if</span> required --------------------------------------------------------------------- 50.88s
</span></span><span class=line><span class=cl>download : Download_container <span class=p>|</span> Download image <span class=k>if</span> required --------------------------------------------------------------------- 47.73s
</span></span><span class=line><span class=cl>kubernetes/preinstall : Preinstall <span class=p>|</span> <span class=nb>wait</span> <span class=k>for</span> the apiserver to be running ------------------------------------------------------ 46.30s
</span></span><span class=line><span class=cl>download : Download_file <span class=p>|</span> Download item --------------------------------------------------------------------------------------- 44.36s
</span></span><span class=line><span class=cl>container-engine/crictl : Download_file <span class=p>|</span> Download item ------------------------------------------------------------------------ 42.56s
</span></span><span class=line><span class=cl>kubernetes/preinstall : Install packages requirements -------------------------------------------------------------------------- 37.19s
</span></span><span class=line><span class=cl>kubernetes/control-plane : Joining control plane node to the cluster. ---------------------------------------------------------- 34.16s
</span></span><span class=line><span class=cl>container-engine/cri-dockerd : Download_file <span class=p>|</span> Download item ------------------------------------------------------------------- 31.95s
</span></span><span class=line><span class=cl>download : Download_file <span class=p>|</span> Download item --------------------------------------------------------------------------------------- 31.86s
</span></span><span class=line><span class=cl>download : Download_container <span class=p>|</span> Download image <span class=k>if</span> required --------------------------------------------------------------------- 30.03s
</span></span><span class=line><span class=cl>container-engine/validate-container-engine : Populate service facts ------------------------------------------------------------ 27.13s
</span></span><span class=line><span class=cl>kubernetes/control-plane : Kubeadm <span class=p>|</span> Initialize first master ------------------------------------------------------------------- 26.53s
</span></span><span class=line><span class=cl>download : Download_container <span class=p>|</span> Download image <span class=k>if</span> required --------------------------------------------------------------------- 24.24s
</span></span><span class=line><span class=cl>download : Download_file <span class=p>|</span> Download item --------------------------------------------------------------------------------------- 23.99s
</span></span><span class=line><span class=cl>download : Download_container <span class=p>|</span> Download image <span class=k>if</span> required --------------------------------------------------------------------- 23.34s
</span></span><span class=line><span class=cl>download : Download_container <span class=p>|</span> Download image <span class=k>if</span> required --------------------------------------------------------------------- 21.88s
</span></span><span class=line><span class=cl>kubernetes/kubeadm : Join to cluster ------------------------------------------------------------------------------------------- 21.07s
</span></span></code></pre></div><h2 id=功能驗證>功能驗證</h2><h3 id=命令補全>命令補全</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 命令補全</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# <span class=nb>echo</span> <span class=s2>&#34;source &lt;(kubectl completion bash)&#34;</span> &gt;&gt; /etc/profile
</span></span><span class=line><span class=cl>root@k8s-master71u:~# <span class=nb>source</span> /etc/profile
</span></span></code></pre></div><h3 id=etchosts>/etc/hosts</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# cat /etc/hosts
</span></span><span class=line><span class=cl>127.0.0.1 localhost localhost.localdomain
</span></span><span class=line><span class=cl>127.0.1.1 k8s-master71u
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The following lines are desirable for IPv6 capable hosts</span>
</span></span><span class=line><span class=cl>::1 ip6-localhost ip6-loopback localhost6 localhost6.localdomain
</span></span><span class=line><span class=cl>fe00::0 ip6-localnet
</span></span><span class=line><span class=cl>ff00::0 ip6-mcastprefix
</span></span><span class=line><span class=cl>ff02::1 ip6-allnodes
</span></span><span class=line><span class=cl>ff02::2 ip6-allrouters
</span></span><span class=line><span class=cl><span class=c1># Ansible inventory hosts BEGIN</span>
</span></span><span class=line><span class=cl>192.168.1.71 k8s-master71u.cluster.local k8s-master71u
</span></span><span class=line><span class=cl>192.168.1.72 k8s-master72u.cluster.local k8s-master72u
</span></span><span class=line><span class=cl>192.168.1.73 k8s-master73u.cluster.local k8s-master73u
</span></span><span class=line><span class=cl>192.168.1.75 k8s-node75u.cluster.local k8s-node75u
</span></span><span class=line><span class=cl>192.168.1.76 k8s-node76u.cluster.local k8s-node76u
</span></span><span class=line><span class=cl><span class=c1># Ansible inventory hosts END</span>
</span></span><span class=line><span class=cl>192.168.1.74 k8s-apiserver-lb.jimmyhome.tw
</span></span></code></pre></div><h3 id=連線配置文件config>連線配置文件(config)</h3><p>可以看到，都是用VIP的domain name，
k8s-apiserver-lb.jimmyhome.tw</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# cat .kube/config
</span></span><span class=line><span class=cl>apiVersion: v1
</span></span><span class=line><span class=cl>clusters:
</span></span><span class=line><span class=cl>- cluster:
</span></span><span class=line><span class=cl>    certificate-authority-data: <span class=nv>LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1USXdNekUwTkRVME9Wb1hEVE16TVRFek1ERTBORFUwT1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHovCmVJQmpqY2xCd3ZtTWV2YlhsaDV4ZW1HcjVxRlBlRkxsMGhQcERGaW1lcHFGREJkZWl0V0ppTmdGSUFVa3JzaE0KdkdtdXVjNy9mMGt5UUFGVFprTDBzUFI3QUlnbHlyeS9Hc0M1dzRZUEdYTHVpZmRoUGczM09QREdTSWNCejRYMAoyNU1aR1hEcWU2TzhTS2JyOFZidnpNVkxBK1JjbldQVTNIOGR2Znd6N2FxY0N3MlRwdFFxU1VnbVhzS1Z0ZTIzCmgrUGJDMW90OUU0UTYzYXFGQmRWei91aGJ4VHVFYm8wb1c5T3p3c2V1RTJLMzQ0RE1qZWxvZy9rMVdNeFA2OGwKYWRuOC9MbkdPNUJtTlp2UUZ6Q05kaXFkcm1YdXRubUk5N3BYY0dyN0tIdzYzYzZ2dTZ5aGZHMXNjRjg3WG40bQpLZmo5eDRDQ0cvVUJWc2RIV1lzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZFK0lJS2NBcTBLbVl3SUpFYjJDWER4VHFibXhNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRWF3SmEwaEVCZmJ5VG1UazlORQoweGZYUm12SnYrOGpZSlVIZExENmkvYXppQnpKMUNPUlRKKzUxbVYrWmNIZFJWbDE3UkNOZDY4MU9RMkRFWkhWCi83aVphSGxvYldNM3cvUjAxSzdXZm4yQUJsQ09lVkU3d3RKOUc0VEMzK2FVNEhaZFhsdlNLSkt6Y0oxbjZ2V04KMjErbUp1UHdLQXlJTVZEYVB6OGx2QVhxbDR2elBGWTRUd1k1OWlORVZYaFBuUHBiOWhiUGVxTUtSMG80UFBTMgp2YmNlOGw5R016aVpEaEQxb0RDSXBacGE5VzVneXFYdDBTbXk3RVJBWkx2V1N3ZHVrVnJoMXcrWVh3TEFzajF6CkZhNGJKWDExOGdQT3JUVi8rYTNXZXVpYk5JOFNNZm5KSXhHTVduUkpEemVwYW1oUUdiZmtFT3BzdHpRQms2ZGoKd0QwPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg</span><span class=o>==</span>
</span></span><span class=line><span class=cl>    server: https://k8s-apiserver-lb.jimmyhome.tw:7443
</span></span><span class=line><span class=cl>  name: cluster.local
</span></span><span class=line><span class=cl>contexts:
</span></span><span class=line><span class=cl>- context:
</span></span><span class=line><span class=cl>    cluster: cluster.local
</span></span><span class=line><span class=cl>    user: kubernetes-admin
</span></span><span class=line><span class=cl>  name: kubernetes-admin@cluster.local
</span></span><span class=line><span class=cl>current-context: kubernetes-admin@cluster.local
</span></span><span class=line><span class=cl>kind: Config
</span></span><span class=line><span class=cl>preferences: <span class=o>{}</span>
</span></span><span class=line><span class=cl>users:
</span></span><span class=line><span class=cl>- name: kubernetes-admin
</span></span><span class=line><span class=cl>  user:
</span></span><span class=line><span class=cl>    client-certificate-data: <span class=nv>LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJSjc2UTFQWXh4Zm93RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TXpFeU1ETXhORFExTkRsYUZ3MHlOREV5TURJeE5EUTFOVEphTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXVOK0pkMHE1T0xnWVlhaE0Kdnp4YTB0M2E5U2psdTZTZzFUWW5TbE5LRTBYeEQrOTErUmhpV0dUc2o3RlhhNmpLSkd2WE1FT1ZWUVN4UktrQQpwNE8ybDM5L3gyUDlPQ3dZWEovZEJCRFRIQUZBdWNzYWRoa0VjV3gvbnNVZWZQUE44cWxNV3YrN1B3clRzQUpFCmJBalZYSEVERVhDM3VVN2ZWM3lHaEJWVnRHZ29mOFdMdEluN3k3b2ozbnUxbldPY1ZVQ0dhQ09ncXNPdURERGQKVms0QUpyWnFWTWllUDAvWkk0YjhuMkJkZHREbUIyRVl5NmpsZ1M5eVIyYk5nVnNjaW0zMS9MTjQxWEI1YnViYQptbzAyWWIzcE5QN3VzYmtRZndNalhPZ1VrM3U2dks2Ky9hSzQ4WWhndUZvWi9wUVR2ejJtUUpKeldWU2dvYW9mCk5VTTdzUUlEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JSUGlDQ25BS3RDcG1NQ0NSRzlnbHc4VTZtNQpzVEFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBbHQwNXJSVkhhc25GTjh6ZlJDUXVOT2hSKzhxNW5tRnNXU0pUCnFIeW92Uys2NFBkRzBJU21jSTYvanVhTmU1ZzhHdENkTlZoMmVSendLakxZZHc0RGZCUXBTTWh5UEYxZlFLYzEKMmthS2twTzh1MjQ4alZscitaQTV3Q2FaeWh3Qy8yTFFIdGxpdjZnWFNXTjZoNU5IUjNQdzhuWFBJMXQ5Q3JsegpUTCtQcFI3Ti9UUXczOWJqOUgrU0RuZTh2eVNoSVBCeXIzbzhSYXA4K3RMZVF6eVMwWWZNU1BTVnJKZjJMVFFwCnBhaWEyYVF5MVRUcXlicEQxODFsY2RzYzZxOU16cVlseE9zdE5QdG1ES3BTNFdsdFE3bDlLZVVLKzhtdmh0TWwKZXRVZ3lleXdPWmdIcUsrd0lLaC9aMzJScG1LODlub05vOHJWTHdvTGxkMjRFSXpDbmc9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg</span><span class=o>==</span>
</span></span><span class=line><span class=cl>    client-key-data: <span class=nv>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</span><span class=o>=</span>
</span></span></code></pre></div><h3 id=節點ready>節點Ready</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# kubectl get nodes -o wide
</span></span><span class=line><span class=cl>NAME            STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
</span></span><span class=line><span class=cl>k8s-master71u   Ready    control-plane   12m   v1.26.3   192.168.1.71   &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-84-generic   docker://20.10.20
</span></span><span class=line><span class=cl>k8s-master72u   Ready    control-plane   12m   v1.26.3   192.168.1.72   &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-84-generic   docker://20.10.20
</span></span><span class=line><span class=cl>k8s-master73u   Ready    control-plane   12m   v1.26.3   192.168.1.73   &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-84-generic   docker://20.10.20
</span></span><span class=line><span class=cl>k8s-node75u     Ready    &lt;none&gt;          11m   v1.26.3   192.168.1.75   &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-84-generic   docker://20.10.20
</span></span><span class=line><span class=cl>k8s-node76u     Ready    &lt;none&gt;          10m   v1.26.3   192.168.1.76   &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-84-generic   docker://20.10.20
</span></span></code></pre></div><h3 id=master都有污點>master都有污點</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# kubectl get nodes k8s-master71u -o yaml <span class=p>|</span> grep -i -A <span class=m>2</span> taint
</span></span><span class=line><span class=cl>  taints:
</span></span><span class=line><span class=cl>  - effect: NoSchedule
</span></span><span class=line><span class=cl>    key: node-role.kubernetes.io/control-plane
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl get nodes k8s-master72u -o yaml <span class=p>|</span> grep -i -A <span class=m>2</span> taint
</span></span><span class=line><span class=cl>  taints:
</span></span><span class=line><span class=cl>  - effect: NoSchedule
</span></span><span class=line><span class=cl>    key: node-role.kubernetes.io/control-plane
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl get nodes k8s-master73u -o yaml <span class=p>|</span> grep -i -A <span class=m>2</span> taint
</span></span><span class=line><span class=cl>  taints:
</span></span><span class=line><span class=cl>  - effect: NoSchedule
</span></span><span class=line><span class=cl>    key: node-role.kubernetes.io/control-plane
</span></span></code></pre></div><h3 id=helm有安裝>helm有安裝</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# helm version
</span></span><span class=line><span class=cl>version.BuildInfo<span class=o>{</span>Version:<span class=s2>&#34;v3.13.1&#34;</span>, GitCommit:<span class=s2>&#34;3547a4b5bf5edb5478ce352e18858d8a552a4110&#34;</span>, GitTreeState:<span class=s2>&#34;clean&#34;</span>, GoVersion:<span class=s2>&#34;go1.20.8&#34;</span><span class=o>}</span>
</span></span></code></pre></div><h3 id=所有pod都running>所有POD都Running</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# kubectl get pod -A -o wide
</span></span><span class=line><span class=cl>NAMESPACE       NAME                                       READY   STATUS    RESTARTS        AGE     IP               NODE            NOMINATED NODE   READINESS GATES
</span></span><span class=line><span class=cl>ingress-nginx   ingress-nginx-controller-46rbx             1/1     Running   <span class=m>0</span>               8m16s   192.168.1.75     k8s-node75u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>ingress-nginx   ingress-nginx-controller-7ccdp             1/1     Running   <span class=m>0</span>               8m16s   192.168.1.72     k8s-master72u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>ingress-nginx   ingress-nginx-controller-fqvxb             1/1     Running   <span class=m>0</span>               8m16s   192.168.1.71     k8s-master71u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>ingress-nginx   ingress-nginx-controller-hjbv7             1/1     Running   <span class=m>0</span>               8m16s   192.168.1.73     k8s-master73u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>ingress-nginx   ingress-nginx-controller-hlkh8             1/1     Running   <span class=m>0</span>               8m16s   192.168.1.76     k8s-node76u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     calico-kube-controllers-777ff6cddb-d4htx   1/1     Running   <span class=m>0</span>               8m31s   10.201.14.129    k8s-node75u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     calico-node-4qhfj                          1/1     Running   <span class=m>0</span>               9m56s   192.168.1.72     k8s-master72u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     calico-node-9wrql                          1/1     Running   <span class=m>0</span>               9m56s   192.168.1.76     k8s-node76u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     calico-node-9xzmn                          1/1     Running   <span class=m>0</span>               9m56s   192.168.1.75     k8s-node75u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     calico-node-m975z                          1/1     Running   <span class=m>0</span>               9m56s   192.168.1.73     k8s-master73u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     calico-node-s9w4x                          1/1     Running   <span class=m>0</span>               9m56s   192.168.1.71     k8s-master71u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     coredns-57c7559cc8-sq8c4                   1/1     Running   <span class=m>0</span>               7m39s   10.201.96.1      k8s-master73u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     coredns-57c7559cc8-vrf7g                   1/1     Running   <span class=m>0</span>               7m47s   10.201.133.1     k8s-master71u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     dns-autoscaler-9875c9b44-9x7cg             1/1     Running   <span class=m>0</span>               7m43s   10.201.126.1     k8s-master72u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-apiserver-k8s-master71u               1/1     Running   <span class=m>1</span> <span class=o>(</span>3m18s ago<span class=o>)</span>   13m     192.168.1.71     k8s-master71u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-apiserver-k8s-master72u               1/1     Running   <span class=m>1</span> <span class=o>(</span>3m18s ago<span class=o>)</span>   12m     192.168.1.72     k8s-master72u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-apiserver-k8s-master73u               1/1     Running   <span class=m>1</span> <span class=o>(</span>3m18s ago<span class=o>)</span>   11m     192.168.1.73     k8s-master73u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-controller-manager-k8s-master71u      1/1     Running   <span class=m>2</span> <span class=o>(</span>3m49s ago<span class=o>)</span>   13m     192.168.1.71     k8s-master71u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-controller-manager-k8s-master72u      1/1     Running   <span class=m>2</span> <span class=o>(</span>3m49s ago<span class=o>)</span>   12m     192.168.1.72     k8s-master72u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-controller-manager-k8s-master73u      1/1     Running   <span class=m>2</span> <span class=o>(</span>3m49s ago<span class=o>)</span>   12m     192.168.1.73     k8s-master73u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-proxy-6rz9s                           1/1     Running   <span class=m>0</span>               11m     192.168.1.76     k8s-node76u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-proxy-jqvw9                           1/1     Running   <span class=m>0</span>               11m     192.168.1.75     k8s-node75u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-proxy-mtclv                           1/1     Running   <span class=m>0</span>               11m     192.168.1.73     k8s-master73u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-proxy-rsv52                           1/1     Running   <span class=m>0</span>               11m     192.168.1.71     k8s-master71u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-proxy-vmnwj                           1/1     Running   <span class=m>0</span>               11m     192.168.1.72     k8s-master72u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-scheduler-k8s-master71u               1/1     Running   <span class=m>1</span>               13m     192.168.1.71     k8s-master71u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-scheduler-k8s-master72u               1/1     Running   <span class=m>1</span>               12m     192.168.1.72     k8s-master72u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     kube-scheduler-k8s-master73u               1/1     Running   <span class=m>2</span> <span class=o>(</span>3m34s ago<span class=o>)</span>   11m     192.168.1.73     k8s-master73u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     metrics-server-694db59894-4hfz5            1/1     Running   <span class=m>0</span>               6m19s   10.201.255.193   k8s-node76u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     nodelocaldns-5kb8v                         1/1     Running   <span class=m>0</span>               7m40s   192.168.1.72     k8s-master72u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     nodelocaldns-894wn                         1/1     Running   <span class=m>0</span>               7m40s   192.168.1.71     k8s-master71u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     nodelocaldns-lkmlx                         1/1     Running   <span class=m>0</span>               7m40s   192.168.1.75     k8s-node75u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     nodelocaldns-q9q9s                         1/1     Running   <span class=m>0</span>               7m40s   192.168.1.73     k8s-master73u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system     nodelocaldns-qgrqs                         1/1     Running   <span class=m>0</span>               7m40s   192.168.1.76     k8s-node76u     &lt;none&gt;           &lt;none&gt;
</span></span></code></pre></div><h3 id=佈屬kubernetes的yaml>佈屬kubernetes的yaml</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# ll /etc/kubernetes/
</span></span><span class=line><span class=cl>total <span class=m>412</span>
</span></span><span class=line><span class=cl>drwxr-xr-x   <span class=m>5</span> kube root   <span class=m>4096</span> Dec  <span class=m>3</span> 14:51  ./
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>105</span> root root   <span class=m>8192</span> Dec  <span class=m>3</span> 14:44  ../
</span></span><span class=line><span class=cl>drwxr-xr-x   <span class=m>4</span> root root     <span class=m>49</span> Dec  <span class=m>3</span> 14:52  addons/
</span></span><span class=line><span class=cl>-rw-------   <span class=m>1</span> root root   <span class=m>5669</span> Dec  <span class=m>3</span> 14:45  admin.conf
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root   <span class=m>1290</span> Dec  <span class=m>3</span> 14:49  calico-config.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>550</span> Dec  <span class=m>3</span> 14:49  calico-crb.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root   <span class=m>4227</span> Dec  <span class=m>3</span> 14:49  calico-cr.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>159</span> Dec  <span class=m>3</span> 14:49  calico-ipamconfig.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root   <span class=m>1581</span> Dec  <span class=m>3</span> 14:50  calico-kube-controllers.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>301</span> Dec  <span class=m>3</span> 14:50  calico-kube-crb.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root   <span class=m>1733</span> Dec  <span class=m>3</span> 14:50  calico-kube-cr.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>107</span> Dec  <span class=m>3</span> 14:50  calico-kube-sa.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>197</span> Dec  <span class=m>3</span> 14:49  calico-node-sa.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root  <span class=m>11091</span> Dec  <span class=m>3</span> 14:49  calico-node.yml
</span></span><span class=line><span class=cl>-rw-------   <span class=m>1</span> root root   <span class=m>5701</span> Dec  <span class=m>3</span> 14:47  controller-manager.conf
</span></span><span class=line><span class=cl>-rw-------   <span class=m>1</span> root root   <span class=m>5684</span> Dec  <span class=m>3</span> 14:45 <span class=s1>&#39;controller-manager.conf.15377.2023-12-03@14:47:03~&#39;</span>
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>451</span> Dec  <span class=m>3</span> 14:51  coredns-clusterrolebinding.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>473</span> Dec  <span class=m>3</span> 14:51  coredns-clusterrole.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>601</span> Dec  <span class=m>3</span> 14:51  coredns-config.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root   <span class=m>3130</span> Dec  <span class=m>3</span> 14:51  coredns-deployment.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>190</span> Dec  <span class=m>3</span> 14:51  coredns-sa.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>591</span> Dec  <span class=m>3</span> 14:51  coredns-svc.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>959</span> Dec  <span class=m>3</span> 14:51  dns-autoscaler-clusterrolebinding.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root   <span class=m>1150</span> Dec  <span class=m>3</span> 14:51  dns-autoscaler-clusterrole.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>763</span> Dec  <span class=m>3</span> 14:51  dns-autoscaler-sa.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root   <span class=m>2569</span> Dec  <span class=m>3</span> 14:51  dns-autoscaler.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root <span class=m>219969</span> Dec  <span class=m>3</span> 14:49  kdd-crds.yml
</span></span><span class=line><span class=cl>-rw-r-----   <span class=m>1</span> root root   <span class=m>3906</span> Dec  <span class=m>3</span> 14:45  kubeadm-config.yaml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>469</span> Dec  <span class=m>3</span> 14:26  kubeadm-images.yaml
</span></span><span class=line><span class=cl>-rw-------   <span class=m>1</span> root root   <span class=m>2017</span> Dec  <span class=m>3</span> 14:46  kubelet.conf
</span></span><span class=line><span class=cl>-rw-------   <span class=m>1</span> root root    <span class=m>793</span> Dec  <span class=m>3</span> 14:45  kubelet-config.yaml
</span></span><span class=line><span class=cl>-rw-------   <span class=m>1</span> root root    <span class=m>513</span> Dec  <span class=m>3</span> 14:44  kubelet.env
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>113</span> Dec  <span class=m>3</span> 14:49  kubernetes-services-endpoint.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>199</span> Dec  <span class=m>3</span> 14:45  kubescheduler-config.yaml
</span></span><span class=line><span class=cl>drwxr-xr-x   <span class=m>2</span> kube root     <span class=m>96</span> Dec  <span class=m>3</span> 14:45  manifests/
</span></span><span class=line><span class=cl>-rw-r-----   <span class=m>1</span> root root    <span class=m>408</span> Dec  <span class=m>3</span> 14:47  node-crb.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root   <span class=m>1035</span> Dec  <span class=m>3</span> 14:51  nodelocaldns-config.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root   <span class=m>2661</span> Dec  <span class=m>3</span> 14:51  nodelocaldns-daemonset.yml
</span></span><span class=line><span class=cl>-rw-r--r--   <span class=m>1</span> root root    <span class=m>149</span> Dec  <span class=m>3</span> 14:51  nodelocaldns-sa.yml
</span></span><span class=line><span class=cl>lrwxrwxrwx   <span class=m>1</span> root root     <span class=m>19</span> Dec  <span class=m>3</span> 14:14  pki -&gt; /etc/kubernetes/ssl/
</span></span><span class=line><span class=cl>-rw-------   <span class=m>1</span> root root   <span class=m>5649</span> Dec  <span class=m>3</span> 14:47  scheduler.conf
</span></span><span class=line><span class=cl>-rw-------   <span class=m>1</span> root root   <span class=m>5632</span> Dec  <span class=m>3</span> 14:45 <span class=s1>&#39;scheduler.conf.15389.2023-12-03@14:47:04~&#39;</span>
</span></span><span class=line><span class=cl>drwxr-xr-x   <span class=m>2</span> root root   <span class=m>4096</span> Dec  <span class=m>3</span> 14:45  ssl/
</span></span></code></pre></div><h3 id=master組件yaml>master組件yaml</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# ll /etc/kubernetes/manifests/
</span></span><span class=line><span class=cl>total <span class=m>20</span>
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>2</span> kube root   <span class=m>96</span> Dec  <span class=m>3</span> 14:45 ./
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>5</span> kube root <span class=m>4096</span> Dec  <span class=m>3</span> 14:51 ../
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>4727</span> Dec  <span class=m>3</span> 14:45 kube-apiserver.yaml
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>3757</span> Dec  <span class=m>3</span> 14:45 kube-controller-manager.yaml
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1698</span> Dec  <span class=m>3</span> 14:45 kube-scheduler.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# calicoctl get ippool -o wide
</span></span><span class=line><span class=cl>NAME           CIDR            NAT    IPIPMODE   VXLANMODE   DISABLED   DISABLEBGPEXPORT   SELECTOR
</span></span><span class=line><span class=cl>default-pool   10.201.0.0/16   <span class=nb>true</span>   Never      Always      <span class=nb>false</span>      <span class=nb>false</span>              all<span class=o>()</span>
</span></span></code></pre></div><h3 id=測試創建pod>測試創建POD</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# kubectl run  test-nginx --image<span class=o>=</span>nginx:alpine
</span></span><span class=line><span class=cl>pod/test-nginx created
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl get pod -o wide
</span></span><span class=line><span class=cl>NAME         READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
</span></span><span class=line><span class=cl>test-nginx   1/1     Running   <span class=m>0</span>          10s   10.201.14.130   k8s-node75u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# curl 10.201.14.130
</span></span><span class=line><span class=cl>&lt;!DOCTYPE html&gt;
</span></span><span class=line><span class=cl>&lt;html&gt;
</span></span><span class=line><span class=cl>&lt;head&gt;
</span></span><span class=line><span class=cl>&lt;title&gt;Welcome to nginx!&lt;/title&gt;
</span></span><span class=line><span class=cl>&lt;style&gt;
</span></span><span class=line><span class=cl>html <span class=o>{</span> color-scheme: light dark<span class=p>;</span> <span class=o>}</span>
</span></span><span class=line><span class=cl>body <span class=o>{</span> width: 35em<span class=p>;</span> margin: <span class=m>0</span> auto<span class=p>;</span>
</span></span><span class=line><span class=cl>font-family: Tahoma, Verdana, Arial, sans-serif<span class=p>;</span> <span class=o>}</span>
</span></span><span class=line><span class=cl>&lt;/style&gt;
</span></span><span class=line><span class=cl>&lt;/head&gt;
</span></span><span class=line><span class=cl>&lt;body&gt;
</span></span><span class=line><span class=cl>&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
</span></span><span class=line><span class=cl>&lt;p&gt;If you see this page, the nginx web server is successfully installed and
</span></span><span class=line><span class=cl>working. Further configuration is required.&lt;/p&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&lt;p&gt;For online documentation and support please refer to
</span></span><span class=line><span class=cl>&lt;a <span class=nv>href</span><span class=o>=</span><span class=s2>&#34;http://nginx.org/&#34;</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
</span></span><span class=line><span class=cl>Commercial support is available at
</span></span><span class=line><span class=cl>&lt;a <span class=nv>href</span><span class=o>=</span><span class=s2>&#34;http://nginx.com/&#34;</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&lt;p&gt;&lt;em&gt;Thank you <span class=k>for</span> using nginx.&lt;/em&gt;&lt;/p&gt;
</span></span><span class=line><span class=cl>&lt;/body&gt;
</span></span><span class=line><span class=cl>&lt;/html&gt;
</span></span></code></pre></div><h3 id=測試deploymentserviceingress>測試Deployment、Service、Ingress</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# cat deployment2.yaml
</span></span><span class=line><span class=cl>apiVersion: apps/v1
</span></span><span class=line><span class=cl>kind: Deployment
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>  labels:
</span></span><span class=line><span class=cl>    app: web2
</span></span><span class=line><span class=cl>  name: web2
</span></span><span class=line><span class=cl>spec:
</span></span><span class=line><span class=cl>  replicas: <span class=m>3</span>
</span></span><span class=line><span class=cl>  selector:
</span></span><span class=line><span class=cl>    matchLabels:
</span></span><span class=line><span class=cl>      app: web2
</span></span><span class=line><span class=cl>  template:
</span></span><span class=line><span class=cl>    metadata:
</span></span><span class=line><span class=cl>      labels:
</span></span><span class=line><span class=cl>        app: web2
</span></span><span class=line><span class=cl>    spec:
</span></span><span class=line><span class=cl>      containers:
</span></span><span class=line><span class=cl>      - image: nginx
</span></span><span class=line><span class=cl>        name: nginx
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl apply -f deployment2.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# cat service2.yaml
</span></span><span class=line><span class=cl>apiVersion: v1
</span></span><span class=line><span class=cl>kind: Service
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>  labels:
</span></span><span class=line><span class=cl>    app: web2
</span></span><span class=line><span class=cl>  name: web2
</span></span><span class=line><span class=cl>spec:
</span></span><span class=line><span class=cl>  ports:
</span></span><span class=line><span class=cl>  - port: <span class=m>80</span>
</span></span><span class=line><span class=cl>    protocol: TCP
</span></span><span class=line><span class=cl>    targetPort: <span class=m>80</span>
</span></span><span class=line><span class=cl>  selector:
</span></span><span class=line><span class=cl>    app: web2
</span></span><span class=line><span class=cl>  type: NodePort
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl apply -f service2.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# cat ingress.yaml
</span></span><span class=line><span class=cl>apiVersion: networking.k8s.io/v1
</span></span><span class=line><span class=cl>kind: Ingress
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>  name: web2
</span></span><span class=line><span class=cl>spec:
</span></span><span class=line><span class=cl>  ingressClassName: nginx
</span></span><span class=line><span class=cl>  rules:
</span></span><span class=line><span class=cl>  - host: web2.jimmyhome.tw
</span></span><span class=line><span class=cl>    http:
</span></span><span class=line><span class=cl>      paths:
</span></span><span class=line><span class=cl>      - backend:
</span></span><span class=line><span class=cl>          service:
</span></span><span class=line><span class=cl>            name: web2
</span></span><span class=line><span class=cl>            port:
</span></span><span class=line><span class=cl>              number: <span class=m>80</span>
</span></span><span class=line><span class=cl>        path: /
</span></span><span class=line><span class=cl>        pathType: Prefix
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl apply -f ingress.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl get pod,svc,ingress -o wide
</span></span><span class=line><span class=cl>NAME                        READY   STATUS    RESTARTS   AGE    IP               NODE          NOMINATED NODE   READINESS GATES
</span></span><span class=line><span class=cl>pod/test-nginx              1/1     Running   <span class=m>0</span>          3m1s   10.201.14.130    k8s-node75u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>pod/web2-5d48fb75c5-dt5xd   1/1     Running   <span class=m>0</span>          113s   10.201.255.195   k8s-node76u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>pod/web2-5d48fb75c5-ggmrz   1/1     Running   <span class=m>0</span>          113s   10.201.14.131    k8s-node75u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>pod/web2-5d48fb75c5-jsvck   1/1     Running   <span class=m>0</span>          113s   10.201.255.194   k8s-node76u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>        AGE   SELECTOR
</span></span><span class=line><span class=cl>service/kubernetes   ClusterIP   10.202.0.1     &lt;none&gt;        443/TCP        22m   &lt;none&gt;
</span></span><span class=line><span class=cl>service/web2         NodePort    10.202.72.45   &lt;none&gt;        80:31568/TCP   82s   <span class=nv>app</span><span class=o>=</span>web2
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME                             CLASS   HOSTS               ADDRESS   PORTS   AGE
</span></span><span class=line><span class=cl>ingress.networking.k8s.io/web2   nginx   web2.jimmyhome.tw             <span class=m>80</span>      56s
</span></span></code></pre></div><p>以下都可以訪問nodeport
http://192.168.1.71:31568/
http://192.168.1.72:31568/
http://192.168.1.73:31568/
http://192.168.1.74:31568/
http://192.168.1.75:31568/
http://192.168.1.76:31568/</p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203230912.png width=811 height=320 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203230912_hu7591cd073b9c65d38f1d9fab27b35a17_45566_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203230912_hu7591cd073b9c65d38f1d9fab27b35a17_45566_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=253 data-flex-basis=608px></p><p>domain name也可以訪問，透過ingress->service->pod</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>chenqingze@chenqingze-MBP ~ % sudo vim /etc/hosts
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>192.168.1.75 web2.jimmyhome.tw
</span></span></code></pre></div><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203231115.png width=820 height=311 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203231115_hu2f7b49825015a5877cf0de25ec8b551d_45442_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203231115_hu2f7b49825015a5877cf0de25ec8b551d_45442_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=263 data-flex-basis=632px></p><h3 id=etcd服務>etcd服務</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# systemctl status etcd.service
</span></span><span class=line><span class=cl>● etcd.service - etcd
</span></span><span class=line><span class=cl>     Loaded: loaded <span class=o>(</span>/etc/systemd/system/etcd.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
</span></span><span class=line><span class=cl>     Active: active <span class=o>(</span>running<span class=o>)</span> since Sun 2023-12-03 14:43:39 UTC<span class=p>;</span> 28min ago
</span></span><span class=line><span class=cl>   Main PID: <span class=m>13409</span> <span class=o>(</span>etcd<span class=o>)</span>
</span></span><span class=line><span class=cl>      Tasks: <span class=m>14</span> <span class=o>(</span>limit: 9387<span class=o>)</span>
</span></span><span class=line><span class=cl>     Memory: 117.0M
</span></span><span class=line><span class=cl>        CPU: 51.716s
</span></span><span class=line><span class=cl>     CGroup: /system.slice/etcd.service
</span></span><span class=line><span class=cl>             └─13409 /usr/local/bin/etcd
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 14:55:55 k8s-master71u etcd<span class=o>[</span>13409<span class=o>]</span>: <span class=o>{</span><span class=s2>&#34;level&#34;</span>:<span class=s2>&#34;info&#34;</span>,<span class=s2>&#34;ts&#34;</span>:<span class=s2>&#34;2023-12-03T14:55:55.63607Z&#34;</span>,<span class=s2>&#34;caller&#34;</span>:<span class=s2>&#34;mvcc/hash.go:137&#34;</span>,<span class=s2>&#34;msg&#34;</span>:<span class=s2>&#34;storing new hash&#34;</span>,<span class=s2>&#34;hash&#34;</span>:1201138327,<span class=s2>&#34;re&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 15:06:09 k8s-master71u etcd[13409]: {&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:&#34;</span>2023-12-03T15:06:09.854317Z<span class=s2>&#34;,&#34;</span>caller<span class=s2>&#34;:&#34;</span>mvcc/index.go:214<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>compact tree index<span class=s2>&#34;,&#34;</span>revision<span class=s2>&#34;:3724}
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 15:06:09 k8s-master71u etcd[13409]: {&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:&#34;</span>2023-12-03T15:06:09.942986Z<span class=s2>&#34;,&#34;</span>caller<span class=s2>&#34;:&#34;</span>mvcc/kvstore_compaction.go:66<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>finished scheduled compac&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 15:06:09 k8s-master71u etcd<span class=o>[</span>13409<span class=o>]</span>: <span class=o>{</span><span class=s2>&#34;level&#34;</span>:<span class=s2>&#34;info&#34;</span>,<span class=s2>&#34;ts&#34;</span>:<span class=s2>&#34;2023-12-03T15:06:09.943036Z&#34;</span>,<span class=s2>&#34;caller&#34;</span>:<span class=s2>&#34;mvcc/hash.go:137&#34;</span>,<span class=s2>&#34;msg&#34;</span>:<span class=s2>&#34;storing new hash&#34;</span>,<span class=s2>&#34;hash&#34;</span>:2096861132,<span class=s2>&#34;r&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 15:07:01 k8s-master71u etcd[13409]: {&#34;</span>level<span class=s2>&#34;:&#34;</span>warn<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:&#34;</span>2023-12-03T15:07:01.091549Z<span class=s2>&#34;,&#34;</span>caller<span class=s2>&#34;:&#34;</span>etcdserver/util.go:170<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>apply request took too long<span class=s2>&#34;,&#34;</span>to&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 15:07:01 k8s-master71u etcd<span class=o>[</span>13409<span class=o>]</span>: <span class=o>{</span><span class=s2>&#34;level&#34;</span>:<span class=s2>&#34;info&#34;</span>,<span class=s2>&#34;ts&#34;</span>:<span class=s2>&#34;2023-12-03T15:07:01.091613Z&#34;</span>,<span class=s2>&#34;caller&#34;</span>:<span class=s2>&#34;traceutil/trace.go:171&#34;</span>,<span class=s2>&#34;msg&#34;</span>:<span class=s2>&#34;trace[2099372944] range&#34;</span>,<span class=s2>&#34;detail&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 15:07:02 k8s-master71u etcd[13409]: {&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:&#34;</span>2023-12-03T15:07:02.663992Z<span class=s2>&#34;,&#34;</span>caller<span class=s2>&#34;:&#34;</span>traceutil/trace.go:171<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>trace<span class=o>[</span>1476779153<span class=o>]</span> transaction<span class=s2>&#34;,&#34;</span>&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 15:11:09 k8s-master71u etcd<span class=o>[</span>13409<span class=o>]</span>: <span class=o>{</span><span class=s2>&#34;level&#34;</span>:<span class=s2>&#34;info&#34;</span>,<span class=s2>&#34;ts&#34;</span>:<span class=s2>&#34;2023-12-03T15:11:09.86509Z&#34;</span>,<span class=s2>&#34;caller&#34;</span>:<span class=s2>&#34;mvcc/index.go:214&#34;</span>,<span class=s2>&#34;msg&#34;</span>:<span class=s2>&#34;compact tree index&#34;</span>,<span class=s2>&#34;revision&#34;</span>:4506<span class=o>}</span>
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 15:11:09 k8s-master71u etcd<span class=o>[</span>13409<span class=o>]</span>: <span class=o>{</span><span class=s2>&#34;level&#34;</span>:<span class=s2>&#34;info&#34;</span>,<span class=s2>&#34;ts&#34;</span>:<span class=s2>&#34;2023-12-03T15:11:09.890206Z&#34;</span>,<span class=s2>&#34;caller&#34;</span>:<span class=s2>&#34;mvcc/kvstore_compaction.go:66&#34;</span>,<span class=s2>&#34;msg&#34;</span>:<span class=s2>&#34;finished scheduled compac&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 15:11:09 k8s-master71u etcd[13409]: {&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:&#34;</span>2023-12-03T15:11:09.890258Z<span class=s2>&#34;,&#34;</span>caller<span class=s2>&#34;:&#34;</span>mvcc/hash.go:137<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>storing new hash<span class=s2>&#34;,&#34;</span>hash<span class=s2>&#34;:2253005372,&#34;</span>r&gt;
</span></span></code></pre></div><h3 id=cri-dockerd服務>cri-dockerd服務</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# systemctl status cri-dockerd
</span></span><span class=line><span class=cl>● cri-dockerd.service - CRI Interface <span class=k>for</span> Docker Application Container Engine
</span></span><span class=line><span class=cl>     Loaded: loaded <span class=o>(</span>/etc/systemd/system/cri-dockerd.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
</span></span><span class=line><span class=cl>     Active: active <span class=o>(</span>running<span class=o>)</span> since Sun 2023-12-03 14:25:01 UTC<span class=p>;</span> 46min ago
</span></span><span class=line><span class=cl>TriggeredBy: ● cri-dockerd.socket
</span></span><span class=line><span class=cl>       Docs: https://docs.mirantis.com
</span></span><span class=line><span class=cl>   Main PID: <span class=m>8096</span> <span class=o>(</span>cri-dockerd<span class=o>)</span>
</span></span><span class=line><span class=cl>      Tasks: <span class=m>14</span>
</span></span><span class=line><span class=cl>     Memory: 19.4M
</span></span><span class=line><span class=cl>        CPU: 38.327s
</span></span><span class=line><span class=cl>     CGroup: /system.slice/cri-dockerd.service
</span></span><span class=line><span class=cl>             └─8096 /usr/local/bin/cri-dockerd --container-runtime-endpoint unix:///var/run/cri-dockerd.sock --cni-conf-dir<span class=o>=</span>/etc/cni/net.d --cni-bin-dir<span class=o>=</span>/opt/cni/bin -&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 14:51:41 k8s-master71u cri-dockerd<span class=o>[</span>8096<span class=o>]</span>: 2023-12-03 14:51:41.774 <span class=o>[</span>INFO<span class=o>][</span>21767<span class=o>]</span> dataplane_linux.go 473: Disabling IPv4 forwarding <span class=nv>ContainerID</span><span class=o>=</span><span class=s2>&#34;b05ee1aac5d857271&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 14:51:41 k8s-master71u cri-dockerd[8096]: 2023-12-03 14:51:41.804 [INFO][21767] k8s.go 411: Added Mac, interface name, and active container ID to endpoint Conta&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 14:51:41 k8s-master71u cri-dockerd[8096]: 2023-12-03 14:51:41.825 [INFO][21767] k8s.go 489: Wrote updated endpoint to datastore ContainerID=&#34;</span>b05ee1aac5d85727107&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 14:51:41 k8s-master71u cri-dockerd<span class=o>[</span>8096<span class=o>]</span>: <span class=nv>time</span><span class=o>=</span><span class=s2>&#34;2023-12-03T14:51:41Z&#34;</span> <span class=nv>level</span><span class=o>=</span>info <span class=nv>msg</span><span class=o>=</span><span class=s2>&#34;Pulling image registry.k8s.io/ingress-nginx/controller:v1.9.4: 05535d57e64&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 14:51:51 k8s-master71u cri-dockerd[8096]: time=&#34;</span>2023-12-03T14:51:51Z<span class=s2>&#34; level=info msg=&#34;</span>Pulling image registry.k8s.io/ingress-nginx/controller:v1.9.4: 3da4cd537b4&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 14:51:52 k8s-master71u cri-dockerd<span class=o>[</span>8096<span class=o>]</span>: <span class=nv>time</span><span class=o>=</span><span class=s2>&#34;2023-12-03T14:51:52Z&#34;</span> <span class=nv>level</span><span class=o>=</span>info <span class=nv>msg</span><span class=o>=</span><span class=s2>&#34;Will attempt to re-write config file /var/lib/docker/containers/e5ebeb0472&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 14:51:53 k8s-master71u cri-dockerd[8096]: time=&#34;</span>2023-12-03T14:51:53Z<span class=s2>&#34; level=info msg=&#34;</span>Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.9.4: Status&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 14:55:33 k8s-master71u cri-dockerd<span class=o>[</span>8096<span class=o>]</span>: <span class=nv>time</span><span class=o>=</span><span class=s2>&#34;2023-12-03T14:55:33Z&#34;</span> <span class=nv>level</span><span class=o>=</span>info <span class=nv>msg</span><span class=o>=</span><span class=s2>&#34;Docker cri received runtime config &amp;RuntimeConfig{NetworkConfig:&amp;NetworkCo&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 14:55:37 k8s-master71u cri-dockerd[8096]: time=&#34;</span>2023-12-03T14:55:37Z<span class=s2>&#34; level=info msg=&#34;</span>Will attempt to re-write config file /var/lib/docker/containers/f23d464928&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 14:56:08 k8s-master71u cri-dockerd<span class=o>[</span>8096<span class=o>]</span>: <span class=nv>time</span><span class=o>=</span><span class=s2>&#34;2023-12-03T14:56:08Z&#34;</span> <span class=nv>level</span><span class=o>=</span>info <span class=nv>msg</span><span class=o>=</span><span class=s2>&#34;Will attempt to re-write config file /var/lib/docker/containers/651f69131d&gt;
</span></span></span></code></pre></div><h3 id=kubelet服務>kubelet服務</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# systemctl status kubelet
</span></span><span class=line><span class=cl>● kubelet.service - Kubernetes Kubelet Server
</span></span><span class=line><span class=cl>     Loaded: loaded <span class=o>(</span>/etc/systemd/system/kubelet.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
</span></span><span class=line><span class=cl>     Active: active <span class=o>(</span>running<span class=o>)</span> since Sun 2023-12-03 14:55:33 UTC<span class=p>;</span> 16min ago
</span></span><span class=line><span class=cl>       Docs: https://github.com/GoogleCloudPlatform/kubernetes
</span></span><span class=line><span class=cl>   Main PID: <span class=m>27162</span> <span class=o>(</span>kubelet<span class=o>)</span>
</span></span><span class=line><span class=cl>      Tasks: <span class=m>16</span> <span class=o>(</span>limit: 9387<span class=o>)</span>
</span></span><span class=line><span class=cl>     Memory: 42.9M
</span></span><span class=line><span class=cl>        CPU: 23.201s
</span></span><span class=line><span class=cl>     CGroup: /system.slice/kubelet.service
</span></span><span class=line><span class=cl>             └─27162 /usr/local/bin/kubelet --v<span class=o>=</span><span class=m>2</span> --node-ip<span class=o>=</span>192.168.1.71 --hostname-override<span class=o>=</span>k8s-master71u --bootstrap-kubeconfig<span class=o>=</span>/etc/kubernetes/bootstrap-kubelet.con&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 15:11:31 k8s-master71u kubelet<span class=o>[</span>27162<span class=o>]</span>: E1203 15:11:31.892549   <span class=m>27162</span> cri_stats_provider.go:643<span class=o>]</span> <span class=s2>&#34;Unable to fetch container log stats&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;failed to get fsstats&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 15:11:33 k8s-master71u kubelet[27162]: I1203 15:11:33.187859   27162 kubelet_getters.go:182] &#34;</span>Pod status updated<span class=s2>&#34; pod=&#34;</span>kube-system/kube-apiserver-k8s-master71u<span class=s2>&#34;&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 15:11:33 k8s-master71u kubelet[27162]: I1203 15:11:33.187909   27162 kubelet_getters.go:182] &#34;</span>Pod status updated<span class=s2>&#34; pod=&#34;</span>kube-system/kube-controller-manager-k8s-m&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 15:11:33 k8s-master71u kubelet<span class=o>[</span>27162<span class=o>]</span>: I1203 15:11:33.187921   <span class=m>27162</span> kubelet_getters.go:182<span class=o>]</span> <span class=s2>&#34;Pod status updated&#34;</span> <span class=nv>pod</span><span class=o>=</span><span class=s2>&#34;kube-system/kube-scheduler-k8s-master71u&#34;</span>&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 15:11:42 k8s-master71u kubelet<span class=o>[</span>27162<span class=o>]</span>: E1203 15:11:42.968925   <span class=m>27162</span> cri_stats_provider.go:643<span class=o>]</span> <span class=s2>&#34;Unable to fetch container log stats&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;failed to get fsstats&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 15:11:42 k8s-master71u kubelet[27162]: E1203 15:11:42.969222   27162 cri_stats_provider.go:643] &#34;</span>Unable to fetch container log stats<span class=s2>&#34; err=&#34;</span>failed to get fsstats&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 15:11:54 k8s-master71u kubelet<span class=o>[</span>27162<span class=o>]</span>: E1203 15:11:54.050355   <span class=m>27162</span> cri_stats_provider.go:643<span class=o>]</span> <span class=s2>&#34;Unable to fetch container log stats&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;failed to get fsstats&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 15:11:54 k8s-master71u kubelet[27162]: E1203 15:11:54.050462   27162 cri_stats_provider.go:643] &#34;</span>Unable to fetch container log stats<span class=s2>&#34; err=&#34;</span>failed to get fsstats&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>03</span> 15:12:05 k8s-master71u kubelet<span class=o>[</span>27162<span class=o>]</span>: E1203 15:12:05.124399   <span class=m>27162</span> cri_stats_provider.go:643<span class=o>]</span> <span class=s2>&#34;Unable to fetch container log stats&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;failed to get fsstats&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 03 15:12:05 k8s-master71u kubelet[27162]: E1203 15:12:05.124943   27162 cri_stats_provider.go:643] &#34;</span>Unable to fetch container log stats<span class=s2>&#34; err=&#34;</span>failed to get fsstats&gt;
</span></span></code></pre></div><h3 id=憑證與金鑰>憑證與金鑰</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# <span class=nb>cd</span> /etc/kubernetes/pki
</span></span><span class=line><span class=cl>root@k8s-master71u:/etc/kubernetes/pki# ll
</span></span><span class=line><span class=cl>total <span class=m>56</span>
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>2</span> root root <span class=m>4096</span> Dec  <span class=m>3</span> 14:45 ./
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>5</span> kube root <span class=m>4096</span> Dec  <span class=m>3</span> 14:51 ../
</span></span><span class=line><span class=cl>-rw-r--r-- <span class=m>1</span> root root <span class=m>1428</span> Dec  <span class=m>3</span> 14:45 apiserver.crt
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1679</span> Dec  <span class=m>3</span> 14:45 apiserver.key
</span></span><span class=line><span class=cl>-rw-r--r-- <span class=m>1</span> root root <span class=m>1164</span> Dec  <span class=m>3</span> 14:45 apiserver-kubelet-client.crt
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1679</span> Dec  <span class=m>3</span> 14:45 apiserver-kubelet-client.key
</span></span><span class=line><span class=cl>-rw-r--r-- <span class=m>1</span> root root <span class=m>1099</span> Dec  <span class=m>3</span> 14:45 ca.crt
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1675</span> Dec  <span class=m>3</span> 14:45 ca.key
</span></span><span class=line><span class=cl>-rw-r--r-- <span class=m>1</span> root root <span class=m>1115</span> Dec  <span class=m>3</span> 14:45 front-proxy-ca.crt
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1675</span> Dec  <span class=m>3</span> 14:45 front-proxy-ca.key
</span></span><span class=line><span class=cl>-rw-r--r-- <span class=m>1</span> root root <span class=m>1119</span> Dec  <span class=m>3</span> 14:45 front-proxy-client.crt
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1675</span> Dec  <span class=m>3</span> 14:45 front-proxy-client.key
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1679</span> Dec  <span class=m>3</span> 14:45 sa.key
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root  <span class=m>451</span> Dec  <span class=m>3</span> 14:45 sa.pub
</span></span><span class=line><span class=cl>root@k8s-master71u:/etc/kubernetes/pki# <span class=k>for</span> i in <span class=k>$(</span>ls *.crt<span class=k>)</span><span class=p>;</span> <span class=k>do</span> <span class=nb>echo</span> <span class=s2>&#34;===== </span><span class=nv>$i</span><span class=s2> =====&#34;</span><span class=p>;</span> openssl x509 -in <span class=nv>$i</span> -text -noout <span class=p>|</span> grep -A <span class=m>3</span> <span class=s1>&#39;Validity&#39;</span> <span class=p>;</span> <span class=k>done</span>
</span></span><span class=line><span class=cl><span class=o>=====</span> apiserver.crt <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:45:49 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Dec  <span class=m>2</span> 14:45:50 <span class=m>2024</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> kube-apiserver
</span></span><span class=line><span class=cl><span class=o>=====</span> apiserver-kubelet-client.crt <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:45:49 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Dec  <span class=m>2</span> 14:45:50 <span class=m>2024</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>O</span> <span class=o>=</span> system:masters, <span class=nv>CN</span> <span class=o>=</span> kube-apiserver-kubelet-client
</span></span><span class=line><span class=cl><span class=o>=====</span> ca.crt <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:45:49 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Nov <span class=m>30</span> 14:45:49 <span class=m>2033</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> <span class=nv>kubernetes</span>
</span></span><span class=line><span class=cl><span class=o>=====</span> front-proxy-ca.crt <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:45:50 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Nov <span class=m>30</span> 14:45:50 <span class=m>2033</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> front-proxy-ca
</span></span><span class=line><span class=cl><span class=o>=====</span> front-proxy-client.crt <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:45:50 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Dec  <span class=m>2</span> 14:45:51 <span class=m>2024</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> front-proxy-client
</span></span></code></pre></div><h3 id=etcd憑證與金鑰>etcd憑證與金鑰</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:/etc/kubernetes/pki# <span class=nb>cd</span> /etc/ssl/etcd/ssl/
</span></span><span class=line><span class=cl>root@k8s-master71u:/etc/ssl/etcd/ssl# ll
</span></span><span class=line><span class=cl>total <span class=m>84</span>
</span></span><span class=line><span class=cl>drwx------ <span class=m>2</span> etcd root <span class=m>4096</span> Dec  <span class=m>3</span> 14:42 ./
</span></span><span class=line><span class=cl>drwx------ <span class=m>3</span> etcd root   <span class=m>37</span> Dec  <span class=m>3</span> 14:42 ../
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1704</span> Dec  <span class=m>3</span> 14:42 admin-k8s-master71u-key.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1476</span> Dec  <span class=m>3</span> 14:42 admin-k8s-master71u.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1704</span> Dec  <span class=m>3</span> 14:42 admin-k8s-master72u-key.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1476</span> Dec  <span class=m>3</span> 14:42 admin-k8s-master72u.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1708</span> Dec  <span class=m>3</span> 14:42 admin-k8s-master73u-key.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1476</span> Dec  <span class=m>3</span> 14:42 admin-k8s-master73u.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1704</span> Dec  <span class=m>3</span> 14:42 ca-key.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1111</span> Dec  <span class=m>3</span> 14:42 ca.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1704</span> Dec  <span class=m>3</span> 14:42 member-k8s-master71u-key.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1480</span> Dec  <span class=m>3</span> 14:42 member-k8s-master71u.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1704</span> Dec  <span class=m>3</span> 14:42 member-k8s-master72u-key.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1480</span> Dec  <span class=m>3</span> 14:42 member-k8s-master72u.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1704</span> Dec  <span class=m>3</span> 14:42 member-k8s-master73u-key.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1480</span> Dec  <span class=m>3</span> 14:42 member-k8s-master73u.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1704</span> Dec  <span class=m>3</span> 14:42 node-k8s-master71u-key.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1476</span> Dec  <span class=m>3</span> 14:42 node-k8s-master71u.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1704</span> Dec  <span class=m>3</span> 14:42 node-k8s-master72u-key.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1476</span> Dec  <span class=m>3</span> 14:42 node-k8s-master72u.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1704</span> Dec  <span class=m>3</span> 14:42 node-k8s-master73u-key.pem*
</span></span><span class=line><span class=cl>-rwx------ <span class=m>1</span> etcd root <span class=m>1476</span> Dec  <span class=m>3</span> 14:42 node-k8s-master73u.pem*
</span></span><span class=line><span class=cl>root@k8s-master71u:/etc/ssl/etcd/ssl# <span class=k>for</span> i in <span class=k>$(</span>ls *.pem <span class=p>|</span> egrep -v <span class=s2>&#34;key&#34;</span><span class=k>)</span><span class=p>;</span> <span class=k>do</span> <span class=nb>echo</span> <span class=s2>&#34;===== </span><span class=nv>$i</span><span class=s2> =====&#34;</span><span class=p>;</span> openssl x509 -in <span class=nv>$i</span> -text -noout <span class=p>|</span> grep -A <span class=m>3</span> <span class=s1>&#39;Validity&#39;</span> <span class=p>;</span> <span class=k>done</span>
</span></span><span class=line><span class=cl><span class=o>=====</span> admin-k8s-master71u.pem <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:42:05 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Nov  <span class=m>9</span> 14:42:05 <span class=m>2123</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> etcd-admin-k8s-master71u
</span></span><span class=line><span class=cl><span class=o>=====</span> admin-k8s-master72u.pem <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:42:06 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Nov  <span class=m>9</span> 14:42:06 <span class=m>2123</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> etcd-admin-k8s-master72u
</span></span><span class=line><span class=cl><span class=o>=====</span> admin-k8s-master73u.pem <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:42:07 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Nov  <span class=m>9</span> 14:42:07 <span class=m>2123</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> etcd-admin-k8s-master73u
</span></span><span class=line><span class=cl><span class=o>=====</span> ca.pem <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:42:05 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Nov  <span class=m>9</span> 14:42:05 <span class=m>2123</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> etcd-ca
</span></span><span class=line><span class=cl><span class=o>=====</span> member-k8s-master71u.pem <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:42:05 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Nov  <span class=m>9</span> 14:42:05 <span class=m>2123</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> etcd-member-k8s-master71u
</span></span><span class=line><span class=cl><span class=o>=====</span> member-k8s-master72u.pem <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:42:06 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Nov  <span class=m>9</span> 14:42:06 <span class=m>2123</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> etcd-member-k8s-master72u
</span></span><span class=line><span class=cl><span class=o>=====</span> member-k8s-master73u.pem <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:42:06 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Nov  <span class=m>9</span> 14:42:06 <span class=m>2123</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> etcd-member-k8s-master73u
</span></span><span class=line><span class=cl><span class=o>=====</span> node-k8s-master71u.pem <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:42:07 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Nov  <span class=m>9</span> 14:42:07 <span class=m>2123</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> etcd-node-k8s-master71u
</span></span><span class=line><span class=cl><span class=o>=====</span> node-k8s-master72u.pem <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:42:07 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Nov  <span class=m>9</span> 14:42:07 <span class=m>2123</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> etcd-node-k8s-master72u
</span></span><span class=line><span class=cl><span class=o>=====</span> node-k8s-master73u.pem <span class=o>=====</span>
</span></span><span class=line><span class=cl>        Validity
</span></span><span class=line><span class=cl>            Not Before: Dec  <span class=m>3</span> 14:42:07 <span class=m>2023</span> GMT
</span></span><span class=line><span class=cl>            Not After : Nov  <span class=m>9</span> 14:42:07 <span class=m>2123</span> GMT
</span></span><span class=line><span class=cl>        Subject: <span class=nv>CN</span> <span class=o>=</span> etcd-node-k8s-master73u
</span></span></code></pre></div><h3 id=安裝rancher管理平台高可用>安裝rancher管理平台(高可用)</h3><p>詳細操作可以參考我這篇：
<a class=link href=https://blog.goldfishbrain-fighting.com/2023/kubernetes-helm/ target=_blank rel=noopener>https://blog.goldfishbrain-fighting.com/2023/kubernetes-helm/</a></p><h4 id=一添加-helm-chart-倉庫>一、添加 Helm Chart 倉庫</h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=s2>&#34;rancher-stable&#34;</span> has been added to your repositories
</span></span></code></pre></div><h4 id=二為-rancher-建立命名空間>二、為 Rancher 建立命名空間</h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# kubectl create namespace cattle-system
</span></span></code></pre></div><h4 id=三選擇-ssl-配置>三、選擇 SSL 配置</h4><p>Rancher 產生的憑證（預設）
需要 cert-manager</p><h4 id=四安裝-cert-manager>四、安裝 cert-manager</h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 如果你手動安裝了CRD，而不是在 Helm 安裝命令中添加了 &#39;--set installCRDs=true&#39; 選項，你應該在升級 Helm Chart 之前升級 CRD 資源。</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.crds.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 添加 Jetstack Helm 倉庫</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# helm repo add jetstack https://charts.jetstack.io
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 更新本地 Helm Chart 倉庫緩存</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# helm repo update
</span></span><span class=line><span class=cl>Hang tight <span class=k>while</span> we grab the latest from your chart repositories...
</span></span><span class=line><span class=cl>...Successfully got an update from the <span class=s2>&#34;jetstack&#34;</span> chart repository
</span></span><span class=line><span class=cl>...Successfully got an update from the <span class=s2>&#34;rancher-stable&#34;</span> chart repository
</span></span><span class=line><span class=cl>Update Complete. ⎈Happy Helming!⎈
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 安裝 cert-manager Helm Chart</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# helm install cert-manager jetstack/cert-manager <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace cert-manager <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --version v1.11.0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 安裝完 cert-manager 后，你可以通過檢查 cert-manager 命名空間中正在運行的 Pod 來驗證它是否已正確部署</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl get pods --namespace cert-manager
</span></span><span class=line><span class=cl>NAME                                       READY   STATUS    RESTARTS   AGE
</span></span><span class=line><span class=cl>cert-manager-64f9f45d6f-vklzz              1/1     Running   <span class=m>0</span>          37s
</span></span><span class=line><span class=cl>cert-manager-cainjector-56bbdd5c47-wv2l7   1/1     Running   <span class=m>0</span>          37s
</span></span><span class=line><span class=cl>cert-manager-webhook-d4f4545d7-pw2cp       1/1     Running   <span class=m>0</span>          37s
</span></span></code></pre></div><h4 id=五根據你選擇的證書選項通過-helm-安裝-rancher>五、根據你選擇的證書選項，通過 Helm 安裝 Rancher</h4><p>採Rancher 生成的證書方式</p><p>Rancher Helm Chart 選項：
<a class=link href=https://ranchermanager.docs.rancher.com/zh/getting-started/installation-and-upgrade/installation-references/helm-chart-options target=_blank rel=noopener>https://ranchermanager.docs.rancher.com/zh/getting-started/installation-and-upgrade/installation-references/helm-chart-options</a></p><p><font color=red>需多指定ingressClassName為nginx</font></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# helm install rancher rancher-stable/rancher <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace cattle-system <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set <span class=nv>hostname</span><span class=o>=</span>rancher.jimmyhome.tw <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set <span class=nv>bootstrapPassword</span><span class=o>=</span>admin <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set ingress.ingressClassName<span class=o>=</span>nginx
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME: rancher
</span></span><span class=line><span class=cl>LAST DEPLOYED: Sun Dec  <span class=m>3</span> 15:40:43 <span class=m>2023</span>
</span></span><span class=line><span class=cl>NAMESPACE: cattle-system
</span></span><span class=line><span class=cl>STATUS: deployed
</span></span><span class=line><span class=cl>REVISION: <span class=m>1</span>
</span></span><span class=line><span class=cl>TEST SUITE: None
</span></span><span class=line><span class=cl>NOTES:
</span></span><span class=line><span class=cl>Rancher Server has been installed.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NOTE: Rancher may take several minutes to fully initialize. Please standby <span class=k>while</span> Certificates are being issued, Containers are started and the Ingress rule comes up.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Check out our docs at https://rancher.com/docs/
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>If you provided your own bootstrap password during installation, browse to https://rancher.jimmyhome.tw to get started.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>If this is the first <span class=nb>time</span> you installed Rancher, get started by running this <span class=nb>command</span> and clicking the URL it generates:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>echo</span> https://rancher.jimmyhome.tw/dashboard/?setup<span class=o>=</span><span class=k>$(</span>kubectl get secret --namespace cattle-system bootstrap-secret -o go-template<span class=o>=</span><span class=s1>&#39;{{.data.bootstrapPassword|base64decode}}&#39;</span><span class=k>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>To get just the bootstrap password on its own, run:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl get secret --namespace cattle-system bootstrap-secret -o go-template<span class=o>=</span><span class=s1>&#39;{{.data.bootstrapPassword|base64decode}}{{ &#34;\n&#34; }}&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Happy Containering!
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 等待 Rancher 運行</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl -n cattle-system rollout status deploy/rancher
</span></span><span class=line><span class=cl>Waiting <span class=k>for</span> deployment <span class=s2>&#34;rancher&#34;</span> rollout to finish: <span class=m>0</span> of <span class=m>3</span> updated replicas are available...
</span></span><span class=line><span class=cl>Waiting <span class=k>for</span> deployment <span class=s2>&#34;rancher&#34;</span> rollout to finish: <span class=m>1</span> of <span class=m>3</span> updated replicas are available...
</span></span><span class=line><span class=cl>Waiting <span class=k>for</span> deployment <span class=s2>&#34;rancher&#34;</span> rollout to finish: <span class=m>2</span> of <span class=m>3</span> updated replicas are available...
</span></span><span class=line><span class=cl>deployment <span class=s2>&#34;rancher&#34;</span> successfully rolled out
</span></span></code></pre></div><h4 id=六驗證-rancher-server-是否部署成功>六、驗證 Rancher Server 是否部署成功</h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# kubectl -n cattle-system get deploy
</span></span><span class=line><span class=cl>NAME      READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span class=line><span class=cl>rancher   3/3     <span class=m>3</span>            <span class=m>3</span>           5m2s
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl -n cattle-system get pod -o wide
</span></span><span class=line><span class=cl>NAME                       READY   STATUS    RESTARTS   AGE     IP               NODE          NOMINATED NODE   READINESS GATES
</span></span><span class=line><span class=cl>helm-operation-hpd69       2/2     Running   <span class=m>0</span>          40s     10.201.14.138    k8s-node75u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>rancher-64cf6ddd96-pcfv5   1/1     Running   <span class=m>0</span>          5m11s   10.201.255.201   k8s-node76u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>rancher-64cf6ddd96-q7dhp   1/1     Running   <span class=m>0</span>          5m11s   10.201.255.202   k8s-node76u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>rancher-64cf6ddd96-rr28z   1/1     Running   <span class=m>0</span>          5m11s   10.201.14.137    k8s-node75u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl -n cattle-system get ingress
</span></span><span class=line><span class=cl>NAME      CLASS   HOSTS                  ADDRESS   PORTS     AGE
</span></span><span class=line><span class=cl>rancher   nginx   rancher.jimmyhome.tw             80, <span class=m>443</span>   5m35s
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 我自己的筆電，先設定/etc/hosts測試網頁訪問</span>
</span></span><span class=line><span class=cl><span class=c1># 如是內部有dns server的，就到dns server設定a record解析</span>
</span></span><span class=line><span class=cl>chenqingze@chenqingze-MBP ~ % sudo vim /etc/hosts
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>192.168.1.75 rancher.jimmyhome.tw
</span></span></code></pre></div><p>打 <a class=link href=https://rancher.jimmyhome.tw target=_blank rel=noopener>https://rancher.jimmyhome.tw</a> 可以看到網頁</p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203234722.png width=736 height=888 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203234722_hu9f8591ad2fe3fbf363ffc1cb19ed2d21_92259_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203234722_hu9f8591ad2fe3fbf363ffc1cb19ed2d21_92259_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=82 data-flex-basis=198px></p><p>由上方helm install rancher完成時，可以看到以下兩個指令，可以獲取到預設admin帳號的密碼</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># If this is the first time you installed Rancher, get started by running this command and clicking the URL it generates</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# <span class=nb>echo</span> https://rancher.jimmyhome.tw/dashboard/?setup<span class=o>=</span><span class=k>$(</span>kubectl get secret --namespace cattle-system bootstrap-secret -o go-template<span class=o>=</span><span class=s1>&#39;{{.data.bootstrapPassword|base64decode}}&#39;</span><span class=k>)</span>
</span></span><span class=line><span class=cl>https://rancher.jimmyhome.tw/dashboard/?setup<span class=o>=</span>admin
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># To get just the bootstrap password on its own, run</span>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl get secret --namespace cattle-system bootstrap-secret -o go-template<span class=o>=</span><span class=s1>&#39;{{.data.bootstrapPassword|base64decode}}{{ &#34;\n&#34; }}&#39;</span>
</span></span><span class=line><span class=cl>admin
</span></span></code></pre></div><p>重設admin帳號的密碼</p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203235051.png width=731 height=884 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203235051_hua6360dfab544f9999a49df0589cd0443_92979_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203235051_hua6360dfab544f9999a49df0589cd0443_92979_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=82 data-flex-basis=198px></p><p>可以看到集群資訊，state為Active
<img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203235121.png width=1450 height=729 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203235121_hu401a3e7c37472f45bfde9274f3bd039b_101794_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203235121_hu401a3e7c37472f45bfde9274f3bd039b_101794_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=198 data-flex-basis=477px></p><p>進入集群，可以看到集群詳細訊息
<img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203235142.png width=1447 height=950 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203235142_hua7e0a667bc875e78714de1963785d3b9_157686_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203235142_hua7e0a667bc875e78714de1963785d3b9_157686_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=152 data-flex-basis=365px></p><p>也可以使用dashboard shell去管理集群
<img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203235210.png width=1448 height=711 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203235210_hu6ce9521b1728d3371090b514716574af_135543_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203235210_hu6ce9521b1728d3371090b514716574af_135543_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=203 data-flex-basis=488px></p><h3 id=安裝rook-ceph分布式存儲系統>安裝rook-ceph分布式存儲系統</h3><p>詳細操作可以參考我這篇：
<a class=link href=https://blog.goldfishbrain-fighting.com/2023/kubernetes-rook-ceph/ target=_blank rel=noopener>https://blog.goldfishbrain-fighting.com/2023/kubernetes-rook-ceph/</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# lsblk
</span></span><span class=line><span class=cl>NAME             MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
</span></span><span class=line><span class=cl>loop0              7:0    <span class=m>0</span>  63.4M  <span class=m>1</span> loop /snap/core20/1974
</span></span><span class=line><span class=cl>loop1              7:1    <span class=m>0</span>  63.5M  <span class=m>1</span> loop /snap/core20/2015
</span></span><span class=line><span class=cl>loop2              7:2    <span class=m>0</span> 111.9M  <span class=m>1</span> loop /snap/lxd/24322
</span></span><span class=line><span class=cl>loop3              7:3    <span class=m>0</span>  40.8M  <span class=m>1</span> loop /snap/snapd/20092
</span></span><span class=line><span class=cl>loop4              7:4    <span class=m>0</span>  40.9M  <span class=m>1</span> loop /snap/snapd/20290
</span></span><span class=line><span class=cl>sda                8:0    <span class=m>0</span>    50G  <span class=m>0</span> disk
</span></span><span class=line><span class=cl>├─sda1             8:1    <span class=m>0</span>     1M  <span class=m>0</span> part
</span></span><span class=line><span class=cl>├─sda2             8:2    <span class=m>0</span>     2G  <span class=m>0</span> part /boot
</span></span><span class=line><span class=cl>└─sda3             8:3    <span class=m>0</span>    48G  <span class=m>0</span> part
</span></span><span class=line><span class=cl>  └─ubuntu--vg-root
</span></span><span class=line><span class=cl>                 253:0    <span class=m>0</span>    48G  <span class=m>0</span> lvm  /var/lib/kubelet/pods/ce7174f8-ad8f-4f42-8e58-2abca8c91424/volume-subpaths/tigera-ca-bundle/calico-node/1
</span></span><span class=line><span class=cl>                                           /
</span></span><span class=line><span class=cl>sdb                8:16   <span class=m>0</span>    16G  <span class=m>0</span> disk
</span></span><span class=line><span class=cl>sr0               11:0    <span class=m>1</span>  1024M  <span class=m>0</span> rom
</span></span><span class=line><span class=cl>root@k8s-master71u:~# lsblk -f
</span></span><span class=line><span class=cl>NAME FSTYPE FSVER LABEL UUID                                   FSAVAIL FSUSE% MOUNTPOINTS
</span></span><span class=line><span class=cl>loop0
</span></span><span class=line><span class=cl>     squash 4.0                                                      <span class=m>0</span>   100% /snap/core20/1974
</span></span><span class=line><span class=cl>loop1
</span></span><span class=line><span class=cl>     squash 4.0                                                      <span class=m>0</span>   100% /snap/core20/2015
</span></span><span class=line><span class=cl>loop2
</span></span><span class=line><span class=cl>     squash 4.0                                                      <span class=m>0</span>   100% /snap/lxd/24322
</span></span><span class=line><span class=cl>loop3
</span></span><span class=line><span class=cl>     squash 4.0                                                      <span class=m>0</span>   100% /snap/snapd/20092
</span></span><span class=line><span class=cl>loop4
</span></span><span class=line><span class=cl>     squash 4.0                                                      <span class=m>0</span>   100% /snap/snapd/20290
</span></span><span class=line><span class=cl>sda
</span></span><span class=line><span class=cl>├─sda1
</span></span><span class=line><span class=cl>│
</span></span><span class=line><span class=cl>├─sda2
</span></span><span class=line><span class=cl>│    xfs                f681192a-1cf2-4362-a74c-745374011700      1.8G     9% /boot
</span></span><span class=line><span class=cl>└─sda3
</span></span><span class=line><span class=cl>     LVM2_m LVM2        JEduSv-mV9g-tzdJ-sYEc-6piR-6nAo-48Srdh
</span></span><span class=line><span class=cl>  └─ubuntu--vg-root
</span></span><span class=line><span class=cl>     xfs                19cd87ec-9741-4295-9762-e87fb4f472c8     37.9G    21% /var/lib/kubelet/pods/ce7174f8-ad8f-4f42-8e58-2abca8c91424/volume-subpaths/tigera-ca-bundle/calico-node/1
</span></span><span class=line><span class=cl>                                                                              /
</span></span><span class=line><span class=cl>sdb
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# lsblk
</span></span><span class=line><span class=cl>NAME             MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
</span></span><span class=line><span class=cl>loop0              7:0    <span class=m>0</span>  63.4M  <span class=m>1</span> loop /snap/core20/1974
</span></span><span class=line><span class=cl>loop1              7:1    <span class=m>0</span>  63.5M  <span class=m>1</span> loop /snap/core20/2015
</span></span><span class=line><span class=cl>loop2              7:2    <span class=m>0</span> 111.9M  <span class=m>1</span> loop /snap/lxd/24322
</span></span><span class=line><span class=cl>loop3              7:3    <span class=m>0</span>  40.8M  <span class=m>1</span> loop /snap/snapd/20092
</span></span><span class=line><span class=cl>loop4              7:4    <span class=m>0</span>  40.9M  <span class=m>1</span> loop /snap/snapd/20290
</span></span><span class=line><span class=cl>sda                8:0    <span class=m>0</span>    50G  <span class=m>0</span> disk
</span></span><span class=line><span class=cl>├─sda1             8:1    <span class=m>0</span>     1M  <span class=m>0</span> part
</span></span><span class=line><span class=cl>├─sda2             8:2    <span class=m>0</span>     2G  <span class=m>0</span> part /boot
</span></span><span class=line><span class=cl>└─sda3             8:3    <span class=m>0</span>    48G  <span class=m>0</span> part
</span></span><span class=line><span class=cl>  └─ubuntu--vg-root
</span></span><span class=line><span class=cl>                 253:0    <span class=m>0</span>    48G  <span class=m>0</span> lvm  /var/lib/kubelet/pods/ce7174f8-ad8f-4f42-8e58-2abca8c91424/volume-subpaths/tigera-ca-bundle/calico-node/1
</span></span><span class=line><span class=cl>                                           /
</span></span><span class=line><span class=cl>sdb                8:16   <span class=m>0</span>    16G  <span class=m>0</span> disk
</span></span><span class=line><span class=cl>sr0               11:0    <span class=m>1</span>  1024M  <span class=m>0</span> rom
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# git clone --single-branch --branch v1.12.8 https://github.com/rook/rook.git
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# <span class=nb>cd</span> rook/deploy/examples
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f crds.yaml
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephblockpoolradosnamespaces.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephblockpools.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephbucketnotifications.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephbuckettopics.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephclients.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephclusters.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephcosidrivers.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephfilesystemmirrors.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephfilesystems.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephfilesystemsubvolumegroups.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephnfses.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephobjectrealms.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephobjectstores.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephobjectstoreusers.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephobjectzonegroups.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephobjectzones.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/cephrbdmirrors.ceph.rook.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/objectbucketclaims.objectbucket.io created
</span></span><span class=line><span class=cl>customresourcedefinition.apiextensions.k8s.io/objectbuckets.objectbucket.io created
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f common.yaml
</span></span><span class=line><span class=cl>namespace/rook-ceph created
</span></span><span class=line><span class=cl>clusterrole.rbac.authorization.k8s.io/cephfs-csi-nodeplugin created
</span></span><span class=line><span class=cl>clusterrole.rbac.authorization.k8s.io/cephfs-external-provisioner-runner created
</span></span><span class=line><span class=cl>clusterrole.rbac.authorization.k8s.io/objectstorage-provisioner-role created
</span></span><span class=line><span class=cl>clusterrole.rbac.authorization.k8s.io/rbd-csi-nodeplugin created
</span></span><span class=line><span class=cl>clusterrole.rbac.authorization.k8s.io/rbd-external-provisioner-runner created
</span></span><span class=line><span class=cl>clusterrole.rbac.authorization.k8s.io/rook-ceph-cluster-mgmt created
</span></span><span class=line><span class=cl>clusterrole.rbac.authorization.k8s.io/rook-ceph-global created
</span></span><span class=line><span class=cl>clusterrole.rbac.authorization.k8s.io/rook-ceph-mgr-cluster created
</span></span><span class=line><span class=cl>clusterrole.rbac.authorization.k8s.io/rook-ceph-mgr-system created
</span></span><span class=line><span class=cl>clusterrole.rbac.authorization.k8s.io/rook-ceph-object-bucket created
</span></span><span class=line><span class=cl>clusterrole.rbac.authorization.k8s.io/rook-ceph-osd created
</span></span><span class=line><span class=cl>clusterrole.rbac.authorization.k8s.io/rook-ceph-system created
</span></span><span class=line><span class=cl>clusterrolebinding.rbac.authorization.k8s.io/cephfs-csi-nodeplugin-role created
</span></span><span class=line><span class=cl>clusterrolebinding.rbac.authorization.k8s.io/cephfs-csi-provisioner-role created
</span></span><span class=line><span class=cl>clusterrolebinding.rbac.authorization.k8s.io/objectstorage-provisioner-role-binding created
</span></span><span class=line><span class=cl>clusterrolebinding.rbac.authorization.k8s.io/rbd-csi-nodeplugin created
</span></span><span class=line><span class=cl>clusterrolebinding.rbac.authorization.k8s.io/rbd-csi-provisioner-role created
</span></span><span class=line><span class=cl>clusterrolebinding.rbac.authorization.k8s.io/rook-ceph-global created
</span></span><span class=line><span class=cl>clusterrolebinding.rbac.authorization.k8s.io/rook-ceph-mgr-cluster created
</span></span><span class=line><span class=cl>clusterrolebinding.rbac.authorization.k8s.io/rook-ceph-object-bucket created
</span></span><span class=line><span class=cl>clusterrolebinding.rbac.authorization.k8s.io/rook-ceph-osd created
</span></span><span class=line><span class=cl>clusterrolebinding.rbac.authorization.k8s.io/rook-ceph-system created
</span></span><span class=line><span class=cl>role.rbac.authorization.k8s.io/cephfs-external-provisioner-cfg created
</span></span><span class=line><span class=cl>role.rbac.authorization.k8s.io/rbd-csi-nodeplugin created
</span></span><span class=line><span class=cl>role.rbac.authorization.k8s.io/rbd-external-provisioner-cfg created
</span></span><span class=line><span class=cl>role.rbac.authorization.k8s.io/rook-ceph-cmd-reporter created
</span></span><span class=line><span class=cl>role.rbac.authorization.k8s.io/rook-ceph-mgr created
</span></span><span class=line><span class=cl>role.rbac.authorization.k8s.io/rook-ceph-osd created
</span></span><span class=line><span class=cl>role.rbac.authorization.k8s.io/rook-ceph-purge-osd created
</span></span><span class=line><span class=cl>role.rbac.authorization.k8s.io/rook-ceph-rgw created
</span></span><span class=line><span class=cl>role.rbac.authorization.k8s.io/rook-ceph-system created
</span></span><span class=line><span class=cl>rolebinding.rbac.authorization.k8s.io/cephfs-csi-provisioner-role-cfg created
</span></span><span class=line><span class=cl>rolebinding.rbac.authorization.k8s.io/rbd-csi-nodeplugin-role-cfg created
</span></span><span class=line><span class=cl>rolebinding.rbac.authorization.k8s.io/rbd-csi-provisioner-role-cfg created
</span></span><span class=line><span class=cl>rolebinding.rbac.authorization.k8s.io/rook-ceph-cluster-mgmt created
</span></span><span class=line><span class=cl>rolebinding.rbac.authorization.k8s.io/rook-ceph-cmd-reporter created
</span></span><span class=line><span class=cl>rolebinding.rbac.authorization.k8s.io/rook-ceph-mgr created
</span></span><span class=line><span class=cl>rolebinding.rbac.authorization.k8s.io/rook-ceph-mgr-system created
</span></span><span class=line><span class=cl>rolebinding.rbac.authorization.k8s.io/rook-ceph-osd created
</span></span><span class=line><span class=cl>rolebinding.rbac.authorization.k8s.io/rook-ceph-purge-osd created
</span></span><span class=line><span class=cl>rolebinding.rbac.authorization.k8s.io/rook-ceph-rgw created
</span></span><span class=line><span class=cl>rolebinding.rbac.authorization.k8s.io/rook-ceph-system created
</span></span><span class=line><span class=cl>serviceaccount/objectstorage-provisioner created
</span></span><span class=line><span class=cl>serviceaccount/rook-ceph-cmd-reporter created
</span></span><span class=line><span class=cl>serviceaccount/rook-ceph-mgr created
</span></span><span class=line><span class=cl>serviceaccount/rook-ceph-osd created
</span></span><span class=line><span class=cl>serviceaccount/rook-ceph-purge-osd created
</span></span><span class=line><span class=cl>serviceaccount/rook-ceph-rgw created
</span></span><span class=line><span class=cl>serviceaccount/rook-ceph-system created
</span></span><span class=line><span class=cl>serviceaccount/rook-csi-cephfs-plugin-sa created
</span></span><span class=line><span class=cl>serviceaccount/rook-csi-cephfs-provisioner-sa created
</span></span><span class=line><span class=cl>serviceaccount/rook-csi-rbd-plugin-sa created
</span></span><span class=line><span class=cl>serviceaccount/rook-csi-rbd-provisioner-sa created
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f operator.yaml
</span></span><span class=line><span class=cl>configmap/rook-ceph-operator-config created
</span></span><span class=line><span class=cl>deployment.apps/rook-ceph-operator created
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get deployments.apps -n rook-ceph
</span></span><span class=line><span class=cl>NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span class=line><span class=cl>rook-ceph-operator   1/1     <span class=m>1</span>            <span class=m>1</span>           22s
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get pod -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                  READY   STATUS    RESTARTS   AGE
</span></span><span class=line><span class=cl>rook-ceph-operator-58775c8bdf-vmdkn   1/1     Running   <span class=m>0</span>          39s
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# vim cluster.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 角色調度</span>
</span></span><span class=line><span class=cl>  placement:
</span></span><span class=line><span class=cl>    mon:
</span></span><span class=line><span class=cl>      nodeAffinity:
</span></span><span class=line><span class=cl>        requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          nodeSelectorTerms:
</span></span><span class=line><span class=cl>          - matchExpressions:
</span></span><span class=line><span class=cl>            - key: ceph-mon
</span></span><span class=line><span class=cl>              operator: In
</span></span><span class=line><span class=cl>              values:
</span></span><span class=line><span class=cl>              - enabled
</span></span><span class=line><span class=cl>      tolerations:
</span></span><span class=line><span class=cl>      - effect: NoSchedule
</span></span><span class=line><span class=cl>        operator: Exists
</span></span><span class=line><span class=cl>    mgr:
</span></span><span class=line><span class=cl>      nodeAffinity:
</span></span><span class=line><span class=cl>        requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          nodeSelectorTerms:
</span></span><span class=line><span class=cl>          - matchExpressions:
</span></span><span class=line><span class=cl>            - key: ceph-mgr
</span></span><span class=line><span class=cl>              operator: In
</span></span><span class=line><span class=cl>              values:
</span></span><span class=line><span class=cl>              - enabled
</span></span><span class=line><span class=cl>      tolerations:
</span></span><span class=line><span class=cl>      - effect: NoSchedule
</span></span><span class=line><span class=cl>        operator: Exists
</span></span><span class=line><span class=cl>    osd:
</span></span><span class=line><span class=cl>      nodeAffinity:
</span></span><span class=line><span class=cl>        requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          nodeSelectorTerms:
</span></span><span class=line><span class=cl>          - matchExpressions:
</span></span><span class=line><span class=cl>            - key: ceph-osd
</span></span><span class=line><span class=cl>              operator: In
</span></span><span class=line><span class=cl>              values:
</span></span><span class=line><span class=cl>              - enabled
</span></span><span class=line><span class=cl>      tolerations:
</span></span><span class=line><span class=cl>      - effect: NoSchedule
</span></span><span class=line><span class=cl>        operator: Exists
</span></span><span class=line><span class=cl>    prepareosd:
</span></span><span class=line><span class=cl>      nodeAffinity:
</span></span><span class=line><span class=cl>        requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          nodeSelectorTerms:
</span></span><span class=line><span class=cl>          - matchExpressions:
</span></span><span class=line><span class=cl>            - key: ceph-osd
</span></span><span class=line><span class=cl>              operator: In
</span></span><span class=line><span class=cl>              values:
</span></span><span class=line><span class=cl>              - enabled
</span></span><span class=line><span class=cl>      tolerations:
</span></span><span class=line><span class=cl>      - effect: NoSchedule
</span></span><span class=line><span class=cl>        operator: Exists
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 角色資源</span>
</span></span><span class=line><span class=cl>  resources:
</span></span><span class=line><span class=cl>    mon:
</span></span><span class=line><span class=cl>      limits:
</span></span><span class=line><span class=cl>        cpu: <span class=s2>&#34;1000m&#34;</span>
</span></span><span class=line><span class=cl>        memory: <span class=s2>&#34;2048Mi&#34;</span>
</span></span><span class=line><span class=cl>      requests:
</span></span><span class=line><span class=cl>        cpu: <span class=s2>&#34;1000m&#34;</span>
</span></span><span class=line><span class=cl>        memory: <span class=s2>&#34;2048Mi&#34;</span>
</span></span><span class=line><span class=cl>    mgr:
</span></span><span class=line><span class=cl>      limits:
</span></span><span class=line><span class=cl>        cpu: <span class=s2>&#34;1000m&#34;</span>
</span></span><span class=line><span class=cl>        memory: <span class=s2>&#34;1024Mi&#34;</span>
</span></span><span class=line><span class=cl>      requests:
</span></span><span class=line><span class=cl>        cpu: <span class=s2>&#34;1000m&#34;</span>
</span></span><span class=line><span class=cl>        memory: <span class=s2>&#34;1024Mi&#34;</span>
</span></span><span class=line><span class=cl>    osd:
</span></span><span class=line><span class=cl>      limits:
</span></span><span class=line><span class=cl>        cpu: <span class=s2>&#34;1000m&#34;</span>
</span></span><span class=line><span class=cl>        memory: <span class=s2>&#34;2048Mi&#34;</span>
</span></span><span class=line><span class=cl>      requests:
</span></span><span class=line><span class=cl>        cpu: <span class=s2>&#34;1000m&#34;</span>
</span></span><span class=line><span class=cl>        memory: <span class=s2>&#34;2048Mi&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 磁碟調度</span>
</span></span><span class=line><span class=cl>    nodes:
</span></span><span class=line><span class=cl>    - name: <span class=s2>&#34;k8s-master71u&#34;</span>
</span></span><span class=line><span class=cl>      devices:
</span></span><span class=line><span class=cl>      - name: <span class=s2>&#34;sdb&#34;</span>
</span></span><span class=line><span class=cl>        config:
</span></span><span class=line><span class=cl>          storeType: bluestore
</span></span><span class=line><span class=cl>          journalSizeMB: <span class=s2>&#34;4096&#34;</span>
</span></span><span class=line><span class=cl>    - name: <span class=s2>&#34;k8s-master72u&#34;</span>
</span></span><span class=line><span class=cl>      devices:
</span></span><span class=line><span class=cl>      - name: <span class=s2>&#34;sdb&#34;</span>
</span></span><span class=line><span class=cl>        config:
</span></span><span class=line><span class=cl>          storeType: bluestore
</span></span><span class=line><span class=cl>          journalSizeMB: <span class=s2>&#34;4096&#34;</span>
</span></span><span class=line><span class=cl>    - name: <span class=s2>&#34;k8s-master73u&#34;</span>
</span></span><span class=line><span class=cl>      devices:
</span></span><span class=line><span class=cl>      - name: <span class=s2>&#34;sdb&#34;</span>
</span></span><span class=line><span class=cl>        config:
</span></span><span class=line><span class=cl>          storeType: bluestore
</span></span><span class=line><span class=cl>          journalSizeMB: <span class=s2>&#34;4096&#34;</span>
</span></span><span class=line><span class=cl>    - name: <span class=s2>&#34;k8s-node75u&#34;</span>
</span></span><span class=line><span class=cl>      devices:
</span></span><span class=line><span class=cl>      - name: <span class=s2>&#34;sdb&#34;</span>
</span></span><span class=line><span class=cl>        config:
</span></span><span class=line><span class=cl>          storeType: bluestore
</span></span><span class=line><span class=cl>          journalSizeMB: <span class=s2>&#34;4096&#34;</span>
</span></span><span class=line><span class=cl>    - name: <span class=s2>&#34;k8s-node76u&#34;</span>
</span></span><span class=line><span class=cl>      devices:
</span></span><span class=line><span class=cl>      - name: <span class=s2>&#34;sdb&#34;</span>
</span></span><span class=line><span class=cl>        config:
</span></span><span class=line><span class=cl>          storeType: bluestore
</span></span><span class=line><span class=cl>          journalSizeMB: <span class=s2>&#34;4096&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 關閉磁碟自動全部調度</span>
</span></span><span class=line><span class=cl>  storage: <span class=c1># cluster level storage configuration and selection</span>
</span></span><span class=line><span class=cl>    <span class=c1>#設置磁碟的參數，調整為false，方便後面訂製</span>
</span></span><span class=line><span class=cl>    useAllNodes: <span class=nb>false</span>
</span></span><span class=line><span class=cl>    useAllDevices: <span class=nb>false</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master71u ceph-mon<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>node/k8s-master71u labeled
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master72u ceph-mon<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>node/k8s-master72u labeled
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master73u ceph-mon<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>node/k8s-master73u labeled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master71u ceph-mgr<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>node/k8s-master71u labeled
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master72u ceph-mgr<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>node/k8s-master72u labeled
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master73u ceph-mgr<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>node/k8s-master73u labeled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master71u ceph-osd<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>node/k8s-master71u labeled
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master72u ceph-osd<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>node/k8s-master72u labeled
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master73u ceph-osd<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>node/k8s-master73u labeled
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-node75u ceph-osd<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>node/k8s-node75u labeled
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-node76u ceph-osd<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>node/k8s-node76u labeled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f cluster.yaml
</span></span><span class=line><span class=cl>cephcluster.ceph.rook.io/rook-ceph created
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get pod -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                                      READY   STATUS      RESTARTS   AGE
</span></span><span class=line><span class=cl>csi-cephfsplugin-nttgx                                    2/2     Running     <span class=m>0</span>          9m33s
</span></span><span class=line><span class=cl>csi-cephfsplugin-provisioner-668dfcf95b-dsxdk             5/5     Running     <span class=m>0</span>          9m33s
</span></span><span class=line><span class=cl>csi-cephfsplugin-provisioner-668dfcf95b-rgfv6             5/5     Running     <span class=m>0</span>          9m33s
</span></span><span class=line><span class=cl>csi-cephfsplugin-s6sc9                                    2/2     Running     <span class=m>0</span>          9m33s
</span></span><span class=line><span class=cl>csi-rbdplugin-bjv2p                                       2/2     Running     <span class=m>0</span>          9m33s
</span></span><span class=line><span class=cl>csi-rbdplugin-provisioner-5b78f67bbb-8ts8q                5/5     Running     <span class=m>0</span>          9m33s
</span></span><span class=line><span class=cl>csi-rbdplugin-provisioner-5b78f67bbb-rr5tn                5/5     Running     <span class=m>0</span>          9m33s
</span></span><span class=line><span class=cl>csi-rbdplugin-xb99w                                       2/2     Running     <span class=m>0</span>          9m33s
</span></span><span class=line><span class=cl>rook-ceph-crashcollector-k8s-master71u-55fcdbd66c-plz8m   1/1     Running     <span class=m>0</span>          2m57s
</span></span><span class=line><span class=cl>rook-ceph-crashcollector-k8s-master72u-675646bf64-8tl2q   1/1     Running     <span class=m>0</span>          3m18s
</span></span><span class=line><span class=cl>rook-ceph-crashcollector-k8s-master73u-84fdc469f7-9fkgh   1/1     Running     <span class=m>0</span>          2m56s
</span></span><span class=line><span class=cl>rook-ceph-crashcollector-k8s-node75u-bbc794dd9-rv4pw      1/1     Running     <span class=m>0</span>          11s
</span></span><span class=line><span class=cl>rook-ceph-crashcollector-k8s-node76u-5fd59cd68b-wljnj     1/1     Running     <span class=m>0</span>          73s
</span></span><span class=line><span class=cl>rook-ceph-mgr-a-59f556dbc9-695bv                          3/3     Running     <span class=m>0</span>          3m33s
</span></span><span class=line><span class=cl>rook-ceph-mgr-b-f4c7855d-j69fj                            3/3     Running     <span class=m>0</span>          3m33s
</span></span><span class=line><span class=cl>rook-ceph-mon-a-b657589fb-c7c4f                           2/2     Running     <span class=m>0</span>          8m2s
</span></span><span class=line><span class=cl>rook-ceph-mon-b-6bdf47675f-tbhg8                          2/2     Running     <span class=m>0</span>          3m59s
</span></span><span class=line><span class=cl>rook-ceph-mon-c-5dcbc556d7-9zmmn                          2/2     Running     <span class=m>0</span>          3m48s
</span></span><span class=line><span class=cl>rook-ceph-operator-58775c8bdf-m29b4                       1/1     Running     <span class=m>0</span>          14m
</span></span><span class=line><span class=cl>rook-ceph-osd-0-5b8dfc978f-fwlsv                          2/2     Running     <span class=m>0</span>          2m58s
</span></span><span class=line><span class=cl>rook-ceph-osd-1-c9544b764-gzcmd                           2/2     Running     <span class=m>0</span>          2m57s
</span></span><span class=line><span class=cl>rook-ceph-osd-2-897687cc9-mrntq                           2/2     Running     <span class=m>0</span>          2m56s
</span></span><span class=line><span class=cl>rook-ceph-osd-3-6c7c659454-9wk9z                          2/2     Running     <span class=m>0</span>          73s
</span></span><span class=line><span class=cl>rook-ceph-osd-4-68dcf49f6-vcpsj                           1/2     Running     <span class=m>0</span>          12s
</span></span><span class=line><span class=cl>rook-ceph-osd-prepare-k8s-master71u-6fjrl                 0/1     Completed   <span class=m>0</span>          3m11s
</span></span><span class=line><span class=cl>rook-ceph-osd-prepare-k8s-master72u-5mcbk                 0/1     Completed   <span class=m>0</span>          3m10s
</span></span><span class=line><span class=cl>rook-ceph-osd-prepare-k8s-master73u-cjklw                 0/1     Completed   <span class=m>0</span>          3m9s
</span></span><span class=line><span class=cl>rook-ceph-osd-prepare-k8s-node75u-tk24w                   0/1     Completed   <span class=m>0</span>          3m8s
</span></span><span class=line><span class=cl>rook-ceph-osd-prepare-k8s-node76u-7hqrg                   0/1     Completed   <span class=m>0</span>          3m8s
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f toolbox.yaml
</span></span><span class=line><span class=cl>deployment.apps/rook-ceph-tools created
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get pod -n rook-ceph <span class=p>|</span> grep -i tool
</span></span><span class=line><span class=cl>rook-ceph-tools-7cd4cd9c9c-54zk2                          1/1     Running     <span class=m>0</span>          34s
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl <span class=nb>exec</span> -it rook-ceph-tools-7cd4cd9c9c-54zk2 -n rook-ceph -- /bin/bash
</span></span><span class=line><span class=cl>bash-4.4$ ceph -s
</span></span><span class=line><span class=cl>  cluster:
</span></span><span class=line><span class=cl>    id:     9da44d3d-58d1-402d-8f8b-66bd89a869a4
</span></span><span class=line><span class=cl>    health: HEALTH_OK
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  services:
</span></span><span class=line><span class=cl>    mon: <span class=m>3</span> daemons, quorum a,b,c <span class=o>(</span>age 5m<span class=o>)</span>
</span></span><span class=line><span class=cl>    mgr: a<span class=o>(</span>active, starting, since 11s<span class=o>)</span>, standbys: b
</span></span><span class=line><span class=cl>    osd: <span class=m>5</span> osds: <span class=m>5</span> up <span class=o>(</span>since 93s<span class=o>)</span>, <span class=m>5</span> in <span class=o>(</span>since 117s<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  data:
</span></span><span class=line><span class=cl>    pools:   <span class=m>1</span> pools, <span class=m>1</span> pgs
</span></span><span class=line><span class=cl>    objects: <span class=m>2</span> objects, <span class=m>449</span> KiB
</span></span><span class=line><span class=cl>    usage:   <span class=m>41</span> MiB used, <span class=m>80</span> GiB / <span class=m>80</span> GiB avail
</span></span><span class=line><span class=cl>    pgs:     <span class=m>1</span> active+clean
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash-4.4$ ceph status
</span></span><span class=line><span class=cl>  cluster:
</span></span><span class=line><span class=cl>    id:     9da44d3d-58d1-402d-8f8b-66bd89a869a4
</span></span><span class=line><span class=cl>    health: HEALTH_OK
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  services:
</span></span><span class=line><span class=cl>    mon: <span class=m>3</span> daemons, quorum a,b,c <span class=o>(</span>age 5m<span class=o>)</span>
</span></span><span class=line><span class=cl>    mgr: a<span class=o>(</span>active, since 10s<span class=o>)</span>, standbys: b
</span></span><span class=line><span class=cl>    osd: <span class=m>5</span> osds: <span class=m>5</span> up <span class=o>(</span>since 104s<span class=o>)</span>, <span class=m>5</span> in <span class=o>(</span>since 2m<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  data:
</span></span><span class=line><span class=cl>    pools:   <span class=m>1</span> pools, <span class=m>1</span> pgs
</span></span><span class=line><span class=cl>    objects: <span class=m>2</span> objects, <span class=m>449</span> KiB
</span></span><span class=line><span class=cl>    usage:   <span class=m>41</span> MiB used, <span class=m>80</span> GiB / <span class=m>80</span> GiB avail
</span></span><span class=line><span class=cl>    pgs:     <span class=m>1</span> active+clean
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash-4.4$ ceph osd status
</span></span><span class=line><span class=cl>ID  HOST            USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE
</span></span><span class=line><span class=cl> <span class=m>0</span>  k8s-master71u  8788k  15.9G      <span class=m>0</span>        <span class=m>0</span>       <span class=m>0</span>        <span class=m>0</span>   exists,up
</span></span><span class=line><span class=cl> <span class=m>1</span>  k8s-master72u  8272k  15.9G      <span class=m>0</span>        <span class=m>0</span>       <span class=m>0</span>        <span class=m>0</span>   exists,up
</span></span><span class=line><span class=cl> <span class=m>2</span>  k8s-master73u  8788k  15.9G      <span class=m>0</span>        <span class=m>0</span>       <span class=m>0</span>        <span class=m>0</span>   exists,up
</span></span><span class=line><span class=cl> <span class=m>3</span>  k8s-node76u    8404k  15.9G      <span class=m>0</span>        <span class=m>0</span>       <span class=m>0</span>        <span class=m>0</span>   exists,up
</span></span><span class=line><span class=cl> <span class=m>4</span>  k8s-node75u    7888k  15.9G      <span class=m>0</span>        <span class=m>0</span>       <span class=m>0</span>        <span class=m>0</span>   exists,up
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash-4.4$ ceph df
</span></span><span class=line><span class=cl>--- RAW STORAGE ---
</span></span><span class=line><span class=cl>CLASS    SIZE   AVAIL    USED  RAW USED  %RAW USED
</span></span><span class=line><span class=cl>ssd    <span class=m>80</span> GiB  <span class=m>80</span> GiB  <span class=m>41</span> MiB    <span class=m>41</span> MiB       0.05
</span></span><span class=line><span class=cl>TOTAL  <span class=m>80</span> GiB  <span class=m>80</span> GiB  <span class=m>41</span> MiB    <span class=m>41</span> MiB       0.05
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>--- POOLS ---
</span></span><span class=line><span class=cl>POOL  ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
</span></span><span class=line><span class=cl>.mgr   <span class=m>1</span>    <span class=m>1</span>  <span class=m>449</span> KiB        <span class=m>2</span>  <span class=m>449</span> KiB      <span class=m>0</span>     <span class=m>25</span> GiB
</span></span><span class=line><span class=cl>bash-4.4$ rados df
</span></span><span class=line><span class=cl>POOL_NAME     USED  OBJECTS  CLONES  COPIES  MISSING_ON_PRIMARY  UNFOUND  DEGRADED  RD_OPS       RD  WR_OPS       WR  USED COMPR  UNDER COMPR
</span></span><span class=line><span class=cl>.mgr       <span class=m>449</span> KiB        <span class=m>2</span>       <span class=m>0</span>       <span class=m>6</span>                   <span class=m>0</span>        <span class=m>0</span>         <span class=m>0</span>     <span class=m>288</span>  <span class=m>494</span> KiB     <span class=m>153</span>  1.3 MiB         <span class=m>0</span> B          <span class=m>0</span> B
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>total_objects    <span class=m>2</span>
</span></span><span class=line><span class=cl>total_used       <span class=m>41</span> MiB
</span></span><span class=line><span class=cl>total_avail      <span class=m>80</span> GiB
</span></span><span class=line><span class=cl>total_space      <span class=m>80</span> GiB
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash-4.4$ cat /etc/ceph/ceph.conf
</span></span><span class=line><span class=cl><span class=o>[</span>global<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>mon_host</span> <span class=o>=</span> 10.202.156.149:6789,10.202.75.120:6789,10.202.62.213:6789
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>[</span>client.admin<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>keyring</span> <span class=o>=</span> /etc/ceph/keyring
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash-4.4$ cat /etc/ceph/keyring
</span></span><span class=line><span class=cl><span class=o>[</span>client.admin<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>key</span> <span class=o>=</span> <span class=nv>AQCtp2xlw3soGRAAVvoyqWe8wzvkkt26JQEGtQ</span><span class=o>==</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash-4.4$ <span class=nb>exit</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# apt update
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# apt install ceph-common
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples#  vim /etc/ceph/ceph.conf
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>[</span>global<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>mon_host</span> <span class=o>=</span> 10.202.156.149:6789,10.202.75.120:6789,10.202.62.213:6789
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>[</span>client.admin<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>keyring</span> <span class=o>=</span> /etc/ceph/keyring
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# vim /etc/ceph/keyring
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>[</span>client.admin<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>key</span> <span class=o>=</span> <span class=nv>AQCtp2xlw3soGRAAVvoyqWe8wzvkkt26JQEGtQ</span><span class=o>==</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# ceph -s
</span></span><span class=line><span class=cl>  cluster:
</span></span><span class=line><span class=cl>    id:     9da44d3d-58d1-402d-8f8b-66bd89a869a4
</span></span><span class=line><span class=cl>    health: HEALTH_OK
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  services:
</span></span><span class=line><span class=cl>    mon: <span class=m>3</span> daemons, quorum a,b,c <span class=o>(</span>age 9m<span class=o>)</span>
</span></span><span class=line><span class=cl>    mgr: a<span class=o>(</span>active, since 4m<span class=o>)</span>, standbys: b
</span></span><span class=line><span class=cl>    osd: <span class=m>5</span> osds: <span class=m>5</span> up <span class=o>(</span>since 6m<span class=o>)</span>, <span class=m>5</span> in <span class=o>(</span>since 6m<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  data:
</span></span><span class=line><span class=cl>    pools:   <span class=m>1</span> pools, <span class=m>1</span> pgs
</span></span><span class=line><span class=cl>    objects: <span class=m>2</span> objects, <span class=m>449</span> KiB
</span></span><span class=line><span class=cl>    usage:   <span class=m>41</span> MiB used, <span class=m>80</span> GiB / <span class=m>80</span> GiB avail
</span></span><span class=line><span class=cl>    pgs:     <span class=m>1</span> active+clean
</span></span></code></pre></div><h5 id=cephfs>cephfs</h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master71u ceph-mds<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master72u ceph-mds<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-master73u ceph-mds<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-node75u ceph-mds<span class=o>=</span>enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl label node k8s-node76u ceph-mds<span class=o>=</span>enabled
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# vim filesystem.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    activeCount: <span class=m>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    placement:
</span></span><span class=line><span class=cl>      nodeAffinity:
</span></span><span class=line><span class=cl>        requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          nodeSelectorTerms:
</span></span><span class=line><span class=cl>          - matchExpressions:
</span></span><span class=line><span class=cl>            - key: ceph-mds
</span></span><span class=line><span class=cl>              operator: In
</span></span><span class=line><span class=cl>              values:
</span></span><span class=line><span class=cl>              - enabled
</span></span><span class=line><span class=cl>      tolerations:
</span></span><span class=line><span class=cl>      - effect: NoSchedule
</span></span><span class=line><span class=cl>        operator: Exists
</span></span><span class=line><span class=cl>      podAntiAffinity:
</span></span><span class=line><span class=cl>        requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          - labelSelector:
</span></span><span class=line><span class=cl>              matchExpressions:
</span></span><span class=line><span class=cl>                - key: app
</span></span><span class=line><span class=cl>                  operator: In
</span></span><span class=line><span class=cl>                  values:
</span></span><span class=line><span class=cl>                    - rook-ceph-mds
</span></span><span class=line><span class=cl>            topologyKey: kubernetes.io/hostname
</span></span><span class=line><span class=cl>        preferredDuringSchedulingIgnoredDuringExecution:
</span></span><span class=line><span class=cl>          - weight: <span class=m>100</span>
</span></span><span class=line><span class=cl>            podAffinityTerm:
</span></span><span class=line><span class=cl>              labelSelector:
</span></span><span class=line><span class=cl>                matchExpressions:
</span></span><span class=line><span class=cl>                  - key: app
</span></span><span class=line><span class=cl>                    operator: In
</span></span><span class=line><span class=cl>                    values:
</span></span><span class=line><span class=cl>                      - rook-ceph-mds
</span></span><span class=line><span class=cl>              topologyKey: topology.kubernetes.io/zone
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f filesystem.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get pod -n rook-ceph -o wide <span class=p>|</span> grep -i rook-ceph-mds
</span></span><span class=line><span class=cl>rook-ceph-mds-myfs-a-fcf6b94fb-wskkq                      2/2     Running     <span class=m>0</span>          18s     10.201.126.14    k8s-master72u   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>rook-ceph-mds-myfs-b-55d97767d4-z872h                     2/2     Running     <span class=m>0</span>          16s     10.201.255.227   k8s-node76u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>rook-ceph-mds-myfs-c-5c9c94fbfb-chfnv                     2/2     Running     <span class=m>0</span>          13s     10.201.14.164    k8s-node75u     &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>rook-ceph-mds-myfs-d-69cc457f9b-hq7vv                     2/2     Running     <span class=m>0</span>          10s     10.201.133.13    k8s-master71u   &lt;none&gt;           &lt;none&gt;
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# ceph -s
</span></span><span class=line><span class=cl>  cluster:
</span></span><span class=line><span class=cl>    id:     9da44d3d-58d1-402d-8f8b-66bd89a869a4
</span></span><span class=line><span class=cl>    health: HEALTH_OK
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  services:
</span></span><span class=line><span class=cl>    mon: <span class=m>3</span> daemons, quorum a,b,c <span class=o>(</span>age 13m<span class=o>)</span>
</span></span><span class=line><span class=cl>    mgr: a<span class=o>(</span>active, since 8m<span class=o>)</span>, standbys: b
</span></span><span class=line><span class=cl>    mds: 2/2 daemons up, <span class=m>2</span> hot standby
</span></span><span class=line><span class=cl>    osd: <span class=m>5</span> osds: <span class=m>5</span> up <span class=o>(</span>since 9m<span class=o>)</span>, <span class=m>5</span> in <span class=o>(</span>since 10m<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  data:
</span></span><span class=line><span class=cl>    volumes: 1/1 healthy
</span></span><span class=line><span class=cl>    pools:   <span class=m>3</span> pools, <span class=m>3</span> pgs
</span></span><span class=line><span class=cl>    objects: <span class=m>41</span> objects, <span class=m>452</span> KiB
</span></span><span class=line><span class=cl>    usage:   <span class=m>43</span> MiB used, <span class=m>80</span> GiB / <span class=m>80</span> GiB avail
</span></span><span class=line><span class=cl>    pgs:     <span class=m>3</span> active+clean
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  io:
</span></span><span class=line><span class=cl>    client:   2.2 KiB/s rd, 2.4 KiB/s wr, <span class=m>2</span> op/s rd, <span class=m>8</span> op/s wr
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# <span class=nb>cd</span> csi/cephfs
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/csi/cephfs# kubectl create -f storageclass.yaml
</span></span><span class=line><span class=cl>storageclass.storage.k8s.io/rook-cephfs created
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/csi/cephfs# kubectl get storageclasses.storage.k8s.io
</span></span><span class=line><span class=cl>NAME          PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
</span></span><span class=line><span class=cl>rook-cephfs   rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           <span class=nb>true</span>                   5s
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/csi/cephfs# ceph fs ls
</span></span><span class=line><span class=cl>name: myfs, metadata pool: myfs-metadata, data pools: <span class=o>[</span>myfs-replicated <span class=o>]</span>
</span></span></code></pre></div><p>驗證pvc,pc使用storageClassName: &ldquo;rook-cephfs"自動生成</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# cat sc.yaml
</span></span><span class=line><span class=cl>apiVersion: apps/v1
</span></span><span class=line><span class=cl>kind: Deployment
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>  name: web-sc
</span></span><span class=line><span class=cl>spec:
</span></span><span class=line><span class=cl>  selector:
</span></span><span class=line><span class=cl>    matchLabels:
</span></span><span class=line><span class=cl>      app: nginx
</span></span><span class=line><span class=cl>  replicas: <span class=m>3</span>
</span></span><span class=line><span class=cl>  template:
</span></span><span class=line><span class=cl>    metadata:
</span></span><span class=line><span class=cl>      labels:
</span></span><span class=line><span class=cl>        app: nginx
</span></span><span class=line><span class=cl>    spec:
</span></span><span class=line><span class=cl>      containers:
</span></span><span class=line><span class=cl>      - name: nginx
</span></span><span class=line><span class=cl>        image: nginx
</span></span><span class=line><span class=cl>        volumeMounts:
</span></span><span class=line><span class=cl>          - name: wwwroot
</span></span><span class=line><span class=cl>            mountPath: /usr/share/nginx/html
</span></span><span class=line><span class=cl>      volumes:
</span></span><span class=line><span class=cl>      - name: wwwroot
</span></span><span class=line><span class=cl>        persistentVolumeClaim:
</span></span><span class=line><span class=cl>          claimName: web-sc
</span></span><span class=line><span class=cl>---
</span></span><span class=line><span class=cl>apiVersion: v1
</span></span><span class=line><span class=cl>kind: PersistentVolumeClaim
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>  name: web-sc
</span></span><span class=line><span class=cl>spec:
</span></span><span class=line><span class=cl>  storageClassName: <span class=s2>&#34;rook-cephfs&#34;</span>
</span></span><span class=line><span class=cl>  accessModes:
</span></span><span class=line><span class=cl>    - ReadWriteMany
</span></span><span class=line><span class=cl>  resources:
</span></span><span class=line><span class=cl>    requests:
</span></span><span class=line><span class=cl>      storage: 5Gi
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl create -f sc.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl get pod,pvc,pv
</span></span><span class=line><span class=cl>NAME                          READY   STATUS    RESTARTS      AGE
</span></span><span class=line><span class=cl>pod/test-nginx                1/1     Running   <span class=m>2</span> <span class=o>(</span>33m ago<span class=o>)</span>   81m
</span></span><span class=line><span class=cl>pod/web-sc-7b6c54fbb9-55rtb   1/1     Running   <span class=m>0</span>             34s
</span></span><span class=line><span class=cl>pod/web-sc-7b6c54fbb9-fxx6r   1/1     Running   <span class=m>0</span>             34s
</span></span><span class=line><span class=cl>pod/web-sc-7b6c54fbb9-vlpwm   1/1     Running   <span class=m>0</span>             34s
</span></span><span class=line><span class=cl>pod/web2-5d48fb75c5-dt5xd     1/1     Running   <span class=m>2</span> <span class=o>(</span>33m ago<span class=o>)</span>   80m
</span></span><span class=line><span class=cl>pod/web2-5d48fb75c5-ggmrz     1/1     Running   <span class=m>2</span> <span class=o>(</span>33m ago<span class=o>)</span>   80m
</span></span><span class=line><span class=cl>pod/web2-5d48fb75c5-jsvck     1/1     Running   <span class=m>2</span> <span class=o>(</span>33m ago<span class=o>)</span>   80m
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span class=line><span class=cl>persistentvolumeclaim/web-sc   Bound    pvc-ec58bbde-7e2c-4cc4-be13-223d10de5d99   5Gi        RWX            rook-cephfs    34s
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE
</span></span><span class=line><span class=cl>persistentvolume/pvc-ec58bbde-7e2c-4cc4-be13-223d10de5d99   5Gi        RWX            Delete           Bound    default/web-sc   rook-cephfs             34s
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# kubectl <span class=nb>exec</span> -ti web-sc-7b6c54fbb9-55rtb -- /bin/bash
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-55rtb:/#
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-55rtb:/# df -h
</span></span><span class=line><span class=cl>Filesystem                                                                                                                                                Size  Used Avail Use% Mounted on
</span></span><span class=line><span class=cl>overlay                                                                                                                                                    48G   20G   29G  41% /
</span></span><span class=line><span class=cl>tmpfs                                                                                                                                                      64M     <span class=m>0</span>   64M   0% /dev
</span></span><span class=line><span class=cl>/dev/mapper/ubuntu--vg-root                                                                                                                                48G   20G   29G  41% /etc/hosts
</span></span><span class=line><span class=cl>shm                                                                                                                                                        64M     <span class=m>0</span>   64M   0% /dev/shm
</span></span><span class=line><span class=cl>10.202.75.120:6789,10.202.62.213:6789,10.202.156.149:6789:/volumes/csi/csi-vol-39f4781c-a08b-4b30-a2f9-5d244cd4f7af/58cdd551-8197-41f0-875b-a050fb0135aa  5.0G     <span class=m>0</span>  5.0G   0% /usr/share/nginx/html
</span></span><span class=line><span class=cl>tmpfs                                                                                                                                                     7.7G   12K  7.7G   1% /run/secrets/kubernetes.io/serviceaccount
</span></span><span class=line><span class=cl>tmpfs                                                                                                                                                     3.9G     <span class=m>0</span>  3.9G   0% /proc/acpi
</span></span><span class=line><span class=cl>tmpfs                                                                                                                                                     3.9G     <span class=m>0</span>  3.9G   0% /proc/scsi
</span></span><span class=line><span class=cl>tmpfs                                                                                                                                                     3.9G     <span class=m>0</span>  3.9G   0% /sys/firmware
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-55rtb:/#
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-55rtb:/# <span class=nb>cd</span> /usr/share/nginx/html/
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-55rtb:/usr/share/nginx/html# touch <span class=m>123</span>
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-55rtb:/usr/share/nginx/html# ls
</span></span><span class=line><span class=cl><span class=m>123</span>
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-55rtb:/usr/share/nginx/html# <span class=nb>exit</span>
</span></span><span class=line><span class=cl><span class=nb>exit</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~# kubectl <span class=nb>exec</span> -ti web-sc-7b6c54fbb9-fxx6r -- /bin/bash
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-fxx6r:/#
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-fxx6r:/# df -h
</span></span><span class=line><span class=cl>Filesystem                                                                                                                                                Size  Used Avail Use% Mounted on
</span></span><span class=line><span class=cl>overlay                                                                                                                                                    48G   20G   29G  41% /
</span></span><span class=line><span class=cl>tmpfs                                                                                                                                                      64M     <span class=m>0</span>   64M   0% /dev
</span></span><span class=line><span class=cl>/dev/mapper/ubuntu--vg-root                                                                                                                                48G   20G   29G  41% /etc/hosts
</span></span><span class=line><span class=cl>shm                                                                                                                                                        64M     <span class=m>0</span>   64M   0% /dev/shm
</span></span><span class=line><span class=cl>10.202.75.120:6789,10.202.62.213:6789,10.202.156.149:6789:/volumes/csi/csi-vol-39f4781c-a08b-4b30-a2f9-5d244cd4f7af/58cdd551-8197-41f0-875b-a050fb0135aa  5.0G     <span class=m>0</span>  5.0G   0% /usr/share/nginx/html
</span></span><span class=line><span class=cl>tmpfs                                                                                                                                                     7.7G   12K  7.7G   1% /run/secrets/kubernetes.io/serviceaccount
</span></span><span class=line><span class=cl>tmpfs                                                                                                                                                     3.9G     <span class=m>0</span>  3.9G   0% /proc/acpi
</span></span><span class=line><span class=cl>tmpfs                                                                                                                                                     3.9G     <span class=m>0</span>  3.9G   0% /proc/scsi
</span></span><span class=line><span class=cl>tmpfs                                                                                                                                                     3.9G     <span class=m>0</span>  3.9G   0% /sys/firmware
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-fxx6r:/#
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-fxx6r:/# <span class=nb>cd</span> /usr/share/nginx/html/
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-fxx6r:/usr/share/nginx/html# ls
</span></span><span class=line><span class=cl><span class=m>123</span>
</span></span><span class=line><span class=cl>root@web-sc-7b6c54fbb9-fxx6r:/usr/share/nginx/html# <span class=nb>exit</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# kubectl get services -n rook-ceph
</span></span><span class=line><span class=cl>NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>             AGE
</span></span><span class=line><span class=cl>rook-ceph-mgr             ClusterIP   10.202.138.211   &lt;none&gt;        9283/TCP            19m
</span></span><span class=line><span class=cl>rook-ceph-mgr-dashboard   ClusterIP   10.202.204.184   &lt;none&gt;        8443/TCP            19m
</span></span><span class=line><span class=cl>rook-ceph-mon-a           ClusterIP   10.202.156.149   &lt;none&gt;        6789/TCP,3300/TCP   24m
</span></span><span class=line><span class=cl>rook-ceph-mon-b           ClusterIP   10.202.75.120    &lt;none&gt;        6789/TCP,3300/TCP   20m
</span></span><span class=line><span class=cl>rook-ceph-mon-c           ClusterIP   10.202.62.213    &lt;none&gt;        6789/TCP,3300/TCP   20m
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl create -f dashboard-external-https.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get services -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>             AGE
</span></span><span class=line><span class=cl>rook-ceph-mgr                            ClusterIP   10.202.138.211   &lt;none&gt;        9283/TCP            21m
</span></span><span class=line><span class=cl>rook-ceph-mgr-dashboard                  ClusterIP   10.202.204.184   &lt;none&gt;        8443/TCP            21m
</span></span><span class=line><span class=cl>rook-ceph-mgr-dashboard-external-https   NodePort    10.202.5.199     &lt;none&gt;        8443:30125/TCP      10s
</span></span><span class=line><span class=cl>rook-ceph-mon-a                          ClusterIP   10.202.156.149   &lt;none&gt;        6789/TCP,3300/TCP   26m
</span></span><span class=line><span class=cl>rook-ceph-mon-b                          ClusterIP   10.202.75.120    &lt;none&gt;        6789/TCP,3300/TCP   21m
</span></span><span class=line><span class=cl>rook-ceph-mon-c                          ClusterIP   10.202.62.213    &lt;none&gt;        6789/TCP,3300/TCP   21m
</span></span></code></pre></div><p>以下網址，都可以訪問nodeport
https://192.168.1.71:30125/#/login?returnUrl=%2Fdashboard
https://192.168.1.72:30125/#/login?returnUrl=%2Fdashboard
https://192.168.1.73:30125/#/login?returnUrl=%2Fdashboard
https://192.168.1.74:30125/#/login?returnUrl=%2Fdashboard
https://192.168.1.75:30125/#/login?returnUrl=%2Fdashboard
https://192.168.1.76:30125/#/login?returnUrl=%2Fdashboard</p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204003829.png width=1449 height=638 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204003829_huafbab9b6fca60d5feecc2c3cae03c81b_153524_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204003829_huafbab9b6fca60d5feecc2c3cae03c81b_153524_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=227 data-flex-basis=545px></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# kubectl get secrets -n rook-ceph rook-ceph-dashboard-password -o yaml
</span></span><span class=line><span class=cl>apiVersion: v1
</span></span><span class=line><span class=cl>data:
</span></span><span class=line><span class=cl>  password: <span class=nv>KF1hVHhQTSc0NjpnYjBYL2NENig</span><span class=o>=</span>
</span></span><span class=line><span class=cl>kind: Secret
</span></span><span class=line><span class=cl>metadata:
</span></span><span class=line><span class=cl>  creationTimestamp: <span class=s2>&#34;2023-12-03T16:12:13Z&#34;</span>
</span></span><span class=line><span class=cl>  name: rook-ceph-dashboard-password
</span></span><span class=line><span class=cl>  namespace: rook-ceph
</span></span><span class=line><span class=cl>  ownerReferences:
</span></span><span class=line><span class=cl>  - apiVersion: ceph.rook.io/v1
</span></span><span class=line><span class=cl>    blockOwnerDeletion: <span class=nb>true</span>
</span></span><span class=line><span class=cl>    controller: <span class=nb>true</span>
</span></span><span class=line><span class=cl>    kind: CephCluster
</span></span><span class=line><span class=cl>    name: rook-ceph
</span></span><span class=line><span class=cl>    uid: 93039daa-6f89-447a-81c7-89d387c1350b
</span></span><span class=line><span class=cl>  resourceVersion: <span class=s2>&#34;25904&#34;</span>
</span></span><span class=line><span class=cl>  uid: b2396e15-0674-4a9d-982b-25d03916a553
</span></span><span class=line><span class=cl>type: kubernetes.io/rook
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# <span class=nb>echo</span> <span class=s2>&#34;KF1hVHhQTSc0NjpnYjBYL2NENig=&#34;</span> <span class=p>|</span> base64 -d
</span></span><span class=line><span class=cl><span class=o>(]</span>aTxPM<span class=err>&#39;</span>46:gb0X/cD6<span class=o>(</span>
</span></span></code></pre></div><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204003900.png width=1449 height=637 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204003900_hu9ed0a3b4e2c1fe4993f9af7dd7c653c3_154845_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204003900_hu9ed0a3b4e2c1fe4993f9af7dd7c653c3_154845_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=227 data-flex-basis=545px></p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204003915.png width=1448 height=947 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204003915_hu887debf3ed1d1e248adb7cdce041c86a_174758_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204003915_hu887debf3ed1d1e248adb7cdce041c86a_174758_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=152 data-flex-basis=366px></p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204003943.png width=858 height=523 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204003943_hu003fb5008293d0a4ff521d9d07b54da3_64523_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204003943_hu003fb5008293d0a4ff521d9d07b54da3_64523_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=164 data-flex-basis=393px></p><p>一直報這個錯誤，導致集群檢查不健康</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# ceph mgr module ls
</span></span><span class=line><span class=cl>MODULE
</span></span><span class=line><span class=cl>balancer              on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>crash                 on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>devicehealth          on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>orchestrator          on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>pg_autoscaler         on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>progress              on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>rbd_support           on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>status                on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>telemetry             on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>volumes               on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>dashboard             on
</span></span><span class=line><span class=cl>iostat                on
</span></span><span class=line><span class=cl>nfs                   on
</span></span><span class=line><span class=cl>prometheus            on
</span></span><span class=line><span class=cl>restful               on
</span></span><span class=line><span class=cl>alerts                -
</span></span><span class=line><span class=cl>cephadm               -
</span></span><span class=line><span class=cl>diskprediction_local  -
</span></span><span class=line><span class=cl>influx                -
</span></span><span class=line><span class=cl>insights              -
</span></span><span class=line><span class=cl>k8sevents             -
</span></span><span class=line><span class=cl>localpool             -
</span></span><span class=line><span class=cl>mds_autoscaler        -
</span></span><span class=line><span class=cl>mirroring             -
</span></span><span class=line><span class=cl>osd_perf_query        -
</span></span><span class=line><span class=cl>osd_support           -
</span></span><span class=line><span class=cl>rook                  -
</span></span><span class=line><span class=cl>selftest              -
</span></span><span class=line><span class=cl>snap_schedule         -
</span></span><span class=line><span class=cl>stats                 -
</span></span><span class=line><span class=cl>telegraf              -
</span></span><span class=line><span class=cl>test_orchestrator     -
</span></span><span class=line><span class=cl>zabbix                -
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# ceph mgr module <span class=nb>enable</span> rook
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples# ceph mgr module ls
</span></span><span class=line><span class=cl>MODULE
</span></span><span class=line><span class=cl>balancer              on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>crash                 on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>devicehealth          on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>orchestrator          on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>pg_autoscaler         on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>progress              on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>rbd_support           on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>status                on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>telemetry             on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>volumes               on <span class=o>(</span>always on<span class=o>)</span>
</span></span><span class=line><span class=cl>dashboard             on
</span></span><span class=line><span class=cl>iostat                on
</span></span><span class=line><span class=cl>nfs                   on
</span></span><span class=line><span class=cl>prometheus            on
</span></span><span class=line><span class=cl>restful               on
</span></span><span class=line><span class=cl>rook                  on
</span></span><span class=line><span class=cl>alerts                -
</span></span><span class=line><span class=cl>cephadm               -
</span></span><span class=line><span class=cl>diskprediction_local  -
</span></span><span class=line><span class=cl>influx                -
</span></span><span class=line><span class=cl>insights              -
</span></span><span class=line><span class=cl>k8sevents             -
</span></span><span class=line><span class=cl>localpool             -
</span></span><span class=line><span class=cl>mds_autoscaler        -
</span></span><span class=line><span class=cl>mirroring             -
</span></span><span class=line><span class=cl>osd_perf_query        -
</span></span><span class=line><span class=cl>osd_support           -
</span></span><span class=line><span class=cl>selftest              -
</span></span><span class=line><span class=cl>snap_schedule         -
</span></span><span class=line><span class=cl>stats                 -
</span></span><span class=line><span class=cl>telegraf              -
</span></span><span class=line><span class=cl>test_orchestrator     -
</span></span><span class=line><span class=cl>zabbix                -
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl create -f bundle.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl get pod
</span></span><span class=line><span class=cl>NAME                                   READY   STATUS    RESTARTS      AGE
</span></span><span class=line><span class=cl>prometheus-rook-prometheus-0                              2/2     Running     <span class=m>0</span>             48s
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl create -f prometheus.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl create -f prometheus-service.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl create -f service-monitor.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl create -f grafana-all.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl get pod,svc -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                                          READY   STATUS      RESTARTS      AGE
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>pod/grafana-654886fbff-czjvf                                  1/1     Running     <span class=m>1</span> <span class=o>(</span>76s ago<span class=o>)</span>   2m2s
</span></span><span class=line><span class=cl>pod/prometheus-rook-prometheus-0                              2/2     Running     <span class=m>0</span>             3m42s
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME                                             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>             AGE
</span></span><span class=line><span class=cl>service/grafana                                  NodePort    10.202.70.24     &lt;none&gt;        3000:31782/TCP      2m2s
</span></span><span class=line><span class=cl>service/prometheus-operated                      ClusterIP   None             &lt;none&gt;        9090/TCP            3m42s
</span></span></code></pre></div><p>以下網址，都可以訪問nodeport</p><p>http://192.168.1.71:31782/login
http://192.168.1.72:31782/login
http://192.168.1.73:31782/login
http://192.168.1.74:31782/login
http://192.168.1.75:31782/login</p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011532.png width=983 height=727 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011532_huf5f1c0df9977db9c6a994dd9bbfbfd68_401802_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011532_huf5f1c0df9977db9c6a994dd9bbfbfd68_401802_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=135 data-flex-basis=324px></p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011610.png width=981 height=760 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011610_hu6c3aa08ff1aa99d5a8ac6dc009800516_409848_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011610_hu6c3aa08ff1aa99d5a8ac6dc009800516_409848_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=129 data-flex-basis=309px></p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011643.png width=1448 height=627 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011643_hufa48ffd4c7b2b6ef3e9dcef6c2ca3dff_113862_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011643_hufa48ffd4c7b2b6ef3e9dcef6c2ca3dff_113862_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=230 data-flex-basis=554px></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~/rook/deploy/examples/monitoring# kubectl get svc -n rook-ceph
</span></span><span class=line><span class=cl>NAME                                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>             AGE
</span></span><span class=line><span class=cl>grafana                                  NodePort    10.202.70.24     &lt;none&gt;        3000:31782/TCP      5m59s
</span></span><span class=line><span class=cl>prometheus-operated                      ClusterIP   None             &lt;none&gt;        9090/TCP            7m39s
</span></span><span class=line><span class=cl>rook-ceph-mgr                            ClusterIP   10.202.138.211   &lt;none&gt;        9283/TCP            65m
</span></span><span class=line><span class=cl>rook-ceph-mgr-dashboard                  ClusterIP   10.202.204.184   &lt;none&gt;        8443/TCP            65m
</span></span><span class=line><span class=cl>rook-ceph-mgr-dashboard-external-https   NodePort    10.202.5.199     &lt;none&gt;        8443:30125/TCP      44m
</span></span><span class=line><span class=cl>rook-ceph-mon-a                          ClusterIP   10.202.156.149   &lt;none&gt;        6789/TCP,3300/TCP   70m
</span></span><span class=line><span class=cl>rook-ceph-mon-b                          ClusterIP   10.202.75.120    &lt;none&gt;        6789/TCP,3300/TCP   66m
</span></span><span class=line><span class=cl>rook-ceph-mon-c                          ClusterIP   10.202.62.213    &lt;none&gt;        6789/TCP,3300/TCP   65m
</span></span><span class=line><span class=cl>rook-prometheus                          NodePort    10.202.7.217     &lt;none&gt;        9090:30900/TCP      7m34s
</span></span></code></pre></div><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011803.png width=1448 height=759 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011803_hu1f870e18f0c64574d02f17997d4471a8_125039_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011803_hu1f870e18f0c64574d02f17997d4471a8_125039_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=190 data-flex-basis=457px></p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011815.png width=1441 height=939 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011815_hu82d59408d9432528e72ef95a24dc46c3_125001_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011815_hu82d59408d9432528e72ef95a24dc46c3_125001_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=153 data-flex-basis=368px></p><p><a class=link href=https://grafana.com/grafana/dashboards/2842-ceph-cluster/ target=_blank rel=noopener>https://grafana.com/grafana/dashboards/2842-ceph-cluster/</a>
<a class=link href=https://grafana.com/grafana/dashboards/5336-ceph-osd-single/ target=_blank rel=noopener>https://grafana.com/grafana/dashboards/5336-ceph-osd-single/</a>
<a class=link href=https://grafana.com/grafana/dashboards/5342-ceph-pools/ target=_blank rel=noopener>https://grafana.com/grafana/dashboards/5342-ceph-pools/</a></p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011903.png width=1446 height=634 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011903_hu31a26029c3bc67d7e7bd77aeff889e53_65602_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011903_hu31a26029c3bc67d7e7bd77aeff889e53_65602_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=228 data-flex-basis=547px></p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011931.png width=923 height=713 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011931_hu400745370edec892cf9b4934df1195c3_87309_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011931_hu400745370edec892cf9b4934df1195c3_87309_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=129 data-flex-basis=310px></p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011947.png width=1449 height=943 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011947_hua1b1196a501c1b4022290a8f10af4d50_244881_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204011947_hua1b1196a501c1b4022290a8f10af4d50_244881_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=153 data-flex-basis=368px></p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204012015.png width=960 height=723 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204012015_hu0d0f75c205a22f750016276011675d24_88153_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204012015_hu0d0f75c205a22f750016276011675d24_88153_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=132 data-flex-basis=318px></p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204012038.png width=1450 height=942 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204012038_hu2638f959af8827f3999ab44a7316741c_154257_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204012038_hu2638f959af8827f3999ab44a7316741c_154257_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=153 data-flex-basis=369px></p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204012115.png width=967 height=712 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204012115_hub942493ce7f2b712a3e500db4f71c6fa_86962_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204012115_hub942493ce7f2b712a3e500db4f71c6fa_86962_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=135 data-flex-basis=325px></p><p><img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204012129.png width=1449 height=947 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204012129_hu57c2dc771d5ccb64aacf6b6a965ee662_168596_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231204012129_hu57c2dc771d5ccb64aacf6b6a965ee662_168596_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=153 data-flex-basis=367px></p><h2 id=reset清除集群設定>reset清除集群設定</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 全部清除，包含所有創建目錄，systemd服務，/etc/hosts</span>
</span></span><span class=line><span class=cl><span class=o>(</span>kubespray-venv<span class=o>)</span> <span class=o>[</span>root@ansible kubespray<span class=o>]</span><span class=c1># ansible-playbook -i inventory/mycluster/hosts.yaml  --become --become-user=root reset.yml</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 流程進行，會再次確認您是否要reset，要輸入yes，ansible才會開始繼續進行</span>
</span></span><span class=line><span class=cl>PLAY <span class=o>[</span>Reset cluster<span class=o>]</span> *******************************************************************************************************************
</span></span><span class=line><span class=cl>週日 <span class=m>03</span> 十二月 <span class=m>2023</span>  14:43:01 +0800 <span class=o>(</span>0:00:02.729<span class=o>)</span>       0:00:44.278 ****************
</span></span><span class=line><span class=cl><span class=o>[</span>Reset Confirmation<span class=o>]</span>
</span></span><span class=line><span class=cl>Are you sure you want to reset cluster state? Type <span class=s1>&#39;yes&#39;</span> to reset your cluster.:
</span></span><span class=line><span class=cl>yes
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 清除完畢之後，主機要systemctl daemon-reload，不然會被判斷還有etcd服務，導致無法佈屬</span>
</span></span><span class=line><span class=cl><span class=c1># 以上執行完畢，重開機，即可重新再佈屬</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@k8s-master72u:~# systemctl status etcd.service
</span></span><span class=line><span class=cl>Warning: The unit file, <span class=nb>source</span> configuration file or drop-ins of etcd.service changed on disk. Run <span class=s1>&#39;systemctl daemon-reload&#39;</span> to reload units.
</span></span><span class=line><span class=cl>○ etcd.service - etcd
</span></span><span class=line><span class=cl>     Loaded: loaded <span class=o>(</span>/etc/systemd/system/etcd.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
</span></span><span class=line><span class=cl>     Active: inactive <span class=o>(</span>dead<span class=o>)</span> since Mon 2023-12-04 17:46:25 UTC<span class=p>;</span> 1min 34s ago
</span></span><span class=line><span class=cl>    Process: <span class=m>906</span> <span class=nv>ExecStart</span><span class=o>=</span>/usr/local/bin/etcd <span class=o>(</span><span class=nv>code</span><span class=o>=</span>killed, <span class=nv>signal</span><span class=o>=</span>TERM<span class=o>)</span>
</span></span><span class=line><span class=cl>   Main PID: <span class=m>906</span> <span class=o>(</span><span class=nv>code</span><span class=o>=</span>killed, <span class=nv>signal</span><span class=o>=</span>TERM<span class=o>)</span>
</span></span><span class=line><span class=cl>        CPU: 6min 4.994s
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Dec <span class=m>04</span> 17:46:24 k8s-master72u etcd<span class=o>[</span>906<span class=o>]</span>: <span class=o>{</span><span class=s2>&#34;level&#34;</span>:<span class=s2>&#34;info&#34;</span>,<span class=s2>&#34;ts&#34;</span>:<span class=s2>&#34;2023-12-04T17:46:24.978391Z&#34;</span>,<span class=s2>&#34;caller&#34;</span>:<span class=s2>&#34;rafthttp/pipeline.go:85&#34;</span>,<span class=s2>&#34;msg&#34;</span>:<span class=s2>&#34;stopped HTTP pipelining with remot&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 04 17:46:24 k8s-master72u etcd[906]: {&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:&#34;</span>2023-12-04T17:46:24.978409Z<span class=s2>&#34;,&#34;</span>caller<span class=s2>&#34;:&#34;</span>rafthttp/stream.go:442<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>stopped stream reader with remote p&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>04</span> 17:46:24 k8s-master72u etcd<span class=o>[</span>906<span class=o>]</span>: <span class=o>{</span><span class=s2>&#34;level&#34;</span>:<span class=s2>&#34;info&#34;</span>,<span class=s2>&#34;ts&#34;</span>:<span class=s2>&#34;2023-12-04T17:46:24.97843Z&#34;</span>,<span class=s2>&#34;caller&#34;</span>:<span class=s2>&#34;rafthttp/stream.go:442&#34;</span>,<span class=s2>&#34;msg&#34;</span>:<span class=s2>&#34;stopped stream reader with remote pe&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 04 17:46:24 k8s-master72u etcd[906]: {&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:&#34;</span>2023-12-04T17:46:24.97845Z<span class=s2>&#34;,&#34;</span>caller<span class=s2>&#34;:&#34;</span>rafthttp/peer.go:335<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>stopped remote peer<span class=s2>&#34;,&#34;</span>remote-peer-id<span class=s2>&#34;:&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 04 17:46:24 k8s-master72u etcd[906]: {&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:&#34;</span>2023-12-04T17:46:24.98816Z<span class=s2>&#34;,&#34;</span>caller<span class=s2>&#34;:&#34;</span>embed/etcd.go:579<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>stopping serving peer traffic<span class=s2>&#34;,&#34;</span>address<span class=s2>&#34;:&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 04 17:46:24 k8s-master72u etcd[906]: {&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:&#34;</span>2023-12-04T17:46:24.988259Z<span class=s2>&#34;,&#34;</span>caller<span class=s2>&#34;:&#34;</span>embed/etcd.go:584<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>stopped serving peer traffic<span class=s2>&#34;,&#34;</span>address<span class=s2>&#34;:&gt;
</span></span></span><span class=line><span class=cl><span class=s2>Dec 04 17:46:24 k8s-master72u etcd[906]: {&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:&#34;</span>2023-12-04T17:46:24.988274Z<span class=s2>&#34;,&#34;</span>caller<span class=s2>&#34;:&#34;</span>embed/etcd.go:378<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>closed etcd server<span class=s2>&#34;,&#34;</span>name<span class=s2>&#34;:&#34;</span>etcd2<span class=s2>&#34;,&#34;</span>data&gt;
</span></span><span class=line><span class=cl>Dec <span class=m>04</span> 17:46:25 k8s-master72u systemd<span class=o>[</span>1<span class=o>]</span>: etcd.service: Deactivated successfully.
</span></span><span class=line><span class=cl>Dec <span class=m>04</span> 17:46:25 k8s-master72u systemd<span class=o>[</span>1<span class=o>]</span>: Stopped etcd.
</span></span><span class=line><span class=cl>Dec <span class=m>04</span> 17:46:25 k8s-master72u systemd<span class=o>[</span>1<span class=o>]</span>: etcd.service: Consumed 6min 4.994s CPU time.
</span></span></code></pre></div><h2 id=如果沒外部vipworker-node透過nginx-proxy反代理訪問master-node的kube-apiserver>如果沒外部VIP，worker node透過nginx proxy反代理，訪問master node的kube-apiserver</h2><p>worker node透過nginx proxy反代理，訪問master node的kube-apiserver，每一個worker node都會有一個nginx pod
<img src=/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203004609.png width=914 height=674 srcset="/2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203004609_hu3312105d17ebb25db3905d7f10fea4ea_51982_480x0_resize_box_3.png 480w, /2023/ansible-deploy-kubernetes-cluster-use-kubespray-haproxy-keepalived/media/Pasted-image-20231203004609_hu3312105d17ebb25db3905d7f10fea4ea_51982_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=135 data-flex-basis=325px></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@k8s-master71u:~# kubectl get pod -A -o wide
</span></span><span class=line><span class=cl>NAMESPACE     NAME                                       READY   STATUS    RESTARTS        AGE     IP             NODE    NOMINATED NODE   READINESS GATES
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kube-system   nginx-proxy-node4                          1/1     Running   <span class=m>0</span>               5m14s   192.168.1.75   node4   &lt;none&gt;           &lt;none&gt;
</span></span><span class=line><span class=cl>kube-system   nginx-proxy-node5                          1/1     Running   <span class=m>0</span>               5m2s    192.168.1.76   node5   &lt;none&gt;           &lt;none&gt;
</span></span></code></pre></div></section><footer class=article-footer><section class=article-tags><a href=/tags/kubernetes/>kubernetes</a>
<a href=/tags/ansible/>ansible</a>
<a href=/tags/kubespray/>kubespray</a>
<a href=/tags/docker/>docker</a>
<a href=/tags/cri-docker/>cri-docker</a>
<a href=/tags/keepalived/>keepalived</a>
<a href=/tags/haproxy/>haproxy</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>相關文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/2023/kubernetes-install/><div class=article-details><h2 class=article-title>kubeadm安裝 高可用版(ubuntu2204+k8s版本1.26.3+docker+cri-docker)</h2></div></a></article><article><a href=/2023/kubernetes-docker/><div class=article-details><h2 class=article-title>使用Docker安裝Rancher管理平台，納管現有k8s集群(單節點-非高可用)</h2></div></a></article><article class=has-image><a href=/2023/kubernetes-rook-ceph/><div class=article-image><img src=/2023/kubernetes-rook-ceph/media/_hu7a89e185921f36bf665966d7e56502de_57574_d6baedf257fa7887ff4d47d713bc7519.png width=250 height=150 loading=lazy alt="Featured image of post 使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統" data-key=kubernetes-rook-ceph data-hash="md5-+ui45GSXzaiNwne0YwCUSg=="></div><div class=article-details><h2 class=article-title>使用Rook在Kubernetes安裝與管理Ceph分布式存儲系統</h2></div></a></article><article class=has-image><a href=/2023/kubernetes-summit-2023/><div class=article-image><img src=/2023/kubernetes-summit-2023/media/_hub7b044231ef4aeecfc91c4975bdc3f98_300198_549732b0a870c59377d4ec865acd8eb7.png width=250 height=150 loading=lazy alt="Featured image of post Kubernetes Summit 2023" data-key=kubernetes-summit-2023 data-hash="md5-l3szp9B4JNVqHi1UN38u6w=="></div><div class=article-details><h2 class=article-title>Kubernetes Summit 2023</h2></div></a></article><article><a href=/2023/kubernetes-helm/><div class=article-details><h2 class=article-title>使用Helm安裝Rancher管理平台，納管現有k8s集群(高可用)</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//blog-goldfishbrain-fighting-com.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2022 -
2023 翻轉吧金魚腦</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 建立<br>主題 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.20.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 設計</section><section class=totalcount>發表了81篇文章 ·
總計36.18k字</section><section class=running-time>本站已運行
<span id=runningdays class=running-days></span></section><section class=visit-count><span id=busuanzi_container_site_pv style=display:none>本站總訪問量 <span id=busuanzi_value_site_pv></span> 次 </span>·
<span id=busuanzi_container_site_uv style=display:none>總訪客數 <span id=busuanzi_value_site_uv></span> 人</span></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><a href=# id=back-to-top title=返回顶部></a><style>#back-to-top{display:none;position:fixed;bottom:20px;right:55px;width:55px;height:55px;border-radius:7px;background-color:rgba(64,158,255,.5);box-shadow:var(--shadow-l2);font-size:30px;text-align:center;line-height:50px;cursor:pointer}#back-to-top:before{content:' ';display:inline-block;position:relative;top:0;transform:rotate(135deg);height:10px;width:10px;border-width:0 0 2px 2px;border-color:var(--back-to-top-color);border-style:solid}#back-to-top:hover:before{border-color:#2674e0}@media screen and (max-width:768px){#back-to-top{bottom:20px;right:20px;width:40px;height:40px;font-size:10px}}@media screen and (min-width:1024px){#back-to-top{bottom:20px;right:40px}}@media screen and (min-width:1280px){#back-to-top{bottom:20px;right:55px}}@media screen and (min-width:1536px){#back-to-top{visibility:hidden}}</style><script>function backToTop(){document.documentElement.scrollIntoView({behavior:"smooth"})}window.onload=function(){let t=this.document.documentElement.scrollTop||this.document.body.scrollTop,e=this.document.getElementById("back-to-top");t>0?e.style.display="inline":e.style.display="none"},window.onscroll=function(){let t=this.document.documentElement.scrollTop||this.document.body.scrollTop,e=this.document.getElementById("back-to-top");t<200?e.style.display="none":(e.style.display="inline",e.addEventListener("click",backToTop,!1))}</script><script>(function(){var t,e=window;if(e.ChannelIO)return e.console.error("ChannelIO script included twice.");t=function(){t.c(arguments)},t.q=[],t.c=function(e){t.q.push(e)},e.ChannelIO=t;function n(){if(e.ChannelIOInitialized)return;e.ChannelIOInitialized=!0;var n,t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src="https://cdn.channel.io/plugin/ch-plugin-web.js",n=document.getElementsByTagName("script")[0],n.parentNode&&n.parentNode.insertBefore(t,n)}document.readyState==="complete"?n():(e.addEventListener("DOMContentLoaded",n),e.addEventListener("load",n))})(),ChannelIO("boot",{pluginKey:"ad479103-2d28-421a-a916-ae09e72ce014"})</script><script>let s1="2023-10-7";s1=new Date(s1.replace(/-/g,"/"));let s2=new Date,timeDifference=s2.getTime()-s1.getTime(),days=Math.floor(timeDifference/(1e3*60*60*24)),hours=Math.floor(timeDifference%(1e3*60*60*24)/(1e3*60*60)),minutes=Math.floor(timeDifference%(1e3*60*60)/(1e3*60)),result=days+"天"+hours+"小時"+minutes+"分鐘";document.getElementById("runningdays").innerHTML=result</script></body></html>